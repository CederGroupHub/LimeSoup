From cou.: 0 Journal
From Soup: 0 J. Anal. At. Spectrom.
 ###### 
From cou.: 1 A novel method based on laser induced breakdown spectroscopy (LIBS) and random forest regression (RFR) was proposed for the quantitative analysis of multiple elements in fourteen steel samples. Normalized LIBS spectra of steel with characteristic lines (Si, Mn, Cr, Ni and Cu) identified by the NIST database were used as analysis spectra. Then, two parameters of RFR were optimized by out-of-bag (OOB) error estimation. The performance of the calibration model was investigated by different input variables (the whole spectral bands (220–800 nm) and spectra feature bands (220–400 nm)). In order to validate the predictive ability of the multiple element calibration RFR model in steel, we compared RFR with partial least-squares (PLS) and support vector machines (SVM) by means of prediction accuracy and root mean square error (RMSE). Thus, the RFR model can eliminate the influence of nonlinear factors due to self-absorption in the plasma and provide a better predictive result. This confirms that the LIBS technique coupled with RFR has good potential for use in the in situ rapid determination of multiple elements in steel and even in the field of metallurgy.
From Soup: 1 A novel method based on laser induced breakdown spectroscopy (LIBS) and random forest regression (RFR) was proposed for the quantitative analysis of multiple elements in fourteen steel samples. Normalized LIBS spectra of steel with characteristic lines (Si, Mn, Cr, Ni and Cu) identified by the NIST database were used as analysis spectra. Then, two parameters of RFR were optimized by out-of-bag (OOB) error estimation. The performance of the calibration model was investigated by different input variables (the whole spectral bands (220–800 nm) and spectra feature bands (220–400 nm)). In order to validate the predictive ability of the multiple element calibration RFR model in steel, we compared RFR with partial least-squares (PLS) and support vector machines (SVM) by means of prediction accuracy and root mean square error (RMSE). Thus, the RFR model can eliminate the influence of nonlinear factors due to self-absorption in the plasma and provide a better predictive result. This confirms that the LIBS technique coupled with RFR has good potential for use in the in situ rapid determination of multiple elements in steel and even in the field of metallurgy.
 ###### 
From cou.: 2 Iron and steel are some of the most significant engineering and construction materials due to their low prices and wide applicability. Some added elements (such as silicon, chromium and nickel) play an important role in improving mechanical and chemical properties of the steel-making processes. Thus, the content of these elements must be strictly controlled, which contributes to improving the performance of the steel materials, and therefore, a rapid and precise analytical method is desirable. It becomes particularly significant to have accurate and sensitive quantitative analysis of steel in metallurgy and related fields. Conventional quantitative analysis technologies for steel mainly include1–6 chemical analysis, atomic emission spectrometry (AES), optical emission spectroscopy (OES), X-ray fluorescence (XRF), inductively coupled plasma-mass spectrometry (ICP-MS), and chromatography. However, these techniques require complicated sample preparation and long analysis time, which hinders their application for in situ, on-line and real-time analysis.
From Soup: 2 Iron and steel are some of the most significant engineering and construction materials due to their low prices and wide applicability. Some added elements (such as silicon, chromium and nickel) play an important role in improving mechanical and chemical properties of the steel-making processes. Thus, the content of these elements must be strictly controlled, which contributes to improving the performance of the steel materials, and therefore, a rapid and precise analytical method is desirable. It becomes particularly significant to have accurate and sensitive quantitative analysis of steel in metallurgy and related fields. Conventional quantitative analysis technologies for steel mainly include 1–6 chemical analysis, atomic emission spectrometry (AES), optical emission spectroscopy (OES), X-ray fluorescence (XRF), inductively coupled plasma-mass spectrometry (ICP-MS), and chromatography. However, these techniques require complicated sample preparation and long analysis time, which hinders their application for in situ , on-line and real-time analysis.
 ###### 
From cou.: 3 Laser induced breakdown spectroscopy (LIBS) is a new type of plasma spectral quantitative analysis technology with the capability of rapid and real-time analysis. Compared with conventional analytical techniques, LIBS has many obvious advantages,7–9 such as simultaneous analysis of multiple elements, all types of the sample (solids, liquids, and gases) can be analyzed, less sample requirement and minimal sample preparation. Therefore, LIBS is considered to be one of the most valuable prospective analysis tool. At present, the LIBS technology has become an international research focus for metallurgical analysis.10–13 The application of LIBS to the metallurgical industry, including iron ore selection,14,15 process control16,17 and iron slag analysis,18 has been widely studied by many groups, and several review works16,19,20 have already been presented.
From Soup: 3 Laser induced breakdown spectroscopy (LIBS) is a new type of plasma spectral quantitative analysis technology with the capability of rapid and real-time analysis. Compared with conventional analytical techniques, LIBS has many obvious advantages, 7–9 such as simultaneous analysis of multiple elements, all types of the sample (solids, liquids, and gases) can be analyzed, less sample requirement and minimal sample preparation. Therefore, LIBS is considered to be one of the most valuable prospective analysis tool. At present, the LIBS technology has become an international research focus for metallurgical analysis. 10–13 The application of LIBS to the metallurgical industry, including iron ore selection, 14,15 process control 16,17 and iron slag analysis, 18 has been widely studied by many groups, and several review works 16,19,20 have already been presented.
 ###### 
From cou.: 4 Quantitative analysis methods on LIBS mainly refers to the calibration method and calibration-free (CF) approach.21 The first one is based on a set of calibration samples of known content, whereas CF-LIBS assumes local thermodynamic equilibrium (LTE) in the laser plasma to calculate its plasma temperature and its electron density, from which the composition of the sample is then derived, regardless of the matrix effect. The simplest and most widespread quantitative analysis method is the standard calibration method, which constructs the relationship between the integrated intensity of the analysis line or intensity ratio (analysis line vs. reference line) of the element of interest and the known concentration of a set of calibration samples. The most common calibration curve is univariate, and its regression model is established by using the intensity of a single feature line and the corresponding concentration of the element under testing. However, the univariate calibration model cannot often meet the requirement of the quantitative analysis due to the fluctuation of laser energy, the inhomogeneity of samples and complex matrix effect.22 The chemical composition of the steel sample is affected by many matrix effects. There is serious overlapping of spectral peaks in the spectrum of the iron substrate, and the traditional univariate calibration model fails to eliminate the impact of these interference factors. The multivariate calibration method is an effective tool to overcome the matrix effect for a complex sample. At present, many multivariate calibration algorithms have been developed for quantitative analysis, such as partial least squares (PLS),23–26 principal components analysis (PCA),27–30 artificial neural network (ANN),31–34 and support vector machines (SVM).35–37 However, random forest regression (RFR), a new regression algorithm based on multiple regression trees, was proposed by Leo Breiman in 2001.38 It is based upon an ensemble of decision trees, from which the prediction of a continuous variable is proven as the average of the predictions of all trees. In RF regression, an ensemble of regression trees is grown from separate bootstrap samples of the training data using the classification and regression tree (CART) algorithm. It has been proven that RFR has a good tolerance for the noise, as well as it avoids over-fitting phenomenon by many researchers.
From Soup: 4 Quantitative analysis methods on LIBS mainly refers to the calibration method and calibration-free (CF) approach. 21 The first one is based on a set of calibration samples of known content, whereas CF-LIBS assumes local thermodynamic equilibrium (LTE) in the laser plasma to calculate its plasma temperature and its electron density, from which the composition of the sample is then derived, regardless of the matrix effect. The simplest and most widespread quantitative analysis method is the standard calibration method, which constructs the relationship between the integrated intensity of the analysis line or intensity ratio (analysis line vs. reference line) of the element of interest and the known concentration of a set of calibration samples. The most common calibration curve is univariate, and its regression model is established by using the intensity of a single feature line and the corresponding concentration of the element under testing. However, the univariate calibration model cannot often meet the requirement of the quantitative analysis due to the fluctuation of laser energy, the inhomogeneity of samples and complex matrix effect. 22 The chemical composition of the steel sample is affected by many matrix effects. There is serious overlapping of spectral peaks in the spectrum of the iron substrate, and the traditional univariate calibration model fails to eliminate the impact of these interference factors. The multivariate calibration method is an effective tool to overcome the matrix effect for a complex sample. At present, many multivariate calibration algorithms have been developed for quantitative analysis, such as partial least squares (PLS), 23–26 principal components analysis (PCA), 27–30 artificial neural network (ANN), 31–34 and support vector machines (SVM). 35–37 However, random forest regression (RFR), a new regression algorithm based on multiple regression trees, was proposed by Leo Breiman in 2001. 38 It is based upon an ensemble of decision trees, from which the prediction of a continuous variable is proven as the average of the predictions of all trees. In RF regression, an ensemble of regression trees is grown from separate bootstrap samples of the training data using the classification and regression tree (CART) algorithm. It has been proven that RFR has a good tolerance for the noise, as well as it avoids over-fitting phenomenon by many researchers.
 ###### 
From cou.: 5 J. Remus proposed an approach of LIBS and RF to identify and classify five different materials (four rock samples and one pen ink sample).39 However, in this study, we present a novel method for quantitative analysis of multiple elements in steel by means of integrating LIBS technology with RFR. Normalized LIBS spectra of steel were used as analysis spectra. Two parameters (ntree-number of trees and mtry-random variables) of the RFR algorithm were optimized using out-of-bag (OOB) error estimation. The performance of the calibration model was investigated by different input variables (the whole spectral bands (220–800 nm) and spectra feature bands (220–400 nm)). Both the whole spectral bands and spectra feature bands are the peak intensity at each wavelength. In order to validate the predictive ability of the multiple element calibration model in steel, we compared the result of RFR with partial least-squares (PLS) and support vector machines (SVM) by means of prediction accuracy and root mean square error (MSE).
From Soup: 5 J. Remus proposed an approach of LIBS and RF to identify and classify five different materials (four rock samples and one pen ink sample). 39 However, in this study, we present a novel method for quantitative analysis of multiple elements in steel by means of integrating LIBS technology with RFR. Normalized LIBS spectra of steel were used as analysis spectra. Two parameters ( n tree -number of trees and m try -random variables) of the RFR algorithm were optimized using out-of-bag (OOB) error estimation. The performance of the calibration model was investigated by different input variables (the whole spectral bands (220–800 nm) and spectra feature bands (220–400 nm)). Both the whole spectral bands and spectra feature bands are the peak intensity at each wavelength. In order to validate the predictive ability of the multiple element calibration model in steel, we compared the result of RFR with partial least-squares (PLS) and support vector machines (SVM) by means of prediction accuracy and root mean square error (MSE).
 ###### 
From cou.: 6 The spectra for steel samples were recorded and collected by the LIBS system. A schematic diagram of the LIBS system on this work is presented in Fig. 1. A dual-wavelength (the optional wavelength at 532 nm and 1064 nm) single pulse Q-switched Nd:YAG laser with the fundamental wavelength at 1064 nm, the pulse laser energy of 80 mJ (6 GW cm−2), the pulse duration of 10 ns full width at half maximum (FWHM), and the repetition rate of 20 Hz was used. The steel samples were placed directly on an X–Y–Z manual micrometric stage. The laser beam was focused onto the sample surface vertically by a 50 mm focal-distance lens, producing a spot of about 2 mm diameter. The emission from the plasma created was collected with a 4 mm aperture, with a 7 mm focus fused silica collimator placed at a 45° angle with respect to the laser pulse and a distance of 3 cm from the sample, and then focused into an optical fiber (with a 1000 nm core diameter and 0.22 numerical aperture), which was coupled to the entrance of the Echelle spectrometer (ARYELLE-UV-VIS, LTB150, German). The spectrometer provides a constant spectral resolution (CSR) of 6000 over a wavelength range of 220–800 nm displayable in a single spectrum. An Electron-Multiplying CCD camera (QImaging, UV enhanced, 1004 × 1002 Pixels, USA), coupled to the spectrometer was used for detection of the dispersed light. The overall linear dispersion of the spectrometer camera system ranges from 37 pm (at 220 nm) to 133 pm per pixel (at 800 nm). To prevent the CCD from detecting the early plasma continuum, a mechanical chopper is used in front of the entrance slit. The experiments were carried out under atmosphere conditions, and the gate width of the spectrometer was set to 2 ms. The detector was set to a 1.5 μs delay time between the laser pulse in order to prevent the detection of bremsstrahlung radiation.
From Soup: 6 The spectra for steel samples were recorded and collected by the LIBS system. A schematic diagram of the LIBS system on this work is presented in Fig. 1 . A dual-wavelength (the optional wavelength at 532 nm and 1064 nm) single pulse Q-switched Nd:YAG laser with the fundamental wavelength at 1064 nm, the pulse laser energy of 80 mJ (6 GW cm −2 ), the pulse duration of 10 ns full width at half maximum (FWHM), and the repetition rate of 20 Hz was used. The steel samples were placed directly on an X – Y – Z manual micrometric stage. The laser beam was focused onto the sample surface vertically by a 50 mm focal-distance lens, producing a spot of about 2 mm diameter. The emission from the plasma created was collected with a 4 mm aperture, with a 7 mm focus fused silica collimator placed at a 45° angle with respect to the laser pulse and a distance of 3 cm from the sample, and then focused into an optical fiber (with a 1000 nm core diameter and 0.22 numerical aperture), which was coupled to the entrance of the Echelle spectrometer (ARYELLE-UV-VIS, LTB150, German). The spectrometer provides a constant spectral resolution (CSR) of 6000 over a wavelength range of 220–800 nm displayable in a single spectrum. An Electron-Multiplying CCD camera (QImaging, UV enhanced, 1004 × 1002 Pixels, USA), coupled to the spectrometer was used for detection of the dispersed light. The overall linear dispersion of the spectrometer camera system ranges from 37 pm (at 220 nm) to 133 pm per pixel (at 800 nm). To prevent the CCD from detecting the early plasma continuum, a mechanical chopper is used in front of the entrance slit. The experiments were carried out under atmosphere conditions, and the gate width of the spectrometer was set to 2 ms. The detector was set to a 1.5 μs delay time between the laser pulse in order to prevent the detection of bremsstrahlung radiation.
 ###### 
From cou.: 7 A total of 14 typical steel samples were kindly provided by the China Xi-ning Special Steel CO., LTD (Xi-ning, Qing-hai, China). Table 1 lists the concentration of certain elements of 14 steel samples. The original size of the steel samples was too long to facilitate the experiment, and then a cylinder (the height was 6 mm) was cut from random locations on each steel sample. LIBS spectra of 50 different positions of each sample surface were gathered. In order to decrease the effects of shot to shot fluctuations, each measured spectrum was obtained by accumulating 20 laser pulses. In this work, the analytical spectrum for each steel sample was the average of 50 LIBS spectra from different positions. The total of the spectra for the steel sample was 14, ten samples were selected for the calibration of the quantitative analysis (PLS, SVM and RFR) model, and the rest of the samples were used for validation of the model. Background emission was subtracted from the spectral lines, and each LIBS spectrum was normalized by the maximum integrated intensity. The data processing and quantitative analysis for steel samples by chemometrics methods were completed on Matlab (version 2007a, Mathworks).
From Soup: 7 A total of 14 typical steel samples were kindly provided by the China Xi-ning Special Steel CO., LTD (Xi-ning, Qing-hai, China). Table 1 lists the concentration of certain elements of 14 steel samples. The original size of the steel samples was too long to facilitate the experiment, and then a cylinder (the height was 6 mm) was cut from random locations on each steel sample. LIBS spectra of 50 different positions of each sample surface were gathered. In order to decrease the effects of shot to shot fluctuations, each measured spectrum was obtained by accumulating 20 laser pulses. In this work, the analytical spectrum for each steel sample was the average of 50 LIBS spectra from different positions. The total of the spectra for the steel sample was 14, ten samples were selected for the calibration of the quantitative analysis (PLS, SVM and RFR) model, and the rest of the samples were used for validation of the model. Background emission was subtracted from the spectral lines, and each LIBS spectrum was normalized by the maximum integrated intensity. The data processing and quantitative analysis for steel samples by chemometrics methods were completed on Matlab (version 2007a, Mathworks).
 ###### 
From cou.: 8 Support vector machine (SVM) is a new and promising classification and regression method proposed by Vapnik.40 It was originally developed for classification problems, but can also be extended to solve non-linear regression problems by means of ε-insensitive loss function. In statistical learning theory, empirical risk is the error of prediction results by the model and real results; however, structure risk is the sum of empirical risk and confidence interval. Traditional learning method is based on empirical risk minimization criterion, and it only emphasized the empirical risk minimum error of the training sample and no minimum confidence limit value; therefore, there is a poorer generalization ability. With regard to structural risk minimization, the training error is used as its optimization constraints, and the minimized trust scope value is used as the optimization target, and its generalization ability is much better than traditional learning methods. Therefore, the SVM method proposed is aimed at minimizing the structural risk rather than the empirical risk, and preserving good generalization ability rather than optimizing the agreement with a given (limited) training set. In support vector regression, the input x is first mapped into a higher dimensional feature space by the use of a kernel function, and then a linear model is constructed in this feature space. The kernel functions used in SVM often include linear or polynomial functions, radial basis functions and sigmoid functions. Parameter C is a regularization constant, which determines the trade-off between the model complexity and the degree to which deviations larger than ε are tolerated in optimization formulation. The generalization performance of SVR depends on a good set of parameters: C, ε and the kernel type and corresponding kernel parameters. The selection of the kernel function and corresponding parameters is very important because they define the distribution of the training set samples in the high dimensional feature space. All SVM models in our present study were implemented using the shareware program LibSVM developed by Lin.41 The radial basis function was used as kernel function in this work. For RBF kernel, the most important parameter is the width of the radial basis function.
From Soup: 8 Support vector machine (SVM) is a new and promising classification and regression method proposed by Vapnik. 40 It was originally developed for classification problems, but can also be extended to solve non-linear regression problems by means of ε -insensitive loss function. In statistical learning theory, empirical risk is the error of prediction results by the model and real results; however, structure risk is the sum of empirical risk and confidence interval. Traditional learning method is based on empirical risk minimization criterion, and it only emphasized the empirical risk minimum error of the training sample and no minimum confidence limit value; therefore, there is a poorer generalization ability. With regard to structural risk minimization, the training error is used as its optimization constraints, and the minimized trust scope value is used as the optimization target, and its generalization ability is much better than traditional learning methods. Therefore, the SVM method proposed is aimed at minimizing the structural risk rather than the empirical risk, and preserving good generalization ability rather than optimizing the agreement with a given (limited) training set. In support vector regression, the input x is first mapped into a higher dimensional feature space by the use of a kernel function, and then a linear model is constructed in this feature space. The kernel functions used in SVM often include linear or polynomial functions, radial basis functions and sigmoid functions. Parameter C is a regularization constant, which determines the trade-off between the model complexity and the degree to which deviations larger than ε are tolerated in optimization formulation. The generalization performance of SVR depends on a good set of parameters: C , ε and the kernel type and corresponding kernel parameters. The selection of the kernel function and corresponding parameters is very important because they define the distribution of the training set samples in the high dimensional feature space. All SVM models in our present study were implemented using the shareware program LibSVM developed by Lin. 41 The radial basis function was used as kernel function in this work. For RBF kernel, the most important parameter is the width of the radial basis function.
 ###### 
From cou.: 9 Random forest (RF) as a new classification algorithm based on multiple classifier was proposed by Leo Breiman.38 In the RF method, combined model {h(X, θk), k = 1,…p} consists of random vector (i.e. regression tree which is concerned with the number and intensity of feature spectrum and consists of branching variables and nodes) by bootstrap resample method. This is a resample method with replacement, in which each tree casts an equal valued vote for the prediction at input vector X and where the {θk} is independent, identically distributed random vector. k is the index for the tree in the forest and p is the total number of trees in the forest. For the k-th regression tree, we generated a random vector θk, independent of the previous random vectors θ1,…θk−1 but with the same distribution, and we grow a tree using the training set and θk, resulting in the predictor {h(X, θk)} where X is an input vector. The predictive vector is numerical, and random forest generated is a multivariate nonlinear regression analysis model. The prediction result of random forest was produced by the average of {h(X, θk)} for k trees. The training set for the random forest model was independent absolutely and selected from random vectors Y and X. The generalization mean square error of the numeric predictive vector is as follows:where EX,Y is a function with regard to X and Y.
From Soup: 9 Random forest (RF) as a new classification algorithm based on multiple classifier was proposed by Leo Breiman. 38 In the RF method, combined model { h ( X , θ k ), k = 1,… p } consists of random vector ( i.e. regression tree which is concerned with the number and intensity of feature spectrum and consists of branching variables and nodes) by bootstrap resample method. This is a resample method with replacement, in which each tree casts an equal valued vote for the prediction at input vector X and where the { θ k } is independent, identically distributed random vector. k is the index for the tree in the forest and p is the total number of trees in the forest. For the k -th regression tree, we generated a random vector θ k , independent of the previous random vectors θ 1 ,… θ k −1 but with the same distribution, and we grow a tree using the training set and θ k , resulting in the predictor { h ( X , θ k )} where X is an input vector. The predictive vector is numerical, and random forest generated is a multivariate nonlinear regression analysis model. The prediction result of random forest was produced by the average of { h ( X , θ k )} for k trees. The training set for the random forest model was independent absolutely and selected from random vectors Y and X . The generalization mean square error of the numeric predictive vector is as follows: where E X , Y is a function with regard to X and Y .
 ###### 
From cou.: 10 Random forest regression has the following characteristics:38
From Soup: 10 Random forest regression has the following characteristics: 38
 ###### 
From cou.: 11 a. When the number of trees in the forest tend to infinity:
From Soup: 11 a. When the number of trees in the forest tend to infinity:
 ###### 
From cou.: 12 α
From Soup: 12 α
 ###### 
From cou.: 13 ν
From Soup: 13 ν
 ###### 
From cou.: 14 k
From Soup: 14 k
 ###### 
From cou.: 15 b. Assume that for all θ and θ, E(Y) = EXh(X, θ):
From Soup: 15 b. Assume that for all θ and θ , E ( Y ) = E X h ( X , θ ):
 ###### 
From cou.: 16 PE
From Soup: 16 PE
 ###### 
From cou.: 17 *
From Soup: 17 *
 ###### 
From cou.: 18 E
From Soup: 18 E
 ###### 
From cou.: 19 θ
From Soup: 19 θ
 ###### 
From cou.: 20 E
From Soup: 20 E
 ###### 
From cou.: 21 X
From Soup: 21 X , Y
 ###### 
From cou.: 22 Y
From Soup: 22 Y
 ###### 
From cou.: 23 Y
From Soup: 23 h
 ###### 
From cou.: 24 h
From Soup: 24 X
 ###### 
From cou.: 25 X
From Soup: 25 θ
 ###### 
From cou.: 26 θ
From Soup: 26 2
 ###### 
From cou.: 27 2
From Soup: 27 Y
 ###### 
From cou.: 28 Y
From Soup: 28 h
 ###### 
From cou.: 29 h
From Soup: 29 X
 ###### 
From cou.: 30 X
From Soup: 30 θ
 ###### 
From cou.: 31 θ
From Soup: 31 θ
 ###### 
From cou.: 32 θ
From Soup: 32 The process of random forest regression algorithm is as follows: 42,43
 ###### 
From cou.: 33 The process of random forest regression algorithm is as follows:42,43
From Soup: 33 (1) There are n samples in the original dataset. b bootstrap sample set were randomly drawn with replacement using the bootstrap resampling method, and thus construct b regression trees. Hence, some of the samples will be repeated, while others will be “left out” from the dataset and form out-of-bag (OOB) samples (about 37% of the samples in the original dataset). This left out data, which is called OOB data, is used to calibrate the performance of each tree. The predictive set for random forest consisted of b OOB data that were generated by no drawn samples for each bootstrap sample.
 ###### 
From cou.: 34 (1) There are n samples in the original dataset. b bootstrap sample set were randomly drawn with replacement using the bootstrap resampling method, and thus construct b regression trees. Hence, some of the samples will be repeated, while others will be “left out” from the dataset and form out-of-bag (OOB) samples (about 37% of the samples in the original dataset). This left out data, which is called OOB data, is used to calibrate the performance of each tree. The predictive set for random forest consisted of b OOB data that were generated by no drawn samples for each bootstrap sample.
From Soup: 34 (2) Suppose the variable number of original data is p , m try ( m try < p ) variable as the alternative branching variable was chosen randomly at each node in every regression tree, and then the optimal branch was selected according to branch optimum rule. In random forests regression, parameter m try = p /3; 38
 ###### 
From cou.: 35 (2) Suppose the variable number of original data is p, mtry(mtry < p) variable as the alternative branching variable was chosen randomly at each node in every regression tree, and then the optimal branch was selected according to branch optimum rule. In random forests regression, parameter mtry = p/3;38
From Soup: 35 (3) Since the recursive branch of each regression tree is top-down, the smallest size set leaf node node size = 5, as a regression tree growth termination conditions. The size and range of spectroscopy in the terminal nodes of the trees increases and the predictive accuracy decreases when the node size is increased above the optimum value.
 ###### 
From cou.: 36 (3) Since the recursive branch of each regression tree is top-down, the smallest size set leaf node node size = 5, as a regression tree growth termination conditions. The size and range of spectroscopy in the terminal nodes of the trees increases and the predictive accuracy decreases when the node size is increased above the optimum value.
From Soup: 36 (4) The b regression trees generated constitute the regression model of random forests, and the performance of the regression model was evaluated using mean square error (MSE) and coefficients of determination ( R 2 ) of OOB data:
 ###### 
From cou.: 37 (4) The b regression trees generated constitute the regression model of random forests, and the performance of the regression model was evaluated using mean square error (MSE) and coefficients of determination (R2) of OOB data:
From Soup: 37 Among them, y i is the bag outside data of the actual value of the dependent variable, y î is the random forest predictive value for the data outside the bag, y 2 is the random forest data predicted variance outside the bag.
 ###### 
From cou.: 38 Among them, yi is the bag outside data of the actual value of the dependent variable, yî is the random forest predictive value for the data outside the bag, y2 is the random forest data predicted variance outside the bag.
From Soup: 38 Fig. 2 shows the averaged normalization spectrum of 2# sample, which includes the emission lines of the major elements in steel. Steel is a complex sample containing many chemical elements and thus related to LIBS spectra characterized by hundreds of atomic lines. Spectral lines of major elements (Si, Mn, Cr, Ni and Cu) in steel samples were detected and identified based on the NIST atomic database, 44 which are summarized in Table 2 . Some of the stronger elemental emission lines were used for quantitative analysis of steel. Steel is relatively rich in Fe emission lines, and the spectral intensity of Si, Mn, Cr, Ni and Cu were affected by the matrix effect from steel samples and the rich iron emission lines. In order to obtain a better quantitative performance, some simple pre-processing methods ( e.g. smooth and de-noise) based on five points moving-average were used to improve the signal-to-background ratio.
 ###### 
From cou.: 39 Fig. 2 shows the averaged normalization spectrum of 2# sample, which includes the emission lines of the major elements in steel. Steel is a complex sample containing many chemical elements and thus related to LIBS spectra characterized by hundreds of atomic lines. Spectral lines of major elements (Si, Mn, Cr, Ni and Cu) in steel samples were detected and identified based on the NIST atomic database,44 which are summarized in Table 2. Some of the stronger elemental emission lines were used for quantitative analysis of steel. Steel is relatively rich in Fe emission lines, and the spectral intensity of Si, Mn, Cr, Ni and Cu were affected by the matrix effect from steel samples and the rich iron emission lines. In order to obtain a better quantitative performance, some simple pre-processing methods (e.g. smooth and de-noise) based on five points moving-average were used to improve the signal-to-background ratio.
From Soup: 39 A calibration model is an essential aspect of quantitative analysis of steels using LIBS and RFR. In general, the establishment of the calibration model mainly includes three parts: (1) the optimization of the modeling conditions sample, including the selection of input variables and the pretreatment method; (2) the selection of training samples in modeling, enough training samples with rich feature information and minimized interference is the precondition for an accurate model, which determines the adaptability and reliability of the calibration model; and (3) the implementation of the modeling algorithm.
 ###### 
From cou.: 40 A calibration model is an essential aspect of quantitative analysis of steels using LIBS and RFR. In general, the establishment of the calibration model mainly includes three parts: (1) the optimization of the modeling conditions sample, including the selection of input variables and the pretreatment method; (2) the selection of training samples in modeling, enough training samples with rich feature information and minimized interference is the precondition for an accurate model, which determines the adaptability and reliability of the calibration model; and (3) the implementation of the modeling algorithm.
From Soup: 40 Two important parameters in RFR are n tree , the number of the trees in the forest, and m try, the number of peaks randomly selected as the candidates for splitting at each node. Theoretically, the predictive error of the regression tree tends to a finite upper bound when n tree reaches a certain value. In other words, n tree increased is over the optimum value. There is a general increase in the computational expense, but the improvement of the predictive accuracy is minor. m try is one of the most major characteristics through each division that introduces random nodes for randomly selected attributes. It was assumed that there were P attributes in the training sample, and m try attributes were extracted randomly as candidate attributes between each of the internal nodes in the decision tree ( m try < P ). The effects of different n tree and m try on the calibration model were investigated by the OOB error estimate (as shown in Fig. 3 ). n tree values were 1, 100, 200, 300, 400, 500, and 600, respectively. When the value of n tree is decreased too far, the results deteriorate significantly. If n tree reaches one, the random forest becomes a single unpruned regression tree. The OOB error of the RFR model is relatively high when n tree is below 300, and it reaches a minimum when n tree reaches 300. In other words, the quantitative accuracy of the RF model was found to be the best. If the number of trees in the forest is increased above the optimum, there is a general increase in computational expense, but the results do not improve significantly, and the OOB error tended to be limited by an upper bound. m try values used to test were 6404, 7319, 8539, 10 247, 12 809, 17 078, 25 617, and 51 234, respectively. When m try becomes very small, not enough peaks are considered at each split, and hence the predictive quality of each tree decreases. The exact value of m try below which a decrease in predictive error is observed will depend on the number and relative importance of peaks present in the data set. Moreover, m try = 17078 (namely P /3) was found to be the best choice based on the OOB error rate. 38 Therefore, the two optimized parameters of the random forest are as follows: n tree = 300 and m try = 17 078.
 ###### 
From cou.: 41 Two important parameters in RFR are ntree, the number of the trees in the forest, and mtry, the number of peaks randomly selected as the candidates for splitting at each node. Theoretically, the predictive error of the regression tree tends to a finite upper bound when ntree reaches a certain value. In other words, ntree increased is over the optimum value. There is a general increase in the computational expense, but the improvement of the predictive accuracy is minor. mtry is one of the most major characteristics through each division that introduces random nodes for randomly selected attributes. It was assumed that there were P attributes in the training sample, and mtry attributes were extracted randomly as candidate attributes between each of the internal nodes in the decision tree (mtry < P). The effects of different ntree and mtry on the calibration model were investigated by the OOB error estimate (as shown in Fig. 3). ntree values were 1, 100, 200, 300, 400, 500, and 600, respectively. When the value of ntree is decreased too far, the results deteriorate significantly. If ntree reaches one, the random forest becomes a single unpruned regression tree. The OOB error of the RFR model is relatively high when ntree is below 300, and it reaches a minimum when ntree reaches 300. In other words, the quantitative accuracy of the RF model was found to be the best. If the number of trees in the forest is increased above the optimum, there is a general increase in computational expense, but the results do not improve significantly, and the OOB error tended to be limited by an upper bound. mtry values used to test were 6404, 7319, 8539, 10247, 12809, 17078, 25617, and 51234, respectively. When mtry becomes very small, not enough peaks are considered at each split, and hence the predictive quality of each tree decreases. The exact value of mtry below which a decrease in predictive error is observed will depend on the number and relative importance of peaks present in the data set. Moreover, mtry = 17078 (namely P/3) was found to be the best choice based on the OOB error rate.38 Therefore, the two optimized parameters of the random forest are as follows: ntree = 300 and mtry = 17078.
From Soup: 41 Input variables are significant for the calibration model of steel. N-fold cross-validation is a method for model selection in terms of the predictive ability of the models. In machine learning, dataset A was divided into training set B and test set C . In the case that the amount of dataset is not large enough, in order to make full use of the dataset to investigate the performance of the model algorithm, dataset A will be randomly divided into n package, one of the n package is as test set, and the rest of the n − 1 package are the train set each time. In this work, the whole spectral bands and feature spectral bands as input variables were investigated to improve the predictive accuracy of RFR, and then the RFR calibration model for multi-elements in steel was validated by OOB estimation and 10-fold cross validation (CV).
 ###### 
From cou.: 42 Input variables are significant for the calibration model of steel. N-fold cross-validation is a method for model selection in terms of the predictive ability of the models. In machine learning, dataset A was divided into training set B and test set C. In the case that the amount of dataset is not large enough, in order to make full use of the dataset to investigate the performance of the model algorithm, dataset A will be randomly divided into n package, one of the n package is as test set, and the rest of the n − 1 package are the train set each time. In this work, the whole spectral bands and feature spectral bands as input variables were investigated to improve the predictive accuracy of RFR, and then the RFR calibration model for multi-elements in steel was validated by OOB estimation and 10-fold cross validation (CV).
From Soup: 42 Table 3 shows the correlation coefficients ( R ) and root mean square error (RMSE) with different input data of the RFR model by means of OOB estimation and 10-fold CV. As seen in Fig. 2 and Table 2 , due to the fact that most spectral lines of major elements (Si, Mn, Cr, Ni and Cu) in steel are distributed in the range of 220–400 nm, the spectral region of 220–400 nm contains key features of the specific element; hence, the spectra of 220–400 nm are selected as the input data. The calibration model of multiple element analysis in steel was constructed by the whole broadband (220–800 nm) and the spectral feature bands (220–400 nm) as input variables. For the calibration model with the whole LIBS spectra as input variables, RMSE and R were 1.5672 and 0.9285, respectively. Although the whole spectral band has rich spectral information, there is much interference information from other element spectra and the matrix effect. The feature spectrum lines for Si, Mn, Cr, Ni and Cu in steel are distributed in the range of 220–400 nm. Therefore, using feature spectra bands as input variables may improve the performance of the calibration quantitative analysis model to some extent. For the spectral feature bands (220–400 nm), RMSE and the correlation coefficient were 0.4691 and 0.9735, respectively. Compared with using the whole spectral bands as input variables to construct the calibration model, it shows a better performance by using feature spectral bands as input variables. Hence, the feature spectral bands (220–400 nm) were selected as input variables to construct the quantitative analysis calibration model of multiple elements in steel. OOB estimation is also a significant cross validation method in RFR. We compared OOB estimation with 5-fold cross-validation (CV) error rates for predictive ability of steel samples. The predictive error of the OOB estimation was lower than that of 10-fold CV. Unlike cross-validation, the OOB estimates required no additional computing. The OOB validation is convenient for the random forest models owing to the utilization of the bootstrap method of data selection.
 ###### 
From cou.: 43 Table 3 shows the correlation coefficients (R) and root mean square error (RMSE) with different input data of the RFR model by means of OOB estimation and 10-fold CV. As seen in Fig. 2 and Table 2, due to the fact that most spectral lines of major elements (Si, Mn, Cr, Ni and Cu) in steel are distributed in the range of 220–400 nm, the spectral region of 220–400 nm contains key features of the specific element; hence, the spectra of 220–400 nm are selected as the input data. The calibration model of multiple element analysis in steel was constructed by the whole broadband (220–800 nm) and the spectral feature bands (220–400 nm) as input variables. For the calibration model with the whole LIBS spectra as input variables, RMSE and R were 1.5672 and 0.9285, respectively. Although the whole spectral band has rich spectral information, there is much interference information from other element spectra and the matrix effect. The feature spectrum lines for Si, Mn, Cr, Ni and Cu in steel are distributed in the range of 220–400 nm. Therefore, using feature spectra bands as input variables may improve the performance of the calibration quantitative analysis model to some extent. For the spectral feature bands (220–400 nm), RMSE and the correlation coefficient were 0.4691 and 0.9735, respectively. Compared with using the whole spectral bands as input variables to construct the calibration model, it shows a better performance by using feature spectral bands as input variables. Hence, the feature spectral bands (220–400 nm) were selected as input variables to construct the quantitative analysis calibration model of multiple elements in steel. OOB estimation is also a significant cross validation method in RFR. We compared OOB estimation with 5-fold cross-validation (CV) error rates for predictive ability of steel samples. The predictive error of the OOB estimation was lower than that of 10-fold CV. Unlike cross-validation, the OOB estimates required no additional computing. The OOB validation is convenient for the random forest models owing to the utilization of the bootstrap method of data selection.
From Soup: 43 In order to validate the predictive abilities of the calibration RFR model of multi-elements in steel, we compared the RFR method with the PLSR and SVM method. Input variables of these three methods for calibration model are feature spectral bands (220–400 nm). For the calibration model based on PLS, the best latent variable optimized by 5-fold cross-validation is 10. When the PLS model was trained upon the training set, the results were as follows: r 2 = 0.873 and RMSE = 1.760. For the calibration model based on SVM, the best parameters selected by GA (genetic algorithm) were used as input for an epsilon regression SVM with a radial basis function (RBF) kernel. The optimum parameters were set as: penalty parameter C = 97.006 and kernel parameter of RBF g = 0.082. The statistics for 10-fold cross validation inside the training set were r 2 = 0.880 and RMSE = 0.726.
 ###### 
From cou.: 44 In order to validate the predictive abilities of the calibration RFR model of multi-elements in steel, we compared the RFR method with the PLSR and SVM method. Input variables of these three methods for calibration model are feature spectral bands (220–400 nm). For the calibration model based on PLS, the best latent variable optimized by 5-fold cross-validation is 10. When the PLS model was trained upon the training set, the results were as follows: r2 = 0.873 and RMSE = 1.760. For the calibration model based on SVM, the best parameters selected by GA (genetic algorithm) were used as input for an epsilon regression SVM with a radial basis function (RBF) kernel. The optimum parameters were set as: penalty parameter C = 97.006 and kernel parameter of RBF g = 0.082. The statistics for 10-fold cross validation inside the training set were r2 = 0.880 and RMSE = 0.726.
From Soup: 44 Based upon the cross-validation results for all three models, random forest has a better predictive performance than the PLS and SVM models of multiple elements in steel. The same is true for the prediction of the external test set. Table 4 lists the predicted results of multiple elements in steel with PLS, SVM and RFR models. The random forest model was able to predict percentage composition of multiple elements in steel for the test set with r 2 = 0.95 and RMSE = 0.69. As we can see in Table 4 , there is a good linear relationship between the predictive value and the conference value of multiple elements in test samples, and the r 2 of five elements in steel are above 0.9000. Because the concentration of Mn element is greater than that of Si, Cr, Ni and Cu in steel, the Mn element for test steel samples shows the best linear relationship. The concentration of Cu in steel is minor and less than 1.45%; therefore, the quantitative analysis result was affected by strong spectral lines from other elements and the matrix effect. The ability of random forest regression to predict the percentage composition of multiple elements in steel without the training set, in conjunction with the 10-fold and out-of-bag cross-validation statistics, suggests that the model is useful for quantitative analysis of multi-elements in steel.
 ###### 
From cou.: 45 Based upon the cross-validation results for all three models, random forest has a better predictive performance than the PLS and SVM models of multiple elements in steel. The same is true for the prediction of the external test set. Table 4 lists the predicted results of multiple elements in steel with PLS, SVM and RFR models. The random forest model was able to predict percentage composition of multiple elements in steel for the test set with r2 = 0.95 and RMSE = 0.69. As we can see in Table 4, there is a good linear relationship between the predictive value and the conference value of multiple elements in test samples, and the r2 of five elements in steel are above 0.9000. Because the concentration of Mn element is greater than that of Si, Cr, Ni and Cu in steel, the Mn element for test steel samples shows the best linear relationship. The concentration of Cu in steel is minor and less than 1.45%; therefore, the quantitative analysis result was affected by strong spectral lines from other elements and the matrix effect. The ability of random forest regression to predict the percentage composition of multiple elements in steel without the training set, in conjunction with the 10-fold and out-of-bag cross-validation statistics, suggests that the model is useful for quantitative analysis of multi-elements in steel.
From Soup: 45 In this study, a novel method based on LIBS and RF was introduced for the quantitative analysis of multiple elements in steel samples. The prediction results of both training and tested samples demonstrated that the developed RFR model is an effective approach for the multi-element analysis of steel samples. 500 trees and 10 172 random variables were optimized and selected as the best parameters for quantitative multi-element analysis of steel samples. The predictive model for the steel sample contains their chemical composition and percentage content of steel samples. The spectral feature bands (220–400 nm) as input variables combined with RFR for LIBS calibration method proved to be an efficient approach for multiple element analysis in steel. The RFR proposed presented good accuracy ( r 2 = 0.95 and RMSE = 0.69) for the prediction of multiple elements (Si, Mn, Cr, Ni and Cu) in steel. Compared with the predictive result using PLS and SVM, the average predicted error rate of RF is lower than the results by PLS and SVM. Therefore, RFR will become a promising regression method for remote, real-time and in situ analysis in quality supervision and process control in the steel industry.
 ###### 
From cou.: 46 In this study, a novel method based on LIBS and RF was introduced for the quantitative analysis of multiple elements in steel samples. The prediction results of both training and tested samples demonstrated that the developed RFR model is an effective approach for the multi-element analysis of steel samples. 500 trees and 10172 random variables were optimized and selected as the best parameters for quantitative multi-element analysis of steel samples. The predictive model for the steel sample contains their chemical composition and percentage content of steel samples. The spectral feature bands (220–400 nm) as input variables combined with RFR for LIBS calibration method proved to be an efficient approach for multiple element analysis in steel. The RFR proposed presented good accuracy (r2 = 0.95 and RMSE = 0.69) for the prediction of multiple elements (Si, Mn, Cr, Ni and Cu) in steel. Compared with the predictive result using PLS and SVM, the average predicted error rate of RF is lower than the results by PLS and SVM. Therefore, RFR will become a promising regression method for remote, real-time and in situ analysis in quality supervision and process control in the steel industry.
From Soup: 46 This research was financially supported by the National Special Fund for the Development of Major Scientific Instruments and Equipment (no. 2011YQ030113) of China, the National Natural Science Foundation of China (no. 21175106 and no. 21375105), the Research Fund for the Doctoral Program of Higher Education of China (no. 20126101110019), and the NWU Graduate Innovation and Creativity Funds (no. YZZ13020).
 ###### 
From cou.: 47 This research was financially supported by the National Special Fund for the Development of Major Scientific Instruments and Equipment (no. 2011YQ030113) of China, the National Natural Science Foundation of China (no. 21175106 and no. 21375105), the Research Fund for the Doctoral Program of Higher Education of China (no. 20126101110019), and the NWU Graduate Innovation and Creativity Funds (no. YZZ13020).
From Soup: 47 A. Varghese and L. George, Spectrochim. Acta, Part A , 2012, 95 , 46–52 CrossRef CAS PubMed .
 ###### 
From cou.: 48 A. Varghese and L. George, Spectrochim. Acta, Part A, 2012, 95, 46–52 CrossRef CAS PubMed .
From Soup: 48 A. Sengupta, B. Rajeswari, R. M. Kadam and R. Acharya, At. Spectrosc. , 2011, 32 , 200–205 CAS .
 ###### 
From cou.: 49 A. Sengupta, B. Rajeswari, R. M. Kadam and R. Acharya, At. Spectrosc., 2011, 32, 200–205 CAS .
From Soup: 49 M. Marotrao Pande, G. Muxing, R. Dumarey, S. Devisscher and B. Blanpain, ISIJ Int. , 2011, 51 , 1778–1787 CrossRef .
 ###### 
From cou.: 50 M. Marotrao Pande, G. Muxing, R. Dumarey, S. Devisscher and B. Blanpain, ISIJ Int., 2011, 51, 1778–1787 CrossRef .
From Soup: 50 H. Ida and J. Kawai, Anal. Bioanal. Chem. , 2004, 379 , 735–738 CrossRef CAS PubMed .
 ###### 
From cou.: 51 H. Ida and J. Kawai, Anal. Bioanal. Chem., 2004, 379, 735–738 CrossRef CAS PubMed .
From Soup: 51 A. G. Coedo, T. Dorado, I. Padilla and J. C. Fariñas, Talanta , 2007, 71 , 2108–2120 CrossRef CAS PubMed .
 ###### 
From cou.: 52 A. G. Coedo, T. Dorado, I. Padilla and J. C. Fariñas, Talanta, 2007, 71, 2108–2120 CrossRef CAS PubMed .
From Soup: 52 V. K. Ponnusamy and J. Jen, J. Chromatogr. A , 2011, 1218 , 6861–6868 CrossRef CAS PubMed .
 ###### 
From cou.: 53 V. K. Ponnusamy and J. Jen, J. Chromatogr. A, 2011, 1218, 6861–6868 CrossRef CAS PubMed .
From Soup: 53 D. A. Cremers and R. C. Chinni, Appl. Spectrosc. Rev. , 2009, 44 , 457–506 CrossRef CAS .
 ###### 
From cou.: 54 D. A. Cremers and R. C. Chinni, Appl. Spectrosc. Rev., 2009, 44, 457–506 CrossRef CAS .
From Soup: 54 F. J. Fortes and J. J. Laserna, Spectrochim. Acta, Part B , 2010, 65 , 975–990 CrossRef PubMed .
 ###### 
From cou.: 55 F. J. Fortes and J. J. Laserna, Spectrochim. Acta, Part B, 2010, 65, 975–990 CrossRef PubMed .
From Soup: 55 D. W. Hahn and N. Omenetto, Appl. Spectrosc. , 2012, 66 , 347–419 CrossRef CAS PubMed .
 ###### 
From cou.: 56 D. W. Hahn and N. Omenetto, Appl. Spectrosc., 2012, 66, 347–419 CrossRef CAS PubMed .
From Soup: 56 S. J. J. Tsai, S. Y. Chen, Y. S. Chung and P. C. Tseng, Anal. Chem. , 2006, 78 , 7432–7439 CrossRef CAS PubMed .
 ###### 
From cou.: 57 S. J. J. Tsai, S. Y. Chen, Y. S. Chung and P. C. Tseng, Anal. Chem., 2006, 78, 7432–7439 CrossRef CAS PubMed .
From Soup: 57 L. M. Cabalín, A. González, J. Ruiz and J. J. Laserna, Spectrochim. Acta, Part B , 2010, 65 , 680–687 CrossRef PubMed .
 ###### 
From cou.: 58 L. M. Cabalín, A. González, J. Ruiz and J. J. Laserna, Spectrochim. Acta, Part B, 2010, 65, 680–687 CrossRef PubMed .
From Soup: 58 J. Gurell, A. Bengtson, M. Falkenström and B. A. M. Hansson, Spectrochim. Acta, Part B , 2012, 74–75 , 46–50 CrossRef CAS PubMed .
 ###### 
From cou.: 59 J. Gurell, A. Bengtson, M. Falkenström and B. A. M. Hansson, Spectrochim. Acta, Part B, 2012, 74–75, 46–50 CrossRef CAS PubMed .
From Soup: 59 R. Noll, V. Sturm, U. Aydin, D. Eilers, C. Gehlen, M. Höhne, A. Lamott, J. Makowe and J. Vrenegor, Spectrochim. Acta, Part B , 2008, 63 , 1159–1166 CrossRef PubMed .
 ###### 
From cou.: 60 R. Noll, V. Sturm, U. Aydin, D. Eilers, C. Gehlen, M. Höhne, A. Lamott, J. Makowe and J. Vrenegor, Spectrochim. Acta, Part B, 2008, 63, 1159–1166 CrossRef PubMed .
From Soup: 60 D. L. Death, A. P. Cunningham and L. J. Pollard, Spectrochim. Acta, Part B , 2008, 63 , 763–769 CrossRef PubMed .
 ###### 
From cou.: 61 D. L. Death, A. P. Cunningham and L. J. Pollard, Spectrochim. Acta, Part B, 2008, 63, 763–769 CrossRef PubMed .
From Soup: 61 P. Yaroshchyk, D. L. Death and S. J. Spencer, J. Anal. At. Spectrom. , 2012, 27 , 92–98 RSC .
 ###### 
From cou.: 62 P. Yaroshchyk, D. L. Death and S. J. Spencer, J. Anal. At. Spectrom., 2012, 27, 92–98 RSC .
From Soup: 62 R. Noll, H. Bette, A. Brysch, M. Kraushaar, I. Monch, L. Peter and V. Sturm, Spectrochim. Acta, Part B , 2001, 56 , 637–649 CrossRef .
 ###### 
From cou.: 63 R. Noll, H. Bette, A. Brysch, M. Kraushaar, I. Monch, L. Peter and V. Sturm, Spectrochim. Acta, Part B, 2001, 56, 637–649 CrossRef .
From Soup: 63 L. Peter, V. Sturm and R. Noll, Appl. Opt. , 2003, 42 , 6199–6204 CrossRef CAS .
 ###### 
From cou.: 64 L. Peter, V. Sturm and R. Noll, Appl. Opt., 2003, 42, 6199–6204 CrossRef CAS .
From Soup: 64 M. A. Gondal, T. Hussain, Z. H. Yamani and A. H. Bakry, J. Environ. Sci. Health, Part A: Toxic/Hazard. Subst. Environ. Eng. , 2007, 42 , 767–775 CrossRef CAS PubMed .
 ###### 
From cou.: 65 M. A. Gondal, T. Hussain, Z. H. Yamani and A. H. Bakry, J. Environ. Sci. Health, Part A: Toxic/Hazard. Subst. Environ. Eng., 2007, 42, 767–775 CrossRef CAS PubMed .
From Soup: 65 M. A. Khater, Spectrochim. Acta, Part B , 2013, 81 , 1–10 CrossRef CAS PubMed .
 ###### 
From cou.: 66 M. A. Khater, Spectrochim. Acta, Part B, 2013, 81, 1–10 CrossRef CAS PubMed .
From Soup: 66 R. Noll, V. Sturm, U. Aydin, D. Eilers, C. Gehlen, M. Hohne, A. Lamott, J. Makowe and J. Vrenegor, Spectrochim. Acta, Part B , 2008, 63 , 1159–1166 CrossRef PubMed .
 ###### 
From cou.: 67 R. Noll, V. Sturm, U. Aydin, D. Eilers, C. Gehlen, M. Hohne, A. Lamott, J. Makowe and J. Vrenegor, Spectrochim. Acta, Part B, 2008, 63, 1159–1166 CrossRef PubMed .
From Soup: 67 A. Ciucci, M. Corsi, V. Palleschi, S. Rastelli, A. Salvetti and E. Tognoni, Appl. Spectrosc. , 1999, 53 , 960–964 CrossRef CAS .
 ###### 
From cou.: 68 A. Ciucci, M. Corsi, V. Palleschi, S. Rastelli, A. Salvetti and E. Tognoni, Appl. Spectrosc., 1999, 53, 960–964 CrossRef CAS .
From Soup: 68 L. Xu, V. Bulatov, V. V. Gridin and I. Schechter, Anal. Chem. , 1997, 69 , 2103–2108 CrossRef CAS PubMed .
 ###### 
From cou.: 69 L. Xu, V. Bulatov, V. V. Gridin and I. Schechter, Anal. Chem., 1997, 69, 2103–2108 CrossRef CAS PubMed .
From Soup: 69 Z. Wang, J. Feng, L. Li, W. Ni and Z. Li, J. Anal. At. Spectrom. , 2011, 26 , 2175–2182 RSC .
 ###### 
From cou.: 70 Z. Wang, J. Feng, L. Li, W. Ni and Z. Li, J. Anal. At. Spectrom., 2011, 26, 2175–2182 RSC .
From Soup: 70 S. Yao, J. Lu, J. Li, K. Chen, J. Li and M. Dong, J. Anal. At. Spectrom. , 2010, 25 , 1733–1738 RSC .
 ###### 
From cou.: 71 S. Yao, J. Lu, J. Li, K. Chen, J. Li and M. Dong, J. Anal. At. Spectrom., 2010, 25, 1733–1738 RSC .
From Soup: 71 P. Yaroshchyk, D. L. Death and S. J. Spencer, J. Anal. At. Spectrom. , 2012, 27 , 92–98 RSC .
 ###### 
From cou.: 72 P. Yaroshchyk, D. L. Death and S. J. Spencer, J. Anal. At. Spectrom., 2012, 27, 92–98 RSC .
From Soup: 72 M. C. Ortiz, L. Sarabia, A. Jurado-Lopez and M. D. Luque de Castro, Anal. Chim. Acta , 2004, 515 , 151–157 CrossRef CAS PubMed .
 ###### 
From cou.: 73 M. C. Ortiz, L. Sarabia, A. Jurado-Lopez and M. D. Luque de Castro, Anal. Chim. Acta, 2004, 515, 151–157 CrossRef CAS PubMed .
From Soup: 73 V. K. Unnikrishnan, K. S. Choudhari, S. D. Kulkarni, R. Nayak, V. B. Kartha and C. Santhosh, RSC Adv. , 2013, 3 , 25872–25880 RSC .
 ###### 
From cou.: 74 V. K. Unnikrishnan, K. S. Choudhari, S. D. Kulkarni, R. Nayak, V. B. Kartha and C. Santhosh, RSC Adv., 2013, 3, 25872–25880 RSC .
From Soup: 74 D. L. Death, A. P. Cunningham and L. J. Pollard, Spectrochim. Acta, Part B , 2008, 63 , 763–769 CrossRef PubMed .
 ###### 
From cou.: 75 D. L. Death, A. P. Cunningham and L. J. Pollard, Spectrochim. Acta, Part B, 2008, 63, 763–769 CrossRef PubMed .
From Soup: 75 M. R. Dong, J. D. Lu, S. C. Yao, J. Li, J. Y. Li, Z. M. Zhong and W. Y. Lu, J. Anal. At. Spectrom. , 2011, 26 , 2183–2188 RSC .
 ###### 
From cou.: 76 M. R. Dong, J. D. Lu, S. C. Yao, J. Li, J. Y. Li, Z. M. Zhong and W. Y. Lu, J. Anal. At. Spectrom., 2011, 26, 2183–2188 RSC .
From Soup: 76 M. M. Tripathi, K. E. Eseller, F. Y. Yueh and J. P. Singh, Spectrochim. Acta, Part B , 2009, 64 , 1212–1218 CrossRef PubMed .
 ###### 
From cou.: 77 M. M. Tripathi, K. E. Eseller, F. Y. Yueh and J. P. Singh, Spectrochim. Acta, Part B, 2009, 64, 1212–1218 CrossRef PubMed .
From Soup: 77 J. EI Haddad, M. Villot-Kadri, A. Ismaël, G. Gallou, K. Michel, D. Bruyere, V. Laperche, L. Canioni and B. Bousquet, Spectrochim. Acta, Part B , 2013, 79 , 51–57 CrossRef PubMed .
 ###### 
From cou.: 78 J. EI Haddad, M. Villot-Kadri, A. Ismaël, G. Gallou, K. Michel, D. Bruyere, V. Laperche, L. Canioni and B. Bousquet, Spectrochim. Acta, Part B, 2013, 79, 51–57 CrossRef PubMed .
From Soup: 78 J. B. Sirven, B. Bousquet, L. Canioni and L. Sarger, Anal. Chem. , 2006, 78 , 1462–1469 CrossRef CAS PubMed .
 ###### 
From cou.: 79 J. B. Sirven, B. Bousquet, L. Canioni and L. Sarger, Anal. Chem., 2006, 78, 1462–1469 CrossRef CAS PubMed .
From Soup: 79 J. B. Sirven, B. Bousquet, L. Canioni, L. Sarger, S. Tellier, M. Potin-Gautier and I. L. Hecho, Anal. Bioanal. Chem. , 2006, 385 , 256–262 CrossRef CAS PubMed .
 ###### 
From cou.: 80 J. B. Sirven, B. Bousquet, L. Canioni, L. Sarger, S. Tellier, M. Potin-Gautier and I. L. Hecho, Anal. Bioanal. Chem., 2006, 385, 256–262 CrossRef CAS PubMed .
From Soup: 80 P. Inakollu, T. Philip, A. K. Rai, F. Y. Yueh and J. P. Singh, Spectrochim. Acta, Part B , 2009, 64 , 99–104 CrossRef PubMed .
 ###### 
From cou.: 81 P. Inakollu, T. Philip, A. K. Rai, F. Y. Yueh and J. P. Singh, Spectrochim. Acta, Part B, 2009, 64, 99–104 CrossRef PubMed .
From Soup: 81 L. Liang, T. Zhang, K. Wang, H. Tang, X. Yang, X. Zhu, Y. Duan and H. Li, Appl. Opt. , 2014, 53 , 544–552 CrossRef CAS PubMed .
 ###### 
From cou.: 82 L. Liang, T. Zhang, K. Wang, H. Tang, X. Yang, X. Zhu, Y. Duan and H. Li, Appl. Opt., 2014, 53, 544–552 CrossRef CAS PubMed .
From Soup: 82 N. C. Dingari, I. Barman, A. K. Myakalwar, S. P. Tewari and M. Kumar Gundawar, Anal. Chem. , 2012, 84 , 2686–2694 CrossRef CAS PubMed .
 ###### 
From cou.: 83 N. C. Dingari, I. Barman, A. K. Myakalwar, S. P. Tewari and M. Kumar Gundawar, Anal. Chem., 2012, 84, 2686–2694 CrossRef CAS PubMed .
From Soup: 83 J. Cisewski, E. Snyder, J. Hannig and L. Oudejans, J. Chemom. , 2012, 26 , 143–149 CrossRef CAS .
 ###### 
From cou.: 84 J. Cisewski, E. Snyder, J. Hannig and L. Oudejans, J. Chemom., 2012, 26, 143–149 CrossRef CAS .
From Soup: 84 L. Breiman, Mach. Learn. , 2001, 45 , 5–32 CrossRef .
 ###### 
From cou.: 85 L. Breiman, Mach. Learn., 2001, 45, 5–32 CrossRef .
From Soup: 85 J. Remus and K. S. Dunsin, Appl. Opt. , 2012, 51 , B49–B56 CrossRef PubMed .
 ###### 
From cou.: 86 J. Remus and K. S. Dunsin, Appl. Opt., 2012, 51, B49–B56 CrossRef PubMed .
From Soup: 86 V. Vapnik, Statistical Learning Theory , Wiley, New York, 1998 Search PubMed .
 ###### 
From cou.: 87 V. Vapnik, Statistical Learning Theory, Wiley, New York, 1998 Search PubMed .
From Soup: 87 C. C. Chang and C. J. Lin, LIBSVM—A Library for Support Vector Machines , http://www.csie.ntu.edu.tw/%7Ecjlin/libsvm/ Search PubMed .
 ###### 
From cou.: 88 C. C. Chang and C. J. Lin, LIBSVM—A Library for Support Vector Machines, http://www.csie.ntu.edu.tw/%7Ecjlin/libsvm/ Search PubMed.
From Soup: 88 L. Breiman and A. Cutler, Random Forest , http://www.stat.berkeley. edu/∼breiman/RandomForests/cc_home.htm Search PubMed .
 ###### 
From cou.: 89 L. Breiman and A. Cutler, Random Forest, http://www.stat.berkeley. edu/∼breiman/RandomForests/cc_home.htm Search PubMed.
From Soup: 89 A. Liaw and M. Wiener, R News , 2002, 2 , 18–22 Search PubMed .
 ###### 
From cou.: 90 A. Liaw and M. Wiener, R News, 2002, 2, 18–22 Search PubMed .
From Soup: 90 A. Kramida, Yu. Ralchenko and J. Reader, NIST ASD Team, NIST Atomic Spectra Database (ver. 5.0), [online] , National Institute of Standards and Technology, Gaithersburg, MD, 2012, http://physics.nist.gov/asd Search PubMed .
 ###### 
From cou.: 91 A. Kramida, Yu. Ralchenko and J. Reader, NIST ASD Team, NIST Atomic Spectra Database (ver. 5.0), [online], National Institute of Standards and Technology, Gaithersburg, MD, 2012, http://physics.nist.gov/asd Search PubMed .
