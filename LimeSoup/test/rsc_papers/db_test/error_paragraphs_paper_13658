From cou.: 0 Journal
From Soup: 0 J. Anal. At. Spectrom.
 ###### 
From cou.: 1 Multivariate data analysis methods have been used to evaluate single shot spectral data, obtained by laser induced breakdown spectroscopy (LIBS), from ten different biological samples (simulants and possible interferents in Biological Warfare Agent (BWA) detection applications). Spectral data as echellograms (2D CCD images) and extracted 1D spectra were used and the classification performance was studied as the number of input variables was altered. Principal component analysis (PCA) indicated a possibility to separate the samples due to spectral differences, and partial least squares discriminant analysis (PLS-DA) was applied to study the predictability in more detail. For full resolution 1D spectra, a normalization of the data mainly resulted in visual effects in the PCA score-plots without significant effect in predictability by the PLS-DA models, however, normalization improved the predictability if the amount of variables were heavily reduced. A quite strong data (variable) reduction could be performed on both the 1D and 2D data without losing significant predictability. Using similar amounts of variables, the prediction models performed better using the echellograms directly compared to the extracted 1D spectra. The problem of spectral data shift (relative ‘database’ spectra) was also investigated, where already small shifts cause the models to fail. However, after a selection of important variables and allowing certain regions for these variables, the impact of shift on predictability could be reduced.
From Soup: 1 Multivariate data analysis methods have been used to evaluate single shot spectral data, obtained by laser induced breakdown spectroscopy (LIBS), from ten different biological samples (simulants and possible interferents in Biological Warfare Agent (BWA) detection applications). Spectral data as echellograms (2D CCD images) and extracted 1D spectra were used and the classification performance was studied as the number of input variables was altered. Principal component analysis (PCA) indicated a possibility to separate the samples due to spectral differences, and partial least squares discriminant analysis (PLS-DA) was applied to study the predictability in more detail. For full resolution 1D spectra, a normalization of the data mainly resulted in visual effects in the PCA score-plots without significant effect in predictability by the PLS-DA models, however, normalization improved the predictability if the amount of variables were heavily reduced. A quite strong data (variable) reduction could be performed on both the 1D and 2D data without losing significant predictability. Using similar amounts of variables, the prediction models performed better using the echellograms directly compared to the extracted 1D spectra. The problem of spectral data shift (relative ‘database’ spectra) was also investigated, where already small shifts cause the models to fail. However, after a selection of important variables and allowing certain regions for these variables, the impact of shift on predictability could be reduced.
 ###### 
From cou.: 2 Classification methods and models have become increasingly important in various kinds of, e.g., medical, bioinformatical, detection, etc., applications.1–5 In our case, it concerns fast and reliable biodetectors based on optical techniques, for “detect-to-warn” purposes in biodefence applications. As there may only be subtle differences in pathogenic and harmless strains of bacteria, combined with commonly relatively low specificity obtained by optical spectroscopy techniques (for biodefence applications), it is of utmost importance to fully evaluate and use the experimental data extracted as well as optimize the experimental set ups to the specific challenge. However, even if some specificity is lost in rapid “detect to warn” set ups such systems may, for example, be useful to initiate a sampling procedure followed by a more time consuming identification step, e.g., Polymerase Chain Reaction (PCR) analysis or other biochemical reaction steps.
From Soup: 2 Classification methods and models have become increasingly important in various kinds of, e.g. , medical, bioinformatical, detection, etc. , applications. 1–5 In our case, it concerns fast and reliable biodetectors based on optical techniques, for “detect-to-warn” purposes in biodefence applications. As there may only be subtle differences in pathogenic and harmless strains of bacteria, combined with commonly relatively low specificity obtained by optical spectroscopy techniques (for biodefence applications), it is of utmost importance to fully evaluate and use the experimental data extracted as well as optimize the experimental set ups to the specific challenge. However, even if some specificity is lost in rapid “detect to warn” set ups such systems may, for example, be useful to initiate a sampling procedure followed by a more time consuming identification step, e.g. , Polymerase Chain Reaction (PCR) analysis or other biochemical reaction steps.
 ###### 
From cou.: 3 Different techniques based on optical spectroscopy are promising as fast detect-to-warn systems and are being continuously improved by extensive research and development efforts (and commercial bioaerosol detection instruments already exist).6 Examples of commonly used techniques in biodetection are, e.g., Laser Induced Fluorescence (LIF),7–9 Flame Emission Spectroscopy (FES),10 Raman spectroscopy,11,12 and Spark or Laser Induced Breakdown Spectroscopy (SIBS/LIBS),9,13–15 to mention a few. However, to further reduce the false positive/negative rates in alarm algorithms based on data acquired by the optical techniques mentioned above, some gain can be expected by optimizing which variables to monitor. Such optimization will in many cases increase the sampling rate (to ensure that all samples of interest are analyzed) and also result in better warning algorithms.
From Soup: 3 Different techniques based on optical spectroscopy are promising as fast detect-to-warn systems and are being continuously improved by extensive research and development efforts (and commercial bioaerosol detection instruments already exist). 6 Examples of commonly used techniques in biodetection are, e.g. , Laser Induced Fluorescence (LIF), 7–9 Flame Emission Spectroscopy (FES), 10 Raman spectroscopy, 11,12 and Spark or Laser Induced Breakdown Spectroscopy (SIBS/LIBS), 9,13–15 to mention a few. However, to further reduce the false positive/negative rates in alarm algorithms based on data acquired by the optical techniques mentioned above, some gain can be expected by optimizing which variables to monitor. Such optimization will in many cases increase the sampling rate (to ensure that all samples of interest are analyzed) and also result in better warning algorithms.
 ###### 
From cou.: 4 In this study, LIBS was used as spectroscopic technique to acquire spectral data from ten different biological samples representing simulants for, e.g., anthrax bacteria, but also a few common background interferent bio-materials such as pollen. Several reports have been published on the use of LIBS for classification on a variety of sample sets, e.g., explosives,16 bio-material,14,15,17 chemical warfare simulants/agents,18 and minerals.19 However, the motivation of the work presented here was mainly to investigate the effect of reducing the number and/or the resolution of sampled data points in the acquired data when performing classification on the sample data set. In addition, the influence of shift of data on the classification performance as well as simple ways to mitigate these problems are also investigated. The results presented herein are also expected to be applicable in other areas of, e.g., spectroscopy, chromatography, mass spectrometry, etc., where large data sets may slow down a “detection/warning” event or reducing the performance of a classification algorithm.
From Soup: 4 In this study, LIBS was used as spectroscopic technique to acquire spectral data from ten different biological samples representing simulants for, e.g. , anthrax bacteria, but also a few common background interferent bio-materials such as pollen. Several reports have been published on the use of LIBS for classification on a variety of sample sets, e.g. , explosives, 16 bio-material, 14,15,17 chemical warfare simulants/agents, 18 and minerals. 19 However, the motivation of the work presented here was mainly to investigate the effect of reducing the number and/or the resolution of sampled data points in the acquired data when performing classification on the sample data set. In addition, the influence of shift of data on the classification performance as well as simple ways to mitigate these problems are also investigated. The results presented herein are also expected to be applicable in other areas of, e.g. , spectroscopy, chromatography, mass spectrometry, etc. , where large data sets may slow down a “detection/warning” event or reducing the performance of a classification algorithm.
 ###### 
From cou.: 5 Briefly, LIBS is an atomic emission spectroscopy technique where the sample is converted to a plasma by a high energy laser pulse and optical emission spectroscopy is then performed as the plasma cools down, allowing to measure characteristic emission from the constituents of the sample. For more in depth theory, see e.g., ref. 20 and references therein. In our set-up, an echelle type spectrograph was used to acquire the optical emission spectra, which resulted in that quite a lot of data (1 Mpixel camera in our case) needs to be read and transferred to a computer. By software conversion of the echellogram into an intensity vs. wavelength 1D spectrum, the ∼106 data points are reduced to 4.8 × 104 (∼5%). In addition, the use of full resolution images limits the overall sampling frequency in this particular set-up.
From Soup: 5 Briefly, LIBS is an atomic emission spectroscopy technique where the sample is converted to a plasma by a high energy laser pulse and optical emission spectroscopy is then performed as the plasma cools down, allowing to measure characteristic emission from the constituents of the sample. For more in depth theory, see e.g. , ref. 20 and references therein. In our set-up, an echelle type spectrograph was used to acquire the optical emission spectra, which resulted in that quite a lot of data (1 Mpixel camera in our case) needs to be read and transferred to a computer. By software conversion of the echellogram into an intensity vs. wavelength 1D spectrum, the ∼10 6 data points are reduced to 4.8 × 10 4 (∼5%). In addition, the use of full resolution images limits the overall sampling frequency in this particular set-up.
 ###### 
From cou.: 6 In the current work, LIBS spectra were acquired from pressed pellets of the different biomaterials, to increase the reproducibility of the single shot spectra and thus having better control of the input data used in the classification models. In a real life bioaerosol monitoring system, however, there will be a need to analyze single μm-sized particles21 and using that data which in general will be of worse quality, compared to spectra from bulk material, in terms of, e.g., signal-to-noise, and a strong variation of relative line intensities is also expected.22
From Soup: 6 In the current work, LIBS spectra were acquired from pressed pellets of the different biomaterials, to increase the reproducibility of the single shot spectra and thus having better control of the input data used in the classification models. In a real life bioaerosol monitoring system, however, there will be a need to analyze single μm-sized particles 21 and using that data which in general will be of worse quality, compared to spectra from bulk material, in terms of, e.g. , signal-to-noise, and a strong variation of relative line intensities is also expected. 22
 ###### 
From cou.: 7 Multivariate data analysis methods, such as Principal Component Analysis (PCA)23,24 and Partial Least Squares (PLS) regression,25 were applied on the obtained spectral data sets, before and after various manipulation and variable reduction steps, and the results (mostly in terms of classification performance), are discussed and related to the data optimization.
From Soup: 7 Multivariate data analysis methods, such as Principal Component Analysis (PCA) 23,24 and Partial Least Squares (PLS) regression, 25 were applied on the obtained spectral data sets, before and after various manipulation and variable reduction steps, and the results (mostly in terms of classification performance), are discussed and related to the data optimization.
 ###### 
From cou.: 8 Another common problem in, e.g., spectroscopy is a possible drift/shift of the captured data and such shifts usually rapidly degrades the performance of classification models. Here, we investigate the possibility to define windows (regions of interest), where important parameters are expected, and using the peak values from each window for building classification models. The measures of classification performance are compared relative to each other, i.e., no specific value is set as a threshold for any given model to be acceptable, as this will depend on the given application and the acceptable rates of errors (e.g., false alarms).
From Soup: 8 Another common problem in, e.g. , spectroscopy is a possible drift/shift of the captured data and such shifts usually rapidly degrades the performance of classification models. Here, we investigate the possibility to define windows (regions of interest), where important parameters are expected, and using the peak values from each window for building classification models. The measures of classification performance are compared relative to each other, i.e. , no specific value is set as a threshold for any given model to be acceptable, as this will depend on the given application and the acceptable rates of errors ( e.g. , false alarms).
 ###### 
From cou.: 9 Finally, the present study only addresses ten substances, even though some very similar, and will of course be a much more favorable task compared to most real life scenarios where the samples of interest are likely to occur within a harmless background of complex composition.
From Soup: 9 Finally, the present study only addresses ten substances, even though some very similar, and will of course be a much more favorable task compared to most real life scenarios where the samples of interest are likely to occur within a harmless background of complex composition.
 ###### 
From cou.: 10 A pulsed (duration ∼5 ns and energy of 40 mJ) Nd:YAG laser (λ = 1064 nm) was focused to a spot diameter of about 300 μm onto pellets of different biomaterials. (Dry samples were compressed into d = 12.7 mm pellets using a compacting pressure of ∼0.2 GPa. No binder was used.) The ablation was performed in standard ambient conditions and the laser induced plasma plume was imaged onto a d = 550 μm optical fiber connected to an echelle spectrograph equipped with an intensified CCD detector with a pixel resolution of 1024 × 1024 13.5 μm square sized pixels. Spectral data was analyzed either as echellograms (the intensity distribution directly from the CCD chip, see Fig. 1) or from extracted 1D (intensity vs. wavelength) spectra. The 1D spectra were extracted, after initial spectral calibration by an HgAr lamp, from the echellograms by a transfer function which was performed by the control software (Andor Solis). Here, a wavelength window of ∼200–900 nm with a spectral resolution λ/Δλ > 4000 was obtained, and presented as ∼24000 data pairs. Converting an echellogram to 1D spectra may unfortunately introduce unwanted features, e.g., due to “cross-talk” between the diffraction orders. In addition, a stable temperature of the hardware, relative to calibration temperature, is of great importance as a change in only a few degrees will shift the position of the echellogram on the CCD chip because of temperature dependent optical properties of certain components. (Different algorithms are implemented in the software to reduce these errors.)
From Soup: 10 A pulsed (duration ∼5 ns and energy of 40 mJ) Nd:YAG laser ( λ = 1064 nm) was focused to a spot diameter of about 300 μm onto pellets of different biomaterials. (Dry samples were compressed into d = 12.7 mm pellets using a compacting pressure of ∼0.2 GPa. No binder was used.) The ablation was performed in standard ambient conditions and the laser induced plasma plume was imaged onto a d = 550 μm optical fiber connected to an echelle spectrograph equipped with an intensified CCD detector with a pixel resolution of 1024 × 1024 13.5 μm square sized pixels. Spectral data was analyzed either as echellograms (the intensity distribution directly from the CCD chip, see Fig. 1 ) or from extracted 1D (intensity vs. wavelength) spectra. The 1D spectra were extracted, after initial spectral calibration by an HgAr lamp, from the echellograms by a transfer function which was performed by the control software (Andor Solis). Here, a wavelength window of ∼200–900 nm with a spectral resolution λ /Δ λ > 4000 was obtained, and presented as ∼24 000 data pairs. Converting an echellogram to 1D spectra may unfortunately introduce unwanted features, e.g. , due to “cross-talk” between the diffraction orders. In addition, a stable temperature of the hardware, relative to calibration temperature, is of great importance as a change in only a few degrees will shift the position of the echellogram on the CCD chip because of temperature dependent optical properties of certain components. (Different algorithms are implemented in the software to reduce these errors.)
 ###### 
From cou.: 11 Compressed pellets, consisting of selected simulants for biological warfare agents and possible interferents (pollen) were used simply to improve the spectral reproducibility and quality, compared to, e.g., powder samples or single bioaerosol particles. To minimize the effects of drift in temperature and other experimental conditions, the whole data set was obtained during one trial. This data set contained observations from ten samples which each provided the data set with >100 observations, for a total of 1055 observations. To include an internal variation within a sample, the ∼100 observations were obtained from two different locations on the sample. The samples, which for convenience were numbered 1 through 10, were: three strains of Bacillus atrophaeus (BG), four strains of Bacillus thuringiensis (BT), Betula pendula (birch) pollen, Pinus sylvestris (pine) pollen, and ovalbumin, see also Table 1. The BG samples are essentially the same microorganism that have been cultivated and prepared in different ways. This is also the case for the BT samples.
From Soup: 11 Compressed pellets, consisting of selected simulants for biological warfare agents and possible interferents (pollen) were used simply to improve the spectral reproducibility and quality, compared to, e.g. , powder samples or single bioaerosol particles. To minimize the effects of drift in temperature and other experimental conditions, the whole data set was obtained during one trial. This data set contained observations from ten samples which each provided the data set with >100 observations, for a total of 1055 observations. To include an internal variation within a sample, the ∼100 observations were obtained from two different locations on the sample. The samples, which for convenience were numbered 1 through 10, were: three strains of Bacillus atrophaeus (BG), four strains of Bacillus thuringiensis (BT), Betula pendula (birch) pollen, Pinus sylvestris (pine) pollen, and ovalbumin, see also Table 1 . The BG samples are essentially the same microorganism that have been cultivated and prepared in different ways. This is also the case for the BT samples.
 ###### 
From cou.: 12 From spectral measurements one may acquire a large amount of data and in a lot of applications the number of variables greatly outnumbers the observations, making multivariate data analysis a well suited method. One suitable method that can provide information if there are similarities between spectra is Principal Component Analysis (PCA). In addition, Partial Least Squares Discriminant Analysis (PLS-DA) was used to evaluate how different manipulation of 1D spectra and 2D echellograms affect the predictability compared to full resolution data.
From Soup: 12 From spectral measurements one may acquire a large amount of data and in a lot of applications the number of variables greatly outnumbers the observations, making multivariate data analysis a well suited method. One suitable method that can provide information if there are similarities between spectra is Principal Component Analysis (PCA). In addition, Partial Least Squares Discriminant Analysis (PLS-DA) was used to evaluate how different manipulation of 1D spectra and 2D echellograms affect the predictability compared to full resolution data.
 ###### 
From cou.: 13 1. Principal component analysis (PCA).
From Soup: 13 1. Principal component analysis (PCA).
 ###### 
From cou.: 14 PCA is a projection method that transforms a data set onto a new variable space. The objective of the PCA algorithm is to find a linear transformation for the data to achieve a diagonal covariance (or correlation) matrix using eigenvalue decomposition. The components (eigenvectors) in the new variable space maximizes the variance in the original data set with as few variables as possible. These new variables are called principal components (PCs) and are constructed in such a way that the first component is put in the direction with the largest variance in the original data set, the second PC is put in the direction with the second largest amount of variance, under the constraint that it has to be orthogonal to the first PC, and so on. Mathematically, PCA can be written aswhere X is the original (measured) data matrix, T the scores and P the loadings. The scores matrix consists of the vectors which are referred to as the principal components and are the representation of the original data in principal component space. The loadings matrix contain the weights, or coefficients, which transform the data into principal component space under the restrictions mentioned above. The E matrix consists of error terms.26–28 Typically, the resulting PCA is illustrated as a “score plot” where data with similar properties will form a point swarm while other similar data will form other, separate point swarms. When using PCA to build models for prediction, it is common to include as many PCs that account for 80 or 90% of the variance in the original data set. (The cumulative variance is straightforward to calculate as the PCs are orthogonal and hence uncorrelated.) Any chosen level of explained variance will thus work as a threshold, where one assumes that the included PCs contain useful information from the signal and the other (higher) PCs are assumed to contain mostly noise and should therefore be omitted. The number of components chosen are important (not only in PCA) in order to avoid a model which is over- or under-fitted, i.e., missing relevant information from too few components or introducing irrelevant information (noise) from too many components. In this study, PCA was only used for visualization purposes as it is a fast and simple approach to get information about how different data treatments affect the data set. For classification modeling purposes, there are more powerful methods, e.g., partial least squares regression to analyze and assess the data.
From Soup: 14 PCA is a projection method that transforms a data set onto a new variable space. The objective of the PCA algorithm is to find a linear transformation for the data to achieve a diagonal covariance (or correlation) matrix using eigenvalue decomposition. The components (eigenvectors) in the new variable space maximizes the variance in the original data set with as few variables as possible. These new variables are called principal components (PCs) and are constructed in such a way that the first component is put in the direction with the largest variance in the original data set, the second PC is put in the direction with the second largest amount of variance, under the constraint that it has to be orthogonal to the first PC, and so on. Mathematically, PCA can be written as where X is the original (measured) data matrix, T the scores and P the loadings. The scores matrix consists of the vectors which are referred to as the principal components and are the representation of the original data in principal component space. The loadings matrix contain the weights, or coefficients, which transform the data into principal component space under the restrictions mentioned above. The E matrix consists of error terms. 26–28 Typically, the resulting PCA is illustrated as a “score plot” where data with similar properties will form a point swarm while other similar data will form other, separate point swarms. When using PCA to build models for prediction, it is common to include as many PCs that account for 80 or 90% of the variance in the original data set. (The cumulative variance is straightforward to calculate as the PCs are orthogonal and hence uncorrelated.) Any chosen level of explained variance will thus work as a threshold, where one assumes that the included PCs contain useful information from the signal and the other (higher) PCs are assumed to contain mostly noise and should therefore be omitted. The number of components chosen are important (not only in PCA) in order to avoid a model which is over- or under-fitted, i.e. , missing relevant information from too few components or introducing irrelevant information (noise) from too many components. In this study, PCA was only used for visualization purposes as it is a fast and simple approach to get information about how different data treatments affect the data set. For classification modeling purposes, there are more powerful methods, e.g. , partial least squares regression to analyze and assess the data.
 ###### 
From cou.: 15 2. Partial least squares regression (PLS).
From Soup: 15 2. Partial least squares regression (PLS).
 ###### 
From cou.: 16 PLS is similar to PCA since it is built by a new set of components, or in this case referred to as factors, and it works especially well when the data has a lot of variables (i.e., when the number of variables is much larger than the number of observations). What makes PLS differ from PCA is that it is a supervised method, that is, there exist a response Y to the data matrix X where the general model is constructed aswhere T and U are the scores and P and Q are the loadings of X and Y respectively. The matrices E and F are error terms. The objective of PLS is to find the matrix β to be able to predict new observations in X. The PLS algorithm26–28 uses the decompositions of X and Y to maximize the covariance between the score matrices T and U. In this study, a special case of PLS was used, Partial Least Squares Discriminant Analysis (PLS-DA), which uses a binary response matrix Y.
From Soup: 16 PLS is similar to PCA since it is built by a new set of components, or in this case referred to as factors, and it works especially well when the data has a lot of variables ( i.e. , when the number of variables is much larger than the number of observations). What makes PLS differ from PCA is that it is a supervised method, that is, there exist a response Y to the data matrix X where the general model is constructed as where T and U are the scores and P and Q are the loadings of X and Y respectively. The matrices E and F are error terms. The objective of PLS is to find the matrix β to be able to predict new observations in X . The PLS algorithm 26–28 uses the decompositions of X and Y to maximize the covariance between the score matrices T and U . In this study, a special case of PLS was used, Partial Least Squares Discriminant Analysis (PLS-DA), which uses a binary response matrix Y .
 ###### 
From cou.: 17 In a supervised model, one also has the benefit of getting a quality measure. In PLS the Q2 value is a measure of how well a model can predict new data. This value is obtained from cross validation, i.e., a method where you create one calibration set and one test set from your data. The calibration set contains all but one observation which instead is put in the test set. The model is then built on the calibration set and tested by predicting the single observation from the test set. The procedure is repeated until all observations have been put in the test set once. The Q2 value is defined as
From Soup: 17 In a supervised model, one also has the benefit of getting a quality measure. In PLS the Q 2 value is a measure of how well a model can predict new data. This value is obtained from cross validation, i.e. , a method where you create one calibration set and one test set from your data. The calibration set contains all but one observation which instead is put in the test set. The model is then built on the calibration set and tested by predicting the single observation from the test set. The procedure is repeated until all observations have been put in the test set once. The Q 2 value is defined as
 ###### 
From cou.: 18 ŷ
From Soup: 18 ŷ
 ###### 
From cou.: 19 i
From Soup: 19 ( i ) i
 ###### 
From cou.: 20 i
From Soup: 20 y
 ###### 
From cou.: 21 y
From Soup: 21 i
 ###### 
From cou.: 22 i
From Soup: 22 Q
 ###### 
From cou.: 23 Q
From Soup: 23 2
 ###### 
From cou.: 24 2
From Soup: 24 e.g.
 ###### 
From cou.: 25 e.g.
From Soup: 25 29
 ###### 
From cou.: 26 29
From Soup: 26 Q
 ###### 
From cou.: 27 Q
From Soup: 27 2
 ###### 
From cou.: 28 2
From Soup: 28 i.e.
 ###### 
From cou.: 29 i.e.
From Soup: 29 Q
 ###### 
From cou.: 30 Q
From Soup: 30 2
 ###### 
From cou.: 31 2
From Soup: 31 n +1
 ###### 
From cou.: 32 n
From Soup: 32 Q
 ###### 
From cou.: 33 Q
From Soup: 33 2
 ###### 
From cou.: 34 2
From Soup: 34 n
 ###### 
From cou.: 35 n
From Soup: 35 Q
 ###### 
From cou.: 36 Q
From Soup: 36 2
 ###### 
From cou.: 37 2
From Soup: 37 Q
 ###### 
From cou.: 38 Q
From Soup: 38 2
 ###### 
From cou.: 39 2
From Soup: 39 30
 ###### 
From cou.: 40 30
From Soup: 40 As mentioned earlier, to obtain 1D spectra in our echelle spectrograph, the whole CCD image has to be read out at full resolution, after which the data is transferred to a computer where a software performs the extraction of the 1D spectrum. In this process, from a practical point of view, there would be little gain in overall sampling rate for our current experimental set-up to reduce the number of variables fed into a prediction model (except some improvement in computational time) if using these extracted 1D spectra for building prediction models. However, some studies were still performed on the extracted 1D spectra, such as reducing the amount of variables and normalization influence on predictability via PLS-DA. Here, the spectral data were normalized to unity. Instead of reducing the variables fed into the models via multivariate selection techniques, 31 the simple assumption that useful information is found where peaks are observed in the spectra was used, i.e. , the significant variables were reduced by an successively increased threshold using the median of the standard deviation. Data were incrementally reduced by only including variables with standard deviation s ≥ 2 N , where is the median standard deviation and N = 0, 1, 2, …, 8. In addition, reduction of variables was also performed by only include the maximum value inside 78 and 16 wavelength windows (Δ λ = ±0.26 nm) centered around the strongest peaks found from normalized and averaged spectra.
 ###### 
From cou.: 41 As mentioned earlier, to obtain 1D spectra in our echelle spectrograph, the whole CCD image has to be read out at full resolution, after which the data is transferred to a computer where a software performs the extraction of the 1D spectrum. In this process, from a practical point of view, there would be little gain in overall sampling rate for our current experimental set-up to reduce the number of variables fed into a prediction model (except some improvement in computational time) if using these extracted 1D spectra for building prediction models. However, some studies were still performed on the extracted 1D spectra, such as reducing the amount of variables and normalization influence on predictability via PLS-DA. Here, the spectral data were normalized to unity. Instead of reducing the variables fed into the models via multivariate selection techniques,31 the simple assumption that useful information is found where peaks are observed in the spectra was used, i.e., the significant variables were reduced by an successively increased threshold using the median of the standard deviation. Data were incrementally reduced by only including variables with standard deviation s ≥ 2N, where is the median standard deviation and N = 0, 1, 2, …, 8. In addition, reduction of variables was also performed by only include the maximum value inside 78 and 16 wavelength windows (Δλ = ±0.26 nm) centered around the strongest peaks found from normalized and averaged spectra.
From Soup: 41 As the step from image (or echellogram) to a 1D intensity vs. wavelength spectrum seems unnecessary for our purposes (especially as errors may be introduced), we also used PLS-DA models directly on the CCD data. Here, there are possibilities to significantly reduce data read-out and transfer times by binning the pixels and/or cropping the image. (Again, to further reduce the number of variables, pixels were chosen statistically as described above.) As the diffraction orders in the echellogram used to extract spectra are found in an area covering approximately half of the CCD area, it was also investigated if only the variables from that part were important for a model. Creating models using PLS-DA on different binned pixels, cropped images, variable reduction and comparing the Q 2 values results in a relative measure of how much is lost (if any) in predictability compared to what could be gained in data reduction. Similar as for the 1D spectra, using pixel windows and its largest pixel value was also studied on the images. This method was then applied to shifted images (which would simulate, e.g. , thermal drift of the echellogram relative its calibration position). All image manipulation, such as cropping, binning and shifts, were performed by the Image Reduction and Analysis Facility (IRAF) software. 32
 ###### 
From cou.: 42 As the step from image (or echellogram) to a 1D intensity vs. wavelength spectrum seems unnecessary for our purposes (especially as errors may be introduced), we also used PLS-DA models directly on the CCD data. Here, there are possibilities to significantly reduce data read-out and transfer times by binning the pixels and/or cropping the image. (Again, to further reduce the number of variables, pixels were chosen statistically as described above.) As the diffraction orders in the echellogram used to extract spectra are found in an area covering approximately half of the CCD area, it was also investigated if only the variables from that part were important for a model. Creating models using PLS-DA on different binned pixels, cropped images, variable reduction and comparing the Q2 values results in a relative measure of how much is lost (if any) in predictability compared to what could be gained in data reduction. Similar as for the 1D spectra, using pixel windows and its largest pixel value was also studied on the images. This method was then applied to shifted images (which would simulate, e.g., thermal drift of the echellogram relative its calibration position). All image manipulation, such as cropping, binning and shifts, were performed by the Image Reduction and Analysis Facility (IRAF) software.32
From Soup: 42 Both the extracted 1D spectra and the echellograms (2D images) were analyzed via multivariate data methods. The main purpose was to see how different types of data reduction would affect prediction performance, as it may open up possibilities to reduce data read out and transfer times. Of course, in our particular set-up, a reduction of the number of variables in the 1D spectra will not reduce the acquisition time. However, analysis was performed on that type of data as it might be useful for other set-ups and applications. A shift in the data relative “calibrated” data was also introduced and its implication on classification performance was studied.
 ###### 
From cou.: 43 Both the extracted 1D spectra and the echellograms (2D images) were analyzed via multivariate data methods. The main purpose was to see how different types of data reduction would affect prediction performance, as it may open up possibilities to reduce data read out and transfer times. Of course, in our particular set-up, a reduction of the number of variables in the 1D spectra will not reduce the acquisition time. However, analysis was performed on that type of data as it might be useful for other set-ups and applications. A shift in the data relative “calibrated” data was also introduced and its implication on classification performance was studied.
From Soup: 43 Average spectra from the different bio-samples are shown in Fig. 2 . Even though the close similarities of some of the samples, see Table 1 , differences can be spotted by the naked eye which suggests that a prediction model could perform well (especially considering that, e.g. , a PLS-DA model only has the ten different samples to consider). However, in a prediction model, single shot spectra will be used, where the signal-to-noise ratio is significantly reduced compared to the average from ∼100 spectra depicted in Fig. 2 .
 ###### 
From cou.: 44 Average spectra from the different bio-samples are shown in Fig. 2. Even though the close similarities of some of the samples, see Table 1, differences can be spotted by the naked eye which suggests that a prediction model could perform well (especially considering that, e.g., a PLS-DA model only has the ten different samples to consider). However, in a prediction model, single shot spectra will be used, where the signal-to-noise ratio is significantly reduced compared to the average from ∼100 spectra depicted in Fig. 2.
From Soup: 44 As a first step, the separability is visualized by PCA score-plots, see Fig. 3 . Here, as examples, score-plots from the untreated (raw single shot spectra), normalized spectra (to reduce the impact of intensity variations), and spectra where the number of variables have been reduced to only 78 by selecting the highest intensity peaks (after normalization) and defining spectral windows (Δ λ = ±0.26 nm) from which the highest intensity value was taken. Each sample is labeled with its unique color and marker combination, however, similar types of simulants have the same color in the plot. As can be seen, each sample creates a cluster (already when considering only the first two components) which shows that there are differences between samples but similarities in spectra within the same sample. When normalizing each spectrum to unity, the groups form smaller clusters, see Fig. 3b . That is, the intensity variation between (and within) each sample group contributes to the variance in the first principal components and may thus affect the classification model. In Fig. 3c , the number of input variables were dramatically reduced by selecting the largest value from normalized spectra within 78 wavelength windows where the strongest peaks over an arbitrary threshold was found when analyzing all spectra. Even this heavy reduction in input variables (from ∼24 000 to 78) still show separate clusters. (As expected, this result confirms that the peaks in the LIBS spectra contain the important information.) Finally, the explained variance decreased after reducing the intensity variation (by normalization) and increased as the number of variables decreased (and only contained strongest peak data). Considering the larger sample subgroups (three BG samples, four BT, two pollen, and ovalbumin) one might observe some grouping, even if only the first two components are visualized here, suggesting spectral similarities, however, no analysis in that direction will be presented in this work.
 ###### 
From cou.: 45 As a first step, the separability is visualized by PCA score-plots, see Fig. 3. Here, as examples, score-plots from the untreated (raw single shot spectra), normalized spectra (to reduce the impact of intensity variations), and spectra where the number of variables have been reduced to only 78 by selecting the highest intensity peaks (after normalization) and defining spectral windows (Δλ = ±0.26 nm) from which the highest intensity value was taken. Each sample is labeled with its unique color and marker combination, however, similar types of simulants have the same color in the plot. As can be seen, each sample creates a cluster (already when considering only the first two components) which shows that there are differences between samples but similarities in spectra within the same sample. When normalizing each spectrum to unity, the groups form smaller clusters, see Fig. 3b. That is, the intensity variation between (and within) each sample group contributes to the variance in the first principal components and may thus affect the classification model. In Fig. 3c, the number of input variables were dramatically reduced by selecting the largest value from normalized spectra within 78 wavelength windows where the strongest peaks over an arbitrary threshold was found when analyzing all spectra. Even this heavy reduction in input variables (from ∼24000 to 78) still show separate clusters. (As expected, this result confirms that the peaks in the LIBS spectra contain the important information.) Finally, the explained variance decreased after reducing the intensity variation (by normalization) and increased as the number of variables decreased (and only contained strongest peak data). Considering the larger sample subgroups (three BG samples, four BT, two pollen, and ovalbumin) one might observe some grouping, even if only the first two components are visualized here, suggesting spectral similarities, however, no analysis in that direction will be presented in this work.
From Soup: 45 Results from PLS-DA prediction models of the 1D spectra are shown in Fig. 4 where Q 2 values are plotted as function of number of variables. Only a small decrease in Q 2 is observed while reducing the number of input variables to ∼500. Below that value, however, a steep decrease can be seen. The number of factors used in the calibration models and cross validation to obtain the plotted Q 2 values were chosen in analogy with the pre-defined rule of significance (see subsection II B 2). Normalizing the 1D spectra does not influence the predictability much even though a quite large difference was observed in the PCA score plots, see Fig. 3 . Interestingly, a large increase in Q 2 was observed if the highest value inside predefined wavelength windows were used to create the model. For example, if only the 78 most intense peaks, within the predefined windows, were used in the PLS-DA, Q 2 increased from 0.57 to about 0.71 compared to selecting input variables by their variance as explained in subsection II C.
 ###### 
From cou.: 46 Results from PLS-DA prediction models of the 1D spectra are shown in Fig. 4 where Q2 values are plotted as function of number of variables. Only a small decrease in Q2 is observed while reducing the number of input variables to ∼500. Below that value, however, a steep decrease can be seen. The number of factors used in the calibration models and cross validation to obtain the plotted Q2 values were chosen in analogy with the pre-defined rule of significance (see subsection II B 2). Normalizing the 1D spectra does not influence the predictability much even though a quite large difference was observed in the PCA score plots, see Fig. 3. Interestingly, a large increase in Q2 was observed if the highest value inside predefined wavelength windows were used to create the model. For example, if only the 78 most intense peaks, within the predefined windows, were used in the PLS-DA, Q2 increased from 0.57 to about 0.71 compared to selecting input variables by their variance as explained in subsection II C.
From Soup: 46 Another way to visualize the performance of the model is to make boxplots of values obtained from test and training sets using PLS models. Here, the model is built on the training set, randomly selected from 2/3 of the observations, and then tested with the remaining third of the observations and examples of the prediction can be seen in Fig. 5 . The red line inside the box corresponds to the median value of the predictions and the boundaries of the box are the 25th and 75th percentiles and will thus include 50% of the observations. The whiskers extend to the most extreme values considered not to be outliers by the algorithm and the outliers are plotted separately (in this case an outlier was defined as an observation lying outside ±2.7 σ , where σ is the standard deviation of the distribution). Since PLS-DA is used, the ideal value of a true positive prediction is one and the ideal value of a true negative prediction is zero, so preferably, the boxes in the different boxplots should be centered on these two values as tight as possible. As seen in Fig. 5 (lower panels), the negative predictions are in general rather well centered around zero in every sample. The positive predictions, however, have a bit larger boxes and extended whiskers and for the model built on 78 variables and 11 factors according to the rule of significance ( Fig. 5b ), the boxes have also moved down to lower values, compared to the model built on normalized full spectra, for most samples. However, of importance is that the boxes of the positive and negative predictions do not overlap, which they do not except maybe for sample 8. As mentioned earlier, Fig. 5 is a visual representation of the performance of the model. To check statistically if the two prediction groups are separate and independent a two sample t -test was used, where the tested null hypothesis was that the positive and negative predictions come from distributions with equal mean. The test confirmed that the null hypothesis could be rejected and that positive and negative predictions are independent distributions at a 5% significance level, see also Table 2 .
 ###### 
From cou.: 47 Another way to visualize the performance of the model is to make boxplots of values obtained from test and training sets using PLS models. Here, the model is built on the training set, randomly selected from 2/3 of the observations, and then tested with the remaining third of the observations and examples of the prediction can be seen in Fig. 5. The red line inside the box corresponds to the median value of the predictions and the boundaries of the box are the 25th and 75th percentiles and will thus include 50% of the observations. The whiskers extend to the most extreme values considered not to be outliers by the algorithm and the outliers are plotted separately (in this case an outlier was defined as an observation lying outside ±2.7σ, where σ is the standard deviation of the distribution). Since PLS-DA is used, the ideal value of a true positive prediction is one and the ideal value of a true negative prediction is zero, so preferably, the boxes in the different boxplots should be centered on these two values as tight as possible. As seen in Fig. 5 (lower panels), the negative predictions are in general rather well centered around zero in every sample. The positive predictions, however, have a bit larger boxes and extended whiskers and for the model built on 78 variables and 11 factors according to the rule of significance (Fig. 5b), the boxes have also moved down to lower values, compared to the model built on normalized full spectra, for most samples. However, of importance is that the boxes of the positive and negative predictions do not overlap, which they do not except maybe for sample 8. As mentioned earlier, Fig. 5 is a visual representation of the performance of the model. To check statistically if the two prediction groups are separate and independent a two sample t-test was used, where the tested null hypothesis was that the positive and negative predictions come from distributions with equal mean. The test confirmed that the null hypothesis could be rejected and that positive and negative predictions are independent distributions at a 5% significance level, see also Table 2.
From Soup: 47 The predicted results of the two models depicted in Fig. 5 are shown in Table 2 . While both models are statistically separated on a 0.05 confidence level, it is also of interest to compare the different true/false positive/negative rates of the two models. Here, eight misclassifications (seven of them between the two pollen) were obtained using full spectra and only a slight increase could be seen using the 78 input variable model with nine misclassifications (seven between the two pollen).
 ###### 
From cou.: 48 The predicted results of the two models depicted in Fig. 5 are shown in Table 2. While both models are statistically separated on a 0.05 confidence level, it is also of interest to compare the different true/false positive/negative rates of the two models. Here, eight misclassifications (seven of them between the two pollen) were obtained using full spectra and only a slight increase could be seen using the 78 input variable model with nine misclassifications (seven between the two pollen).
From Soup: 48 In practice, another problem commonly occurs. When performing calibration of your system, there is often a change in the x -axis. One way of compensating for these changes is to introduce an interpolation step for all spectra (‘database’ and new measurements) to make sure that a common x -scale is used. Additional smoothing of the data by, e.g. , a Savitzky–Golay filter may also be performed. (The interpolation step (and filter) commonly also reduce the number of variables.) However, in our case, introducing an interpolation only marginally reduced the Q 2 values and applying a smoothing filter on the data only decreases the Q 2 values for heavy variable reduction (if only variables with large variance are included), i.e. , reducing the number of variables to <500, results not shown.
 ###### 
From cou.: 49 In practice, another problem commonly occurs. When performing calibration of your system, there is often a change in the x-axis. One way of compensating for these changes is to introduce an interpolation step for all spectra (‘database’ and new measurements) to make sure that a common x-scale is used. Additional smoothing of the data by, e.g., a Savitzky–Golay filter may also be performed. (The interpolation step (and filter) commonly also reduce the number of variables.) However, in our case, introducing an interpolation only marginally reduced the Q2 values and applying a smoothing filter on the data only decreases the Q2 values for heavy variable reduction (if only variables with large variance are included), i.e., reducing the number of variables to <500, results not shown.
From Soup: 49 To explore the effect of spectral shift on the prediction models, spectra were shifted in x (wavelength scale) by 1, 2, 4 and 8 data point steps up and down while testing them in a model constructed from non shifted data. The true negative and positive rates for the shifted spectra can be seen in Fig. 6 . For the full resolution spectra (black symbols), a shift in maximum two steps only affects the model slightly, while a larger decrease was found for four steps and above. However, if using the highest value found in the predefined 78 wavelength windows (where only a minor decrease in misclassifications were found compared to the full 1D spectra, see Table 2 ), a larger shift can be tolerated without losing much in predictability (green symbols).
 ###### 
From cou.: 50 To explore the effect of spectral shift on the prediction models, spectra were shifted in x (wavelength scale) by 1, 2, 4 and 8 data point steps up and down while testing them in a model constructed from non shifted data. The true negative and positive rates for the shifted spectra can be seen in Fig. 6. For the full resolution spectra (black symbols), a shift in maximum two steps only affects the model slightly, while a larger decrease was found for four steps and above. However, if using the highest value found in the predefined 78 wavelength windows (where only a minor decrease in misclassifications were found compared to the full 1D spectra, see Table 2), a larger shift can be tolerated without losing much in predictability (green symbols).
From Soup: 50 For the images, a similar approach as for the 1D spectra was used. That is, the full resolution images were used to build a prediction model and compared to models built on statistically reduced number of variables. Images were also binned in different ways to further reduce the image data size. The effect of cropping the images to only include the region from which the 1D spectra were extracted (see Fig. 1 ) was also studied. In addition, the impact of a shift in x or y of the echellogram (where the shift in y simulates a hardware temperature drift in our case) was also studied.
 ###### 
From cou.: 51 For the images, a similar approach as for the 1D spectra was used. That is, the full resolution images were used to build a prediction model and compared to models built on statistically reduced number of variables. Images were also binned in different ways to further reduce the image data size. The effect of cropping the images to only include the region from which the 1D spectra were extracted (see Fig. 1) was also studied. In addition, the impact of a shift in x or y of the echellogram (where the shift in y simulates a hardware temperature drift in our case) was also studied.
From Soup: 51 Q 2 values as function of number of variables for the different binned full and cropped images are shown in Fig. 7 . No large changes in Q 2 was observed when using the cropped (approximately half-size) images when comparing to the full size images and similar number of variables. As the peaks/spots outside the main echellogram area (from where the 1D spectra are extracted) originates from additional diffraction orders and will thus be of redundant character, the similar Q 2 values should then be expected for the cropped images. For both the image sizes, the Q 2 value only decreases slightly when binning the pixels and reducing the number of variables to ∼100. Below ∼100 variables, however, a steep decrease occurs. Compared to the 1D spectra, however, see Fig. 4 , one can reduce the amount of variables more while still obtaining a relatively large Q 2 and there are also higher Q 2 values for the image data at the same amount of variables. As the absolute values of Q 2 decreases (at similar amounts of variables) and that the steep decrease is observed at a larger number of variables for the 1D spectral data ( Fig. 4 ), it clearly indicates that there is information lost when converting the echellograms into intensity vs. wavelength spectra.
 ###### 
From cou.: 52 Q 2 values as function of number of variables for the different binned full and cropped images are shown in Fig. 7. No large changes in Q2 was observed when using the cropped (approximately half-size) images when comparing to the full size images and similar number of variables. As the peaks/spots outside the main echellogram area (from where the 1D spectra are extracted) originates from additional diffraction orders and will thus be of redundant character, the similar Q2 values should then be expected for the cropped images. For both the image sizes, the Q2 value only decreases slightly when binning the pixels and reducing the number of variables to ∼100. Below ∼100 variables, however, a steep decrease occurs. Compared to the 1D spectra, however, see Fig. 4, one can reduce the amount of variables more while still obtaining a relatively large Q2 and there are also higher Q2 values for the image data at the same amount of variables. As the absolute values of Q2 decreases (at similar amounts of variables) and that the steep decrease is observed at a larger number of variables for the 1D spectral data (Fig. 4), it clearly indicates that there is information lost when converting the echellograms into intensity vs. wavelength spectra.
From Soup: 52 In general, binning the images reduces the maximum Q 2 value (at similar number of variables), however, when reducing the variables using the thresholds in variance, larger Q 2 values are found for the binned images at less variables. Normalizing the images slightly improves the Q 2 value, especially for heavy binning ( e.g. , 32 × 32).
 ###### 
From cou.: 53 In general, binning the images reduces the maximum Q2 value (at similar number of variables), however, when reducing the variables using the thresholds in variance, larger Q2 values are found for the binned images at less variables. Normalizing the images slightly improves the Q2 value, especially for heavy binning (e.g., 32 × 32).
From Soup: 53 Boxplots of two prediction models are depicted in Fig. 8 and can be compared with the results presented in Fig. 5 . Here, the result for a prediction model based on the full resolution, cropped images are shown in Fig. 8a , where tighter boxes closer to the ideal values can be seen compared to the full resolution 1D spectra (see Fig. 4a ), again illustrating the advantage of using images instead of the extracted spectra. As an example of binned and reduced variables, the result from a model created from cropped images binned 32 × 32 and only using the 51 variables with largest variance is shown in Fig. 8b . Such heavy reduction of the image data still results in separate distributions of the classifications. For the full resolution, but cropped, echellograms ( Q 2 = 0.92 using 12 factors), a total of four observations were misclassified (three between the pollen) and by using only the 51 largest variance variables from the 32 × 32 binned and cropped images ( Q 2 = 0.81 using 13 factors) a total of six observations were misclassified (five between the pollen). Again, these results illustrate the better performance of a classification algorithm built on data from the echellograms as compared to extracting the more commonly used 1D spectra. For our purposes, i.e. , to speed up readout and data transfer times without loss of predictability, these results are indeed very helpful. In this case, instead of having to read out the whole full resolution CCD image and perform a conversion/extraction of a 1D spectra it is in some cases enough (or even better) to only read out half the CCD area and also bin the pixels to some extent before building, or feeding data into, a prediction model. It should also be noted, however, that the prediction accuracy (in terms of, e.g. , tnr and tpr) is not directly proportional to the Q 2 values. That is, the randomly chosen images to build and validate the model may be more favorable for a model corresponding to a lower Q 2 . (In addition, there might still be an influence of over- or under-fitting of the models by choosing the number of factors by the rule of significance used here.) For example, using the normalized cropped and 32 × 32 binned echellograms (572 variables and model based on 13 factors with Q 2 = 0.85) results in only two misclassifications and using normalized cropped images binned 1 × 8 (70 296 variables and model based on 13 factors with Q 2 = 0.90) results in 100% correct classification for the randomly selected spectra used in this study.
 ###### 
From cou.: 54 Boxplots of two prediction models are depicted in Fig. 8 and can be compared with the results presented in Fig. 5. Here, the result for a prediction model based on the full resolution, cropped images are shown in Fig. 8a, where tighter boxes closer to the ideal values can be seen compared to the full resolution 1D spectra (see Fig. 4a), again illustrating the advantage of using images instead of the extracted spectra. As an example of binned and reduced variables, the result from a model created from cropped images binned 32 × 32 and only using the 51 variables with largest variance is shown in Fig. 8b. Such heavy reduction of the image data still results in separate distributions of the classifications. For the full resolution, but cropped, echellograms (Q2 = 0.92 using 12 factors), a total of four observations were misclassified (three between the pollen) and by using only the 51 largest variance variables from the 32 × 32 binned and cropped images (Q2 = 0.81 using 13 factors) a total of six observations were misclassified (five between the pollen). Again, these results illustrate the better performance of a classification algorithm built on data from the echellograms as compared to extracting the more commonly used 1D spectra. For our purposes, i.e., to speed up readout and data transfer times without loss of predictability, these results are indeed very helpful. In this case, instead of having to read out the whole full resolution CCD image and perform a conversion/extraction of a 1D spectra it is in some cases enough (or even better) to only read out half the CCD area and also bin the pixels to some extent before building, or feeding data into, a prediction model. It should also be noted, however, that the prediction accuracy (in terms of, e.g., tnr and tpr) is not directly proportional to the Q2 values. That is, the randomly chosen images to build and validate the model may be more favorable for a model corresponding to a lower Q2. (In addition, there might still be an influence of over- or under-fitting of the models by choosing the number of factors by the rule of significance used here.) For example, using the normalized cropped and 32 × 32 binned echellograms (572 variables and model based on 13 factors with Q2 = 0.85) results in only two misclassifications and using normalized cropped images binned 1 × 8 (70296 variables and model based on 13 factors with Q2 = 0.90) results in 100% correct classification for the randomly selected spectra used in this study.
From Soup: 54 Finally, the impact of shift in image input data was also investigated. The cropped images were shifted in x - and y -directions and a model made from the original (not shifted) images was used to predict these shifted images and Fig. 9 shows the tpr and tnr obtained. As can be seen, the model is more sensitive to a shift in y -direction (which corresponds to the thermal drift direction), where already a one pixel shift significantly reduces the tpr for the full resolution images. Likely, this behavior is because of the asymmetric distribution of a spot in the echellogram, i.e. , an elliptic intensity distribution can be seen (especially at shorter wavelengths where the pixel resolution in pixels per nm is higher) with the large axis in the x direction. One way to reduce the problem caused by image shift could be to bin the images before creating a model and test set. In Fig. 9b it can be seen that the model becomes slightly more robust and almost similar values are obtained for one pixel shift in any direction (note that in this case the number of variables also decreases to 572 compared to ∼520k for the full resolution cropped images). In addition, and similar to the 1D spectra, regions of interest were defined and the largest pixel value was then found in each window and used to build a prediction model. As an example, using 51 32 × 32 pixel windows (regions selected by largest variance in 32 × 32 binned images) and finding the largest value also seem to improve the robustness of the model, see Fig. 9c . Here, however, only 51 variables were used resulting in slightly lower tpr and tnr rates (13 misclassifications) as compared to the models built with more variables ( Fig. 9a and b ). For a more direct comparison with the wavelength windows used on the 1D spectra, see Fig. 6 , pixel windows centered at the positions on the echellogram corresponding to the 78 wavelengths were defined. To keep the same wavelength window size in the x -direction (±0.26 nm, as for the 1D spectra), the size of the pixel window was continuously decreased from ±15 pixels @ 280 nm to ±5 pixels @ 868 nm. The window size in the y -direction was ±5 pixels for all pixel windows. By selecting the pixel windows in this way, an even more robust model was found compared to the other models (see Fig. 6 and 9 ), however, with a small loss in accuracy (a total of 15 misclassifications at zero shift).
 ###### 
From cou.: 55 Finally, the impact of shift in image input data was also investigated. The cropped images were shifted in x- and y-directions and a model made from the original (not shifted) images was used to predict these shifted images and Fig. 9 shows the tpr and tnr obtained. As can be seen, the model is more sensitive to a shift in y-direction (which corresponds to the thermal drift direction), where already a one pixel shift significantly reduces the tpr for the full resolution images. Likely, this behavior is because of the asymmetric distribution of a spot in the echellogram, i.e., an elliptic intensity distribution can be seen (especially at shorter wavelengths where the pixel resolution in pixels per nm is higher) with the large axis in the x direction. One way to reduce the problem caused by image shift could be to bin the images before creating a model and test set. In Fig. 9b it can be seen that the model becomes slightly more robust and almost similar values are obtained for one pixel shift in any direction (note that in this case the number of variables also decreases to 572 compared to ∼520k for the full resolution cropped images). In addition, and similar to the 1D spectra, regions of interest were defined and the largest pixel value was then found in each window and used to build a prediction model. As an example, using 51 32 × 32 pixel windows (regions selected by largest variance in 32 × 32 binned images) and finding the largest value also seem to improve the robustness of the model, see Fig. 9c. Here, however, only 51 variables were used resulting in slightly lower tpr and tnr rates (13 misclassifications) as compared to the models built with more variables (Fig. 9a and b). For a more direct comparison with the wavelength windows used on the 1D spectra, see Fig. 6, pixel windows centered at the positions on the echellogram corresponding to the 78 wavelengths were defined. To keep the same wavelength window size in the x-direction (±0.26 nm, as for the 1D spectra), the size of the pixel window was continuously decreased from ±15 pixels @ 280 nm to ±5 pixels @ 868 nm. The window size in the y-direction was ±5 pixels for all pixel windows. By selecting the pixel windows in this way, an even more robust model was found compared to the other models (see Fig. 6 and 9), however, with a small loss in accuracy (a total of 15 misclassifications at zero shift).
From Soup: 55 The study of both the extracted 1D spectra and images showed that the LIBS data of the different biosamples can be quite well separated using multivariate data analysis methods. The PCA of the spectra and images showed that differences between samples were observable and that normalized data showed even more distinct separation of the samples while illustrating only the first two components. However, for 1D spectra the predictability of the models showed little or no improvement with normalized data whereas for the images a slight increase was observed (especially for the heavily binned images). From the analysis of single shot 1D LIBS spectra it was found that a threshold can be used to reduce the number of wavelengths used for classification to about 500 without losing a significant amount of predictability (here represented by the Q 2 value). Using wavelength windows centered around the strongest peaks in the spectra created more robust models if introducing a spectral shift in the test set.
 ###### 
From cou.: 56 The study of both the extracted 1D spectra and images showed that the LIBS data of the different biosamples can be quite well separated using multivariate data analysis methods. The PCA of the spectra and images showed that differences between samples were observable and that normalized data showed even more distinct separation of the samples while illustrating only the first two components. However, for 1D spectra the predictability of the models showed little or no improvement with normalized data whereas for the images a slight increase was observed (especially for the heavily binned images). From the analysis of single shot 1D LIBS spectra it was found that a threshold can be used to reduce the number of wavelengths used for classification to about 500 without losing a significant amount of predictability (here represented by the Q2 value). Using wavelength windows centered around the strongest peaks in the spectra created more robust models if introducing a spectral shift in the test set.
From Soup: 56 Using PLS-DA on the images as input resulted in an improvement in predictability compared to the spectra. In addition, only the region containing the main echellogram is needed when constructing the models. The images initially have more variables than the spectra, however, the current work shows that it is possible to reduce the number of pixels to orders of magnitudes less than the number of data points in a 1D spectrum and still obtain a better predictability.
 ###### 
From cou.: 57 Using PLS-DA on the images as input resulted in an improvement in predictability compared to the spectra. In addition, only the region containing the main echellogram is needed when constructing the models. The images initially have more variables than the spectra, however, the current work shows that it is possible to reduce the number of pixels to orders of magnitudes less than the number of data points in a 1D spectrum and still obtain a better predictability.
From Soup: 57 Reducing the data size by decreasing the resolution of the images, i.e. , by binning the pixels, also proved to be possible with little change in, e.g. , tpr and tpr rates. Even a quite heavy binning ( e.g. , 32 × 32) can be performed without significantly affecting the predictability (compared to using full resolution images).
 ###### 
From cou.: 58 Reducing the data size by decreasing the resolution of the images, i.e., by binning the pixels, also proved to be possible with little change in, e.g., tpr and tpr rates. Even a quite heavy binning (e.g., 32 × 32) can be performed without significantly affecting the predictability (compared to using full resolution images).
From Soup: 58 The simulation of, e.g. , thermal shifts of the image revealed the fragility of PLS-DA models and a higher sensitivity was observed for shifts in the y direction. To slightly improve the robustness, methods such as binning and/or introduce windows (regions of interest) from which the largest value is taken could be used.
 ###### 
From cou.: 59 The simulation of, e.g., thermal shifts of the image revealed the fragility of PLS-DA models and a higher sensitivity was observed for shifts in the y direction. To slightly improve the robustness, methods such as binning and/or introduce windows (regions of interest) from which the largest value is taken could be used.
From Soup: 59 This work was funded by the Swedish Department of Defence, Project no. 440-A404114 and a Grant from the European Union Infrastructure Committee for the project “Two Stage Rapid Biological Surveillance and Alarm System for Airborne Threats (TWOBIAS)”, Grant no. FP7-242297. The authors would also like to thank Dr T. Tjärnhage for fruitful discussions.
 ###### 
From cou.: 60 This work was funded by the Swedish Department of Defence, Project no. 440-A404114 and a Grant from the European Union Infrastructure Committee for the project “Two Stage Rapid Biological Surveillance and Alarm System for Airborne Threats (TWOBIAS)”, Grant no. FP7-242297. The authors would also like to thank Dr T. Tjärnhage for fruitful discussions.
From Soup: 60 P. Baldi, S. Brunak, Y. Chauvin, C. A. F. Andersen and F. Nielsen, Bioinformatics , 2000, 16 , 412 CrossRef CAS PubMed .
 ###### 
From cou.: 61 P. Baldi, S. Brunak, Y. Chauvin, C. A. F. Andersen and F. Nielsen, Bioinformatics, 2000, 16, 412 CrossRef CAS PubMed .
From Soup: 61 Y. Saeys, I. Inza and P. Larranaga, Bioinformatics , 2007, 23 , 2507 CrossRef CAS PubMed .
 ###### 
From cou.: 62 Y. Saeys, I. Inza and P. Larranaga, Bioinformatics, 2007, 23, 2507 CrossRef CAS PubMed .
From Soup: 62 P. J. Lisboa, Neural Network , 2002, 15 , 11 CrossRef CAS .
 ###### 
From cou.: 63 P. J. Lisboa, Neural Network, 2002, 15, 11 CrossRef CAS .
From Soup: 63 H. Lohninger and K. Varmuza, Anal. Chem. , 1987, 59 , 236 CrossRef CAS .
 ###### 
From cou.: 64 H. Lohninger and K. Varmuza, Anal. Chem., 1987, 59, 236 CrossRef CAS .
From Soup: 64 P. Jonsson, A. I. Johansson, J. Gullberg, J. Trygg, J. A. B. Grung, S. Marklund, M. Sjöström, H. Antti and T. Moritz, Anal. Chem. , 2005, 77 , 5635 CrossRef CAS PubMed .
 ###### 
From cou.: 65 P. Jonsson, A. I. Johansson, J. Gullberg, J. Trygg, J. A. B. Grung, S. Marklund, M. Sjöström, H. Antti and T. Moritz, Anal. Chem., 2005, 77, 5635 CrossRef CAS PubMed .
From Soup: 65 Bioaerosol Detection Technologies , ed. P. Jonsson, G. Olofsson and T. Tjärnhage, Springer-Verlag, New York, 2014 Search PubMed .
 ###### 
From cou.: 66 Bioaerosol Detection Technologies, ed. P. Jonsson, G. Olofsson and T. Tjärnhage, Springer-Verlag, New York, 2014 Search PubMed .
From Soup: 66 R. DeFreez, Proc. SPIE , 2009, 7484:74840H , 1 Search PubMed .
 ###### 
From cou.: 67 R. DeFreez, Proc. SPIE, 2009, 7484:74840H, 1 Search PubMed .
From Soup: 67 V. Sivaprakasam, A. L. Huston, C. Scotto and J. D. Eversole, Opt. Express , 2004, 12 , 4457 CrossRef CAS .
 ###### 
From cou.: 68 V. Sivaprakasam, A. L. Huston, C. Scotto and J. D. Eversole, Opt. Express, 2004, 12, 4457 CrossRef CAS .
From Soup: 68 J. D. Hybl, S. M. Tysk, S. R. Berry and M. P. Jordan, Appl. Opt. , 2006, 45 , 8806 CrossRef CAS .
 ###### 
From cou.: 69 J. D. Hybl, S. M. Tysk, S. R. Berry and M. P. Jordan, Appl. Opt., 2006, 45, 8806 CrossRef CAS .
From Soup: 69 C. D. Clark, P. Campuzano-Jost, D. S. Covert, R. C. Richter, H. Maring, A. J. Hynes and E. S. Saltzman, J. Aerosol Sci. , 2001, 32 , 765 CrossRef CAS .
 ###### 
From cou.: 70 C. D. Clark, P. Campuzano-Jost, D. S. Covert, R. C. Richter, H. Maring, A. J. Hynes and E. S. Saltzman, J. Aerosol Sci., 2001, 32, 765 CrossRef CAS .
From Soup: 70 H. Felix-Rivera and S. Hernandez-Rivera, Sens. Imag. Int. J. , 2012, 13 , 1 CrossRef PubMed .
 ###### 
From cou.: 71 H. Felix-Rivera and S. Hernandez-Rivera, Sens. Imag. Int. J., 2012, 13, 1 CrossRef PubMed .
From Soup: 71 A. Sengupta, N. Brar and E. J. Davis, J. Colloid Interface Sci. , 2007, 309 , 36 CrossRef CAS PubMed .
 ###### 
From cou.: 72 A. Sengupta, N. Brar and E. J. Davis, J. Colloid Interface Sci., 2007, 309, 36 CrossRef CAS PubMed .
From Soup: 72 D. W. Hahn and M. M. Lunden, Aerosol Sci. Technol. , 2000, 33 , 30 CrossRef CAS .
 ###### 
From cou.: 73 D. W. Hahn and M. M. Lunden, Aerosol Sci. Technol., 2000, 33, 30 CrossRef CAS .
From Soup: 73 J. D. Hybl, G. A. Lithgow and S. G. Buckley, Appl. Spectrosc. , 2003, 57 , 1207 CrossRef CAS .
 ###### 
From cou.: 74 J. D. Hybl, G. A. Lithgow and S. G. Buckley, Appl. Spectrosc., 2003, 57, 1207 CrossRef CAS .
From Soup: 74 S. J. Rehse, Q. I. Mohaidat and S. Palchaudhuri, Appl. Opt. , 2010, 49 , C27 CrossRef CAS .
 ###### 
From cou.: 75 S. J. Rehse, Q. I. Mohaidat and S. Palchaudhuri, Appl. Opt., 2010, 49, C27 CrossRef CAS .
From Soup: 75 F. C. DeLucia and J. L. Gottfried, Appl. Opt. , 2012, 51 , B83 CrossRef CAS PubMed .
 ###### 
From cou.: 76 F. C. DeLucia and J. L. Gottfried, Appl. Opt., 2012, 51, B83 CrossRef CAS PubMed .
From Soup: 76 A. C. Samuels, F. C. DeLucia, K. L. McNesby and A. W. Miziolek, Appl. Opt. , 2003, 42 , 6205 CrossRef CAS .
 ###### 
From cou.: 77 A. C. Samuels, F. C. DeLucia, K. L. McNesby and A. W. Miziolek, Appl. Opt., 2003, 42, 6205 CrossRef CAS .
From Soup: 77 C. A. Munson, F. C. DeLucia, T. Piehler, K. L. McNesby and A. W. Miziolek, Spectrochim. Acta, Part B , 2005, 60 , 1217 CrossRef PubMed .
 ###### 
From cou.: 78 C. A. Munson, F. C. DeLucia, T. Piehler, K. L. McNesby and A. W. Miziolek, Spectrochim. Acta, Part B, 2005, 60, 1217 CrossRef PubMed .
From Soup: 78 N. L. Lanza, R. C. Wiens, S. M. Clegg, A. M. Ollila, S. D. Humphries, H. E. Newsom and J. E. Barefield, Appl. Opt. , 2010, 49 , C211 CrossRef CAS .
 ###### 
From cou.: 79 N. L. Lanza, R. C. Wiens, S. M. Clegg, A. M. Ollila, S. D. Humphries, H. E. Newsom and J. E. Barefield, Appl. Opt., 2010, 49, C211 CrossRef CAS .
From Soup: 79 Laser Induced Breakdown Spectroscopy , ed. A. W. Miziolek, V. Palleschi and I. Schechter, Cambridge University Press, Cambridge, 2006 Search PubMed .
 ###### 
From cou.: 80 Laser Induced Breakdown Spectroscopy, ed. A. W. Miziolek, V. Palleschi and I. Schechter, Cambridge University Press, Cambridge, 2006 Search PubMed .
From Soup: 80 T. Tjärnhage, P.-Å. Gradmark, A. Larsson, A. Mohammed, L. Landström, E. Sagerfors, P. Jonsson, F. Kullander and M. Andersson, Opt. Commun. , 2013, 296 , 106 CrossRef PubMed .
 ###### 
From cou.: 81 T. Tjärnhage, P.-Å. Gradmark, A. Larsson, A. Mohammed, L. Landström, E. Sagerfors, P. Jonsson, F. Kullander and M. Andersson, Opt. Commun., 2013, 296, 106 CrossRef PubMed .
From Soup: 81 G. A. Lithgow and S. G. Buckley, Appl. Phys. Lett. , 2005, 87 , 011501 CrossRef PubMed .
 ###### 
From cou.: 82 G. A. Lithgow and S. G. Buckley, Appl. Phys. Lett., 2005, 87, 011501 CrossRef PubMed .
From Soup: 82 K. Pearson, Philos. Mag. , 1901, 2 , 559 CrossRef .
 ###### 
From cou.: 83 K. Pearson, Philos. Mag., 1901, 2, 559 CrossRef .
From Soup: 83 H. Hotelling, J. Educ. Psychol. , 1933, 24 , 417 CrossRef PubMed .
 ###### 
From cou.: 84 H. Hotelling, J. Educ. Psychol., 1933, 24, 417 CrossRef PubMed .
From Soup: 84 S. Wold, M. Sjöström and L. Eriksson, Chemom. Intell. Lab. Syst. , 2001, 58 , 109 CrossRef CAS .
 ###### 
From cou.: 85 S. Wold, M. Sjöström and L. Eriksson, Chemom. Intell. Lab. Syst., 2001, 58, 109 CrossRef CAS .
From Soup: 85 P. Geladi and B. R. Kowalski, Anal. Chim. Acta , 1986, 185 , 1 CrossRef CAS .
 ###### 
From cou.: 86 P. Geladi and B. R. Kowalski, Anal. Chim. Acta, 1986, 185, 1 CrossRef CAS .
From Soup: 86 S. Wold, K. Esbensen and P. Geladi, Chemom. Intell. Lab. Syst. , 1987, 2 , 37 CrossRef CAS .
 ###### 
From cou.: 87 S. Wold, K. Esbensen and P. Geladi, Chemom. Intell. Lab. Syst., 1987, 2, 37 CrossRef CAS .
From Soup: 87 T. Rajalahti and O. M. Kvalheim, Int. J. Pharm. , 2011, 417 , 280 CrossRef CAS PubMed .
 ###### 
From cou.: 88 T. Rajalahti and O. M. Kvalheim, Int. J. Pharm., 2011, 417, 280 CrossRef CAS PubMed .
From Soup: 88 P. Geladi, B. Sethson, J. Nystrom, T. Lillhonga, T. Lestander and J. Burger, Spectrochim. Acta, Part B , 2004, 59 , 1347 Search PubMed .
 ###### 
From cou.: 89 P. Geladi, B. Sethson, J. Nystrom, T. Lillhonga, T. Lestander and J. Burger, Spectrochim. Acta, Part B, 2004, 59, 1347 Search PubMed .
From Soup: 89 K. Dobbin and R. Simon, BMC Med. Genomics , 2011, 4 , 31 CrossRef PubMed .
 ###### 
From cou.: 90 K. Dobbin and R. Simon, BMC Med. Genomics, 2011, 4, 31 CrossRef PubMed .
From Soup: 90 T. Mehmood, K. H. Liland, L. Snipen and S. Sæbø, Chemom. Intell. Lab. Syst. , 2012, 118 , 62 CrossRef CAS PubMed .
 ###### 
From cou.: 91 T. Mehmood, K. H. Liland, L. Snipen and S. Sæbø, Chemom. Intell. Lab. Syst., 2012, 118, 62 CrossRef CAS PubMed .
From Soup: 91 Accessed August 2014, http://iraf.noao.edu/.
 ###### 
From cou.: 92 Accessed August 2014, http://iraf.noao.edu/.
