From cou.: 0 Journal
From Soup: 0 Mol. BioSyst.
 ###### 
From cou.: 1 Protein structure space is believed to consist of a finite set of discrete folds, unlike the protein sequence space which is astronomically large, indicating that proteins from the available sequence space are likely to adopt one of the many folds already observed. In spite of extensive sequence–structure correlation data, protein structure prediction still remains an open question with researchers having tried different approaches (experimental as well as computational). One of the challenges of protein structure prediction is to identify the native protein structures from a milieu of decoys/models. In this work, a rigorous investigation of Protein Structure Networks (PSNs) has been performed to detect native structures from decoys/models. Ninety four parameters obtained from network studies have been optimally combined with Support Vector Machines (SVM) to derive a general metric to distinguish decoys/models from the native protein structures with an accuracy of 94.11%. Recently, for the first time in the literature we had shown that PSN has the capability to distinguish native proteins from decoys. A major difference between the present work and the previous study is to explore the transition profiles at different strengths of non-covalent interactions and SVM has indeed identified this as an important parameter. Additionally, the SVM trained algorithm is also applied to the recent CASP10 predicted models. The novelty of the network approach is that it is based on general network properties of native protein structures and that a given model can be assessed independent of any reference structure. Thus, the approach presented in this paper can be valuable in validating the predicted structures. A web-server has been developed for this purpose and is freely available at http://vishgraph.mbu.iisc.ernet.in/GraProStr/PSN-QA.html.
From Soup: 1 Protein structure space is believed to consist of a finite set of discrete folds, unlike the protein sequence space which is astronomically large, indicating that proteins from the available sequence space are likely to adopt one of the many folds already observed. In spite of extensive sequence–structure correlation data, protein structure prediction still remains an open question with researchers having tried different approaches (experimental as well as computational). One of the challenges of protein structure prediction is to identify the native protein structures from a milieu of decoys/models. In this work, a rigorous investigation of Protein Structure Networks (PSNs) has been performed to detect native structures from decoys/models. Ninety four parameters obtained from network studies have been optimally combined with Support Vector Machines (SVM) to derive a general metric to distinguish decoys/models from the native protein structures with an accuracy of 94.11%. Recently, for the first time in the literature we had shown that PSN has the capability to distinguish native proteins from decoys. A major difference between the present work and the previous study is to explore the transition profiles at different strengths of non-covalent interactions and SVM has indeed identified this as an important parameter. Additionally, the SVM trained algorithm is also applied to the recent CASP10 predicted models. The novelty of the network approach is that it is based on general network properties of native protein structures and that a given model can be assessed independent of any reference structure. Thus, the approach presented in this paper can be valuable in validating the predicted structures. A web-server has been developed for this purpose and is freely available at http://vishgraph.mbu.iisc.ernet.in/GraProStr/PSN-QA.html.
 ###### 
From cou.: 2 Proteins are known to adopt unique well-defined three-dimensional structures to carry out their functions efficiently.1 The number of available protein sequences far exceeds that of the solved structures, emphasizing the need to model the structures and thereby understand the sequence–structure–function relationships in proteins. Predicting the three-dimensional structure of a native protein, given its amino acid sequence, has been a challenge for many decades. Towards this goal, many large scale statistical studies on experimentally determined protein structures have been performed, each describing various aspects that are crucial to reconstruct a well-folded protein structure. Such studies have highlighted the roles of specific amino acids as helix breakers or initiators.2 Further, the role of hydrophobic residues to form the core of a protein structure and the various pair-wise interactions that are crucial to impart stability to secondary and super-secondary structures have also been investigated.3–8 Based on these large scale statistical studies, gross understandings of the rules that govern protein folding have been accomplished and are widely discussed in the literature.9–12
From Soup: 2 Proteins are known to adopt unique well-defined three-dimensional structures to carry out their functions efficiently. 1 The number of available protein sequences far exceeds that of the solved structures, emphasizing the need to model the structures and thereby understand the sequence–structure–function relationships in proteins . Predicting the three-dimensional structure of a native protein , given its amino acid sequence, has been a challenge for many decades. Towards this goal, many large scale statistical studies on experimentally determined protein structures have been performed, each describing various aspects that are crucial to reconstruct a well-folded protein structure. Such studies have highlighted the roles of specific amino acids as helix breakers or initiators. 2 Further, the role of hydrophobic residues to form the core of a protein structure and the various pair-wise interactions that are crucial to impart stability to secondary and super-secondary structures have also been investigated. 3–8 Based on these large scale statistical studies, gross understandings of the rules that govern protein folding have been accomplished and are widely discussed in the literature. 9–12
 ###### 
From cou.: 3 For a number of years now, protein structures have been treated as networks that help in obtaining a mathematical abstraction of the global topological features.13,14 Based on this approach investigation such as identification of groups of interacting residues important for folding/function, residues important for protein stability and fluctuations15,16 have been performed. Graph theoretical concepts have been used to address the problem of protein structure selection,17 and further to discriminate native proteins from their decoy sets.18,19 In the majority of these studies, the protein structure networks are generated at the Cα/backbone level, which although useful, may not be sufficient to capture the chemical and geometrical properties that arise due to side chain interactions of a residue. Studies on protein structure networks [PSNs] that are generated at the level of side chain atoms have been extensively carried out in our laboratory and it is observed that adding side chain details makes the network much more realistic, allowing the network to capture minute details of the protein topology.20–22 Sidechain based PSN studies of native proteins show patterns that are missing in random networks, highlighting that paradigms do exist in nature that govern protein folding and also underlines the capability of PSNs to capture the same.23
From Soup: 3 For a number of years now, protein structures have been treated as networks that help in obtaining a mathematical abstraction of the global topological features. 13,14 Based on this approach investigation such as identification of groups of interacting residues important for folding/function, residues important for protein stability and fluctuations 15,16 have been performed. Graph theoretical concepts have been used to address the problem of protein structure selection, 17 and further to discriminate native proteins from their decoy sets. 18,19 In the majority of these studies, the protein structure networks are generated at the C α /backbone level, which although useful, may not be sufficient to capture the chemical and geometrical properties that arise due to side chain interactions of a residue. Studies on protein structure networks [PSNs] that are generated at the level of side chain atoms have been extensively carried out in our laboratory and it is observed that adding side chain details makes the network much more realistic, allowing the network to capture minute details of the protein topology. 20–22 Sidechain based PSN studies of native proteins show patterns that are missing in random networks, highlighting that paradigms do exist in nature that govern protein folding and also underlines the capability of PSNs to capture the same. 23
 ###### 
From cou.: 4 One of the approaches to understand protein folding is to identify parameters for the comparison of native and decoy/model structures. Decoys are protein structures with minor conformational deviations from their native fold and are generated by various computational methods, such as molecular dynamics simulations24–26 and discrete state models.27 It is believed that the root mean square deviation (RMSD), hydrogen bond patterns, accessible surface area, interaction energy of amino acid pairs28–30 and the position of conserved residues31 in decoy/model sets differ from the native structures, therefore forming an excellent source to study and identify unique properties of a natively folded protein structure. Another critical aspect of modelling protein structure is to assess the quality of structures generated. This involves developing effective methods and designing scoring functions to rank the structures based on their quality. Studies have been performed to develop these scoring functions based on accessible surface area and amino acid neighbourhood considerations.32 Many knowledge based potentials33,34 have also been developed based on large scale studies to identify interactions that are preferred in a natively folded structure. Characterising native structures using the network properties of a protein structure are also available in the literature.17–19 A large scale community-wide prediction of protein structures and evaluation of the predicted models have been facilitated by the Critical Assessment of Structure Prediction [CASP] group35 and forms an excellent resource to obtain non-native like structures.
From Soup: 4 One of the approaches to understand protein folding is to identify parameters for the comparison of native and decoy/model structures. Decoys are protein structures with minor conformational deviations from their native fold and are generated by various computational methods, such as molecular dynamics simulations 24–26 and discrete state models. 27 It is believed that the root mean square deviation (RMSD), hydrogen bond patterns, accessible surface area, interaction energy of amino acid pairs 28–30 and the position of conserved residues 31 in decoy/model sets differ from the native structures, therefore forming an excellent source to study and identify unique properties of a natively folded protein structure. Another critical aspect of modelling protein structure is to assess the quality of structures generated. This involves developing effective methods and designing scoring functions to rank the structures based on their quality. Studies have been performed to develop these scoring functions based on accessible surface area and amino acid neighbourhood considerations. 32 Many knowledge based potentials 33,34 have also been developed based on large scale studies to identify interactions that are preferred in a natively folded structure. Characterising native structures using the network properties of a protein structure are also available in the literature. 17–19 A large scale community-wide prediction of protein structures and evaluation of the predicted models have been facilitated by the Critical Assessment of Structure Prediction [CASP] group 35 and forms an excellent resource to obtain non-native like structures.
 ###### 
From cou.: 5 As mentioned above, our laboratory has focused on PSNs arising due to the interaction of side-chains. Recently, we explored the potential of PSNs to distinguish decoys/models from the native protein structures36 for the first time in the area of protein structure prediction. The network properties calculated for PSNs constructed at lower interaction strength [(Imin) = 1%, described in the method section] indicated that such a formalism can be used to distinguish the native structures from decoys. On the other hand, the transition profile of network parameters as a function of interaction strength has been shown to be unique for the native protein structures37 and an exploration of network features at all levels of interaction strengths would increase this distinguishing capability. In the present study these features are incorporated in a rigorous manner. Towards this goal, the PSNs are constructed and the packing of residues at different Imins are explored to study the changes in network parameters, as it transitions from a dense network (lower Imin) to a sparse network (higher Imin). While, the PSNs capture network properties at the side chain level and are much more detailed, main chain hydrogen bonds [MHB] have also been shown to be crucial for packing of the polypeptide chain8 and have been included in this study. These features that characterises native proteins are passed to Support Vector Machines (SVM)38 to generate a classifier. SVMs have been widely used for predicting protein secondary structures,39–42 protein SCOP classes43 and protein binding sites.44 Machine learning algorithms have also been used for predicting the quality of protein structures45,46 using features based on secondary structures, solvent accessibility and atom–atom contacts. In the present study, the network properties of the PSNs are used to build a SVM classifier.
From Soup: 5 As mentioned above, our laboratory has focused on PSNs arising due to the interaction of side-chains. Recently, we explored the potential of PSNs to distinguish decoys/models from the native protein structures 36 for the first time in the area of protein structure prediction. The network properties calculated for PSNs constructed at lower interaction strength [( I min ) = 1%, described in the method section] indicated that such a formalism can be used to distinguish the native structures from decoys. On the other hand, the transition profile of network parameters as a function of interaction strength has been shown to be unique for the native protein structures 37 and an exploration of network features at all levels of interaction strengths would increase this distinguishing capability. In the present study these features are incorporated in a rigorous manner. Towards this goal, the PSNs are constructed and the packing of residues at different I min s are explored to study the changes in network parameters, as it transitions from a dense network (lower I min ) to a sparse network (higher I min ). While, the PSNs capture network properties at the side chain level and are much more detailed, main chain hydrogen bonds [MHB] have also been shown to be crucial for packing of the polypeptide chain 8 and have been included in this study. These features that characterises native proteins are passed to Support Vector Machines (SVM) 38 to generate a classifier. SVMs have been widely used for predicting protein secondary structures, 39–42 protein SCOP classes 43 and protein binding sites. 44 Machine learning algorithms have also been used for predicting the quality of protein structures 45,46 using features based on secondary structures, solvent accessibility and atom–atom contacts. In the present study, the network properties of the PSNs are used to build a SVM classifier.
 ###### 
From cou.: 6 In the present study, a total of 94 features that best describes the various aspects of a PSN are integrated to build an SVM model, which shows an overall accuracy of 94.11%. The features are also ranked based on how well they distinguish the decoy/model structures from the natives. Interestingly, the transition behaviour of the protein as it progresses from lower Imin to higher Imin has been shown to contribute maximally to distinguish the decoy from the native structures, highlighting the uniqueness of transition profiles of network parameters in the native structures. The model is further validated using near-native decoy structures from the Rosetta dataset,47,48 quality assessed structures of CASP9 [http://predictioncenter.org/casp9] and native structures [http://www.pdb.org]. A complete network analysis of the recently released CASP10 models has also been performed and compared with CASP10 assessments. Furthermore, a free web-server (http://vishgraph.mbu.iisc.ernet.in/GraProStr/PSN-QA.html) has also been developed to check the quality of a model. Details of the methodology are provided in the next section, followed by results and discussion.
From Soup: 6 In the present study, a total of 94 features that best describes the various aspects of a PSN are integrated to build an SVM model, which shows an overall accuracy of 94.11%. The features are also ranked based on how well they distinguish the decoy/model structures from the natives. Interestingly, the transition behaviour of the protein as it progresses from lower I min to higher I min has been shown to contribute maximally to distinguish the decoy from the native structures, highlighting the uniqueness of transition profiles of network parameters in the native structures. The model is further validated using near-native decoy structures from the Rosetta dataset, 47,48 quality assessed structures of CASP9 [http://predictioncenter.org/casp9] and native structures [http://www.pdb.org]. A complete network analysis of the recently released CASP10 models has also been performed and compared with CASP10 assessments. Furthermore, a free web-server (http://vishgraph.mbu.iisc.ernet.in/GraProStr/PSN-QA.html) has also been developed to check the quality of a model. Details of the methodology are provided in the next section, followed by results and discussion.
 ###### 
From cou.: 7 Decoy/model datasets are downloaded from the following sources and structures with residues lower than hundred are filtered out to ensure a proper hydrophobic core:
From Soup: 7 Decoy/model datasets are downloaded from the following sources and structures with residues lower than hundred are filtered out to ensure a proper hydrophobic core:
 ###### 
From cou.: 8 (a) University of Libre de Bruxelles (http://babylone.ulb.ac.be/decoys)29
From Soup: 8 (a) University of Libre de Bruxelles (http://babylone.ulb.ac.be/decoys) 29
 ###### 
From cou.: 9 (b) Baker laboratory (http://depts.washington.edu/bakerpg/decoys/)47,48
From Soup: 9 (b) Baker laboratory (http://depts.washington.edu/bakerpg/decoys/) 47,48
 ###### 
From cou.: 10 (c) Decoys ‘R’ Us (http://dd.compbio.washington.edu/download.shtml)49
From Soup: 10 (c) Decoys ‘R’ Us (http://dd.compbio.washington.edu/download.shtml) 49
 ###### 
From cou.: 11 (d) CASP3/7/8/9/10 (http://predictioncenter.org/casp3/http://predictioncenter.org/casp7/http://predictioncenter.org/casp8http://predictioncenter.org/casp9http://predictioncenter.org/casp10)
From Soup: 11 (d) CASP3/7/8/9/10 (http://predictioncenter.org/casp3/http://predictioncenter.org/casp7/http://predictioncenter.org/casp8http://predictioncenter.org/casp9http://predictioncenter.org/casp10)
 ###### 
From cou.: 12 A total of 308 native structures and their corresponding decoys/models (29543) are obtained from the above sites. Additionally, PDB structures are also downloaded from RCSB50 (http://www.pdb.org) to increase the representation of native structures in the datasets. The monomeric protein crystal structures with resolution less than 3 Å, R-factor less than 0.25 and residue number greater than 100 are selected. Finally, the native dataset consists of 5422 proteins and the decoy/model set consists of 29543 structures. Individual details for each dataset are provided in Table 1 and details of the individual proteins in the dataset are provided in Table S1 (ESI†).
From Soup: 12 A total of 308 native structures and their corresponding decoys/models (29 543) are obtained from the above sites. Additionally, PDB structures are also downloaded from RCSB 50 (http://www.pdb.org) to increase the representation of native structures in the datasets. The monomeric protein crystal structures with resolution less than 3 Å, R -factor less than 0.25 and residue number greater than 100 are selected. Finally, the native dataset consists of 5422 proteins and the decoy/model set consists of 29 543 structures. Individual details for each dataset are provided in Table 1 and details of the individual proteins in the dataset are provided in Table S1 (ESI † ).
 ###### 
From cou.: 13 A Protein Structure Network [PSN] gives an insight into the topological features of a protein structure based on the non-covalent side-chain interactions. Details to build a protein structure network are provided in previous studies20,37 and a brief description is provided here. Amino acids in the protein structures are considered as nodes and edges are made between residues with non-covalent interactions and quantified by the parameter [Iij]:where, Iij = strength of interaction between residues i and j, where |i − j| ≥ 2; nij = number of distinct interacting atom pairs between i and j within a distance cut-off of 4.5 Å (excluding the backbone atoms); Ni and Nj are the normalization values for residues i and j obtained from a statistically significant dataset of proteins, as defined earlier.20 PSNs are constructed at different interaction strengths as defined by Imin and the values ranging from 0% to 7% have been used in this study. The network parameters which capture the global topological features such as, simple pair-wise interactions [NCov], size of the largest cluster [SLClu] (largest cluster is calculated using the Depth First Search algorithm51), size of the largest k-2 community [ComSk2],52 cumulative size of the top3 k-1 community [Top3-ComSk1],53 clustering coefficients of the PSN [CCoe], sub-network formed by the largest cluster [CCoe-LClu] and largest k-2 community [CCoe-Lcomm] have been calculated and analysed for the different categories. Differences in the values of the aforementioned global parameters at successive Imins are calculated for a discrete representation of the slope of the transition profiles. Table 2 provides the network parameters used in the present study with a brief description of each parameter. Fig. 1 shows the representation of k-1 and k-2 community on an example protein structure (1ERV).
From Soup: 13 A Protein Structure Network [PSN] gives an insight into the topological features of a protein structure based on the non-covalent side-chain interactions. Details to build a protein structure network are provided in previous studies 20,37 and a brief description is provided here. Amino acids in the protein structures are considered as nodes and edges are made between residues with non-covalent interactions and quantified by the parameter [ I ij ]: where, I ij = strength of interaction between residues i and j , where | i − j | ≥ 2; n ij = number of distinct interacting atom pairs between i and j within a distance cut-off of 4.5 Å (excluding the backbone atoms); N i and N j are the normalization values for residues i and j obtained from a statistically significant dataset of proteins , as defined earlier. 20 PSNs are constructed at different interaction strengths as defined by I min and the values ranging from 0% to 7% have been used in this study. The network parameters which capture the global topological features such as, simple pair-wise interactions [NCov], size of the largest cluster [SLClu] (largest cluster is calculated using the Depth First Search algorithm 51 ), size of the largest k-2 community [ComSk2], 52 cumulative size of the top3 k-1 community [Top3-ComSk1], 53 clustering coefficients of the PSN [CCoe], sub-network formed by the largest cluster [CCoe-LClu] and largest k-2 community [CCoe-Lcomm] have been calculated and analysed for the different categories. Differences in the values of the aforementioned global parameters at successive I min s are calculated for a discrete representation of the slope of the transition profiles. Table 2 provides the network parameters used in the present study with a brief description of each parameter. Fig. 1 shows the representation of k-1 and k-2 community on an example protein structure (1ERV).
 ###### 
From cou.: 14 The behaviour of the network properties is also compared with random networks. Two extreme cases of random networks,23 RM1 and RM3, are generated. The RM1 models are based on the Erdos–Renyi random networks and have no similarity to protein structure topology. The size of the network is fixed to 100 and networks are generated at different probability values that correspond to the connections obtained at different Imin values. The RM3 models are specific cases of random models where the position of each node/residue and the number of interactions between the residues are kept the same as in native PSN at a given Imin and the interactions [edges] are then randomly generated throughout the PSNs. Ten random networks at different Imin for RM1 and RM3 are generated.
From Soup: 14 The behaviour of the network properties is also compared with random networks. Two extreme cases of random networks, 23 RM1 and RM3, are generated. The RM1 models are based on the Erdos–Renyi random networks and have no similarity to protein structure topology. The size of the network is fixed to 100 and networks are generated at different probability values that correspond to the connections obtained at different I min values. The RM3 models are specific cases of random models where the position of each node/residue and the number of interactions between the residues are kept the same as in native PSN at a given I min and the interactions [edges] are then randomly generated throughout the PSNs. Ten random networks at different I min for RM1 and RM3 are generated.
 ###### 
From cou.: 15 Support Vector Machine (SVM) is a machine learning algorithm, used mainly for dataset classification. This method uses a training set, which consists of instance-label pairs (attributes and class label for a set of data points) to obtain a hyperplane on the nth dimension that best separates the data. The rules learnt from the training set are used to build a classifier which is further validated using test cases.38 LIBSVM54 is a library of SVM that is freely available (http://www.csie.ntu.edu.tw/~cjlin/libsvm) and is used in the present study.
From Soup: 15 Support Vector Machine (SVM) is a machine learning algorithm, used mainly for dataset classification. This method uses a training set, which consists of instance-label pairs (attributes and class label for a set of data points) to obtain a hyperplane on the n th dimension that best separates the data. The rules learnt from the training set are used to build a classifier which is further validated using test cases. 38 LIBSVM 54 is a library of SVM that is freely available (http://www.csie.ntu.edu.tw/~cjlin/libsvm) and is used in the present study.
 ###### 
From cou.: 16 The network parameters provided in Table 2 are calculated at different Imin [from 0% to 7%] for the native and the decoy/model datasets. Along with these, MHB for each native and decoy/model structure is calculated using HBPlus,55 with the default settings. Finally, a total of 94 features are obtained to build the SVM classifier. While most of the parameters are derived from the network properties of the PSNs at different Imin and describe the features at the side chain level one of the parameters, MHB, describes the structure at the back bone level.
From Soup: 16 The network parameters provided in Table 2 are calculated at different I min [from 0% to 7%] for the native and the decoy/model datasets. Along with these, MHB for each native and decoy/model structure is calculated using HBPlus, 55 with the default settings. Finally, a total of 94 features are obtained to build the SVM classifier. While most of the parameters are derived from the network properties of the PSNs at different I min and describe the features at the side chain level one of the parameters, MHB, describes the structure at the back bone level.
 ###### 
From cou.: 17 Since the number of decoy/model structures far exceeds the number of native structures, the dataset is distributed randomly 10 times, such that in each random subset, the training set consists of an equal number [3000] of native proteins and decoy/model structures, while the remaining are considered as a test set. Data pre-processing is performed in two steps. Firstly, the data is scaled to range from −10 to 10. Since, the features provided in SVM are independent of each other and the scale of the value for different features may vary, the data is scaled such that in all the features the lowest value = −10 and the highest value = 10. On doing this, any bias that arises due to the different value ranges of the different features are removed. The radial basis kernel function (RBF) has been used, since the number of features is less than the number of data points and so a non-linear kernel function is required to map the data on a higher dimensional space. Using this, iterative cross-validation accuracy is performed to obtain the optimal values of c and g that give the best accuracy for the given training set. ‘c’ defines the strictness of the classification, with higher ‘c’ values implying increased strictness. The parameter g (gamma), defines the smoothness of the boundary that separates the two classes, with a larger gamma value representing a smoother and regular boundary. A good classifier would be one that has higher c and g values. However, a trade-off does exist as higher strictness can lead to over-classification of data making the classifier less robust and general. Once the dataset is optimised, classification accuracy is calculated based on the LIBSVM protocol. Table 3 gives the details of accuracy obtained for the 10 random datasets. The dataset [d2] with the best accuracy is considered for further analysis.
From Soup: 17 Since the number of decoy/model structures far exceeds the number of native structures, the dataset is distributed randomly 10 times, such that in each random subset, the training set consists of an equal number [3000] of native proteins and decoy/model structures, while the remaining are considered as a test set. Data pre-processing is performed in two steps. Firstly, the data is scaled to range from −10 to 10. Since, the features provided in SVM are independent of each other and the scale of the value for different features may vary, the data is scaled such that in all the features the lowest value = −10 and the highest value = 10. On doing this, any bias that arises due to the different value ranges of the different features are removed. The radial basis kernel function (RBF) has been used, since the number of features is less than the number of data points and so a non-linear kernel function is required to map the data on a higher dimensional space. Using this, iterative cross-validation accuracy is performed to obtain the optimal values of c and g that give the best accuracy for the given training set. ‘ c ’ defines the strictness of the classification, with higher ‘ c ’ values implying increased strictness. The parameter g (gamma), defines the smoothness of the boundary that separates the two classes, with a larger gamma value representing a smoother and regular boundary. A good classifier would be one that has higher c and g values. However, a trade-off does exist as higher strictness can lead to over-classification of data making the classifier less robust and general. Once the dataset is optimised, classification accuracy is calculated based on the LIBSVM protocol. Table 3 gives the details of accuracy obtained for the 10 random datasets. The dataset [d2] with the best accuracy is considered for further analysis.
 ###### 
From cou.: 18 Feature selection is a method commonly applied in SVM to identify features that best classify the data. Identifying important features not only helps in understanding the basis of classification but also reduces run time by removing the unimportant features from the SVM classification model. Feature selection is performed based on two methods, each of which is described below.
From Soup: 18 Feature selection is a method commonly applied in SVM to identify features that best classify the data. Identifying important features not only helps in understanding the basis of classification but also reduces run time by removing the unimportant features from the SVM classification model. Feature selection is performed based on two methods, each of which is described below.
 ###### 
From cou.: 19 (a) Fselect:56 calculates the F-score and measures the discrimination capability of a feature. The following equation is used to calculate the F-score
From Soup: 19 (a) Fselect: 56 calculates the F -score and measures the discrimination capability of a feature. The following equation is used to calculate the F -score
 ###### 
From cou.: 20 i
From Soup: 20 i
 ###### 
From cou.: 21 (+)
From Soup: 21 (+)
 ###### 
From cou.: 22 i
From Soup: 22 i
 ###### 
From cou.: 23 (−)
From Soup: 23 (−)
 ###### 
From cou.: 24 i
From Soup: 24 i
 ###### 
From cou.: 25 i
From Soup: 25 i
 ###### 
From cou.: 26 th
From Soup: 26 th
 ###### 
From cou.: 27 x
From Soup: 27 x
 ###### 
From cou.: 28 (+)
From Soup: 28 (+)
 ###### 
From cou.: 29 k
From Soup: 29 k , i
 ###### 
From cou.: 30 i
From Soup: 30 x
 ###### 
From cou.: 31 x
From Soup: 31 (−)
 ###### 
From cou.: 32 (−)
From Soup: 32 k , i
 ###### 
From cou.: 33 k
From Soup: 33 i
 ###### 
From cou.: 34 i
From Soup: 34 th
 ###### 
From cou.: 35 i
From Soup: 35 k
 ###### 
From cou.: 36 th
From Soup: 36 th
 ###### 
From cou.: 37 k
From Soup: 37 F
 ###### 
From cou.: 38 th
From Soup: 38 i
 ###### 
From cou.: 39 F
From Soup: 39 th
 ###### 
From cou.: 40 i
From Soup: 40 (b) ReliefF: 57 this is another method for estimating attribute importance. Unlike Fselect, this method does not assume that the features are independent of each other and therefore are capable of identifying best features even if the features are dependent on each other. It uses regression analysis to calculate the importance of each feature. Matlab implementation of ReliefF is used for the present study.
 ###### 
From cou.: 41 th
From Soup: 41 The classifier obtained is further validated using the near native structures of Rosetta, quality assessed structures of CASP9, and all 5422 native proteins . The near native decoy structures of Rosetta serve as the negative control, while all native proteins form the positive control. The CASP9 quality assessed structures are the best modelled structures and consist of a mixture of native like and near native structures.
 ###### 
From cou.: 42 (b) ReliefF:57 this is another method for estimating attribute importance. Unlike Fselect, this method does not assume that the features are independent of each other and therefore are capable of identifying best features even if the features are dependent on each other. It uses regression analysis to calculate the importance of each feature. Matlab implementation of ReliefF is used for the present study.
From Soup: 42 While the manuscript was under preparation, native structures for the CASP10 targets were not completely released. Native structures for the CASP10 targets released on or before 11th September 2012 were included in the training set and test set to build and test the classifier. However, the complete list of predicted models and the corresponding native structures (barring a few) were released later and have been analysed using the SVM classifier and critically compared with the CASP10 quality assessment.
 ###### 
From cou.: 43 The classifier obtained is further validated using the near native structures of Rosetta, quality assessed structures of CASP9, and all 5422 native proteins. The near native decoy structures of Rosetta serve as the negative control, while all native proteins form the positive control. The CASP9 quality assessed structures are the best modelled structures and consist of a mixture of native like and near native structures.
From Soup: 43 The network parameters described in Table 2 are evaluated for each of the 5422 natives and 29 543 decoy/model structures at different I min . Additionally, the back-bone packing feature is captured by calculating the MHB for all the structures. Behaviour of the network features of all the decoys/models and natives, results pertaining to the classifier, validation and prediction are presented in the following sections.
 ###### 
From cou.: 44 While the manuscript was under preparation, native structures for the CASP10 targets were not completely released. Native structures for the CASP10 targets released on or before 11th September 2012 were included in the training set and test set to build and test the classifier. However, the complete list of predicted models and the corresponding native structures (barring a few) were released later and have been analysed using the SVM classifier and critically compared with the CASP10 quality assessment.
From Soup: 44 As explained in the method section, PSNs are generated for each of the protein native and decoy/model structures and the network parameters evaluated. The list of parameters is provided in Table 2 and a brief description is provided here. NCov represents the number of non-covalent interactions in a protein structure and is a good measure of the integrity of the protein structure. Since side chain atoms are the major contributors of non-covalent interactions, this parameter also accounts for the chemistry of the amino acid in a structural context. Size of the largest cluster [SLClu] represents the global connectivity of the protein structure. A cluster is defined as the set of connected nodes and the cluster that is made of the maximum number of nodes is termed as the largest cluster. This property represents bond percolation in a PSN. Other higher order connectivities such as cliques and communities are also calculated using CFinder. 53 A k-clique is defined as a set of k nodes, in which all nodes are connected to each other. A union of k-cliques formed by sharing k-1 or k-2 are called a k-1 or k-2 community respectively. A detailed representation of communities is shown in Fig. 1 . Large communities represent the percolation of highly connected units. Both k-1 and k-2 communities are evaluated for the present study. The clustering coefficient [CCoe] is a measure of the inter-connectivity of nodes and defines the cliquishness of a PSN. Network behaviour is investigated for all the native and decoy/model sets through these parameters, by plotting them as a function of I min . Below we discuss the nature of these parameters by presenting the results for CASP3, CASP7 and all the data put together.
 ###### 
From cou.: 45 The network parameters described in Table 2 are evaluated for each of the 5422 natives and 29543 decoy/model structures at different Imin. Additionally, the back-bone packing feature is captured by calculating the MHB for all the structures. Behaviour of the network features of all the decoys/models and natives, results pertaining to the classifier, validation and prediction are presented in the following sections.
From Soup: 45 Fig. 2 describes the transition profile of the four basic network properties [NCov, SLClu, ComSk2, CCoe] as a function of I min for two example datasets, CASP3 [C3], CASP7 [C7] and the average property over all datasets [AD]. Fig. 2a shows the transition profile of NCov in a native protein and their decoys/models for all the three datasets. C3 consists of 971 modelled structures corresponding to a single native structure; while C7 consists of 6 native structures and AD consists of 5422 native structures. A closer look at Fig. 2a for C3 clearly suggests that the native profile [shown in blue] shows higher values at lower I min and drastically falls down as it approaches higher I min , when compared to its decoy structures. A similar pattern is observed for all the other network properties for C3 (SLClu, ComSk2, CCoe) and clearly indicates patterns specific to the natives and the decoys/models. If we observe the profiles for CASP9 and CASP10 datasets (Fig. S1, ESI † ), a reverse trend is observed for NCov and SLClu at lower I min , although the values are lower than that of the decoy structures at higher I min . [It should be noted that plots for a single protein and its decoy can provide better insight as in the case of C3 while some information is lost when averaged over a large number of data points]. Nevertheless, the striking feature exhibited consistently by the network parameters is the transition of the values from I min = 1% to I min = 4%. This is highly evident from the middle panel of Fig. 2 . This panel shows the transition profiles for C7 native structures but fails to show any significant transition for the decoy/model sets. This behaviour holds true for all the network properties and is also observed in the combined profile comparison for all the native proteins and decoys/models in the dataset ( Fig. 2 right panel). This is further validated by comparing the profiles of native, decoy/model and random structures as shown in Fig. S2 (ESI † ). Random structures have been earlier explored in our laboratory 23 in comparison to native structures to understand the percolation behaviour of a natively folded structure. Two extreme cases of random structures, RM1 and RM3, are considered for this analysis. For this study, the RM3 model was based on the CASP3 native structure (1BL0). As expected a distinct transition is completely missing from the profiles of RM1 structures (black lines) while the RM3 structures (green colour) show transition profiles nearer to the decoy/model structures than the native structures, emphasising the subtle but unique feature of the native structures.
 ###### 
From cou.: 46 As explained in the method section, PSNs are generated for each of the protein native and decoy/model structures and the network parameters evaluated. The list of parameters is provided in Table 2 and a brief description is provided here. NCov represents the number of non-covalent interactions in a protein structure and is a good measure of the integrity of the protein structure. Since side chain atoms are the major contributors of non-covalent interactions, this parameter also accounts for the chemistry of the amino acid in a structural context. Size of the largest cluster [SLClu] represents the global connectivity of the protein structure. A cluster is defined as the set of connected nodes and the cluster that is made of the maximum number of nodes is termed as the largest cluster. This property represents bond percolation in a PSN. Other higher order connectivities such as cliques and communities are also calculated using CFinder.53 A k-clique is defined as a set of k nodes, in which all nodes are connected to each other. A union of k-cliques formed by sharing k-1 or k-2 are called a k-1 or k-2 community respectively. A detailed representation of communities is shown in Fig. 1. Large communities represent the percolation of highly connected units. Both k-1 and k-2 communities are evaluated for the present study. The clustering coefficient [CCoe] is a measure of the inter-connectivity of nodes and defines the cliquishness of a PSN. Network behaviour is investigated for all the native and decoy/model sets through these parameters, by plotting them as a function of Imin. Below we discuss the nature of these parameters by presenting the results for CASP3, CASP7 and all the data put together.
From Soup: 46 The above results have shown that the interplay between the three network features (henceforth mentioned as characteristic features) (1) higher value at lower I min , (2) lower value at higher I min and finally (3) the transition from I min = 1%–4% is able to capture the differences in the decoy/model and the native structures in a clear manner. As mentioned in the method section, the differences in the values of the parameters [such as d(NCov), d(SLClu) and d(ComSk2)] for the consecutive I min s are calculated to represent this transitional behaviour. And these parameters are considered for building the SVM model along with other network parameters. Since, all the features represent the protein structure at the detailed side-chain level; other features that capture the backbone packing of the protein structure were also explored. Importance of the back-bone packing of protein structures has been discussed before. 8 We analysed the importance of MHB as a feature for SVM. MHB is calculated for all the 5422 native proteins using HBPlus 55 and a frequency plot generated. As can be observed from Fig. 3(a) , the majority of the proteins showed an MHB [normalised by the size of the protein ] value in the range of 0.6–1.1 (60% to 110% of the protein size). Therefore, the main-chain hydrogen bond is also included as a feature in SVM.
 ###### 
From cou.: 47 Fig. 2 describes the transition profile of the four basic network properties [NCov, SLClu, ComSk2, CCoe] as a function of Imin for two example datasets, CASP3 [C3], CASP7 [C7] and the average property over all datasets [AD]. Fig. 2a shows the transition profile of NCov in a native protein and their decoys/models for all the three datasets. C3 consists of 971 modelled structures corresponding to a single native structure; while C7 consists of 6 native structures and AD consists of 5422 native structures. A closer look at Fig. 2a for C3 clearly suggests that the native profile [shown in blue] shows higher values at lower Imin and drastically falls down as it approaches higher Imin, when compared to its decoy structures. A similar pattern is observed for all the other network properties for C3 (SLClu, ComSk2, CCoe) and clearly indicates patterns specific to the natives and the decoys/models. If we observe the profiles for CASP9 and CASP10 datasets (Fig. S1, ESI†), a reverse trend is observed for NCov and SLClu at lower Imin, although the values are lower than that of the decoy structures at higher Imin. [It should be noted that plots for a single protein and its decoy can provide better insight as in the case of C3 while some information is lost when averaged over a large number of data points]. Nevertheless, the striking feature exhibited consistently by the network parameters is the transition of the values from Imin = 1% to Imin = 4%. This is highly evident from the middle panel of Fig. 2. This panel shows the transition profiles for C7 native structures but fails to show any significant transition for the decoy/model sets. This behaviour holds true for all the network properties and is also observed in the combined profile comparison for all the native proteins and decoys/models in the dataset (Fig. 2 right panel). This is further validated by comparing the profiles of native, decoy/model and random structures as shown in Fig. S2 (ESI†). Random structures have been earlier explored in our laboratory23 in comparison to native structures to understand the percolation behaviour of a natively folded structure. Two extreme cases of random structures, RM1 and RM3, are considered for this analysis. For this study, the RM3 model was based on the CASP3 native structure (1BL0). As expected a distinct transition is completely missing from the profiles of RM1 structures (black lines) while the RM3 structures (green colour) show transition profiles nearer to the decoy/model structures than the native structures, emphasising the subtle but unique feature of the native structures.
From Soup: 47 Here we have employed the techniques of SVM to understand the importance of topological parameters in determining the uniqueness of protein structures as described in the method section. Specifically, we are interested in distinguishing the native structures from the structures generated as decoys and identifying native and non-native structures from predicted models. The details of model building and the results pertaining to feature selection are described below.
 ###### 
From cou.: 48 The above results have shown that the interplay between the three network features (henceforth mentioned as characteristic features) (1) higher value at lower Imin, (2) lower value at higher Imin and finally (3) the transition from Imin = 1%–4% is able to capture the differences in the decoy/model and the native structures in a clear manner. As mentioned in the method section, the differences in the values of the parameters [such as d(NCov), d(SLClu) and d(ComSk2)] for the consecutive Imins are calculated to represent this transitional behaviour. And these parameters are considered for building the SVM model along with other network parameters. Since, all the features represent the protein structure at the detailed side-chain level; other features that capture the backbone packing of the protein structure were also explored. Importance of the back-bone packing of protein structures has been discussed before.8 We analysed the importance of MHB as a feature for SVM. MHB is calculated for all the 5422 native proteins using HBPlus55 and a frequency plot generated. As can be observed from Fig. 3(a), the majority of the proteins showed an MHB [normalised by the size of the protein] value in the range of 0.6–1.1 (60% to 110% of the protein size). Therefore, the main-chain hydrogen bond is also included as a feature in SVM.
From Soup: 48 The network properties described in the above section are integrated into a mathematical framework to build a model that best classifies the decoy/model structures from the native proteins , using SVM. 93 network features ranging from I min = 0%–7% and MHB are integrated into the model. The data of 5422 native and 29 543 decoy/model structures are used as inputs to obtain ten random subsets. They are generated by randomly selecting 3000 native proteins and 3000 decoy/model structures as the training set and the remaining are used as test sets. The data is pre-processed before performing SVM as explained before. Table 3 lists the 10 subsets and the corresponding accuracy and AUC (area under curve) for each subset. Subset d2 gives the highest accuracy with an AUC = 0.9853 ( Fig. 4a ) and is considered for further analysis. It should however be noted that the other subsets have also yielded good accuracy.
 ###### 
From cou.: 49 Here we have employed the techniques of SVM to understand the importance of topological parameters in determining the uniqueness of protein structures as described in the method section. Specifically, we are interested in distinguishing the native structures from the structures generated as decoys and identifying native and non-native structures from predicted models. The details of model building and the results pertaining to feature selection are described below.
From Soup: 49 Feature selection is used to identify features that can best differentiate the native structures from the decoy/modelled structures. This analysis helps in short-listing a set of features from the pool of available ones that truly captures the uniqueness of a protein structure. Two methods have been used for this analysis as described in the methods section. As can be seen from Fig. 4b , both the methods predict similar results, with MHB being the top scoring feature. Features that capture the transition profile of the protein structure as a function of I min , such as d(NCov (2,3) ), d(ComSk2 (2,3) ), d(SLClu (1,2) ) are also predicted as top scorers along with SLClu, ComSk2, top1-ComSk1 and CCoe at different I min values. Network properties such as NCov show similar differentiating capacity at all I min s, while for parameters such as the SLClu, ComSk2, and CCoe, the differentiating capacity decreases at higher I min values. This trend is reversed for the k-1 communities that can best capture the differences between the native structures and the decoy/model structures at higher I min values. Table S2 (ESI † ) gives a list of all the features used in the method and the corresponding F -scores and ReliefF scores. The results obtained by feature selection methods correspond very well with the patterns observed in Fig. 2 and again emphasises the interplay between the three characteristic features important for determining the quality of a native protein structure.
 ###### 
From cou.: 50 The network properties described in the above section are integrated into a mathematical framework to build a model that best classifies the decoy/model structures from the native proteins, using SVM. 93 network features ranging from Imin = 0%–7% and MHB are integrated into the model. The data of 5422 native and 29543 decoy/model structures are used as inputs to obtain ten random subsets. They are generated by randomly selecting 3000 native proteins and 3000 decoy/model structures as the training set and the remaining are used as test sets. The data is pre-processed before performing SVM as explained before. Table 3 lists the 10 subsets and the corresponding accuracy and AUC (area under curve) for each subset. Subset d2 gives the highest accuracy with an AUC = 0.9853 (Fig. 4a) and is considered for further analysis. It should however be noted that the other subsets have also yielded good accuracy.
From Soup: 50 During the process of model building the model is trained and tested using the respective datasets. Apart from these validations, some more rigorous tests are performed for specific cases. For this, we selected three datasets: (1) Rosetta near native structures, (2) CASP9 quality assessed structures and (3) all native structures.
 ###### 
From cou.: 51 Feature selection is used to identify features that can best differentiate the native structures from the decoy/modelled structures. This analysis helps in short-listing a set of features from the pool of available ones that truly captures the uniqueness of a protein structure. Two methods have been used for this analysis as described in the methods section. As can be seen from Fig. 4b, both the methods predict similar results, with MHB being the top scoring feature. Features that capture the transition profile of the protein structure as a function of Imin, such as d(NCov(2,3)), d(ComSk2(2,3)), d(SLClu(1,2)) are also predicted as top scorers along with SLClu, ComSk2, top1-ComSk1 and CCoe at different Imin values. Network properties such as NCov show similar differentiating capacity at all Imins, while for parameters such as the SLClu, ComSk2, and CCoe, the differentiating capacity decreases at higher Imin values. This trend is reversed for the k-1 communities that can best capture the differences between the native structures and the decoy/model structures at higher Imin values. Table S2 (ESI†) gives a list of all the features used in the method and the corresponding F-scores and ReliefF scores. The results obtained by feature selection methods correspond very well with the patterns observed in Fig. 2 and again emphasises the interplay between the three characteristic features important for determining the quality of a native protein structure.
From Soup: 51 Rosetta near native structures are known to be closer to the native structures and therefore differentiating these from the native structures is a challenging task. A dataset of Rosetta native structures (19) and their corresponding near native structures (380) are concatenated to form the input file. This forms a negative control and we expect the classifier to predict all the near native structures as decoy/model structures while all the native proteins as native. A striking accuracy of 99.74% is obtained. All the 380 near-native structures are classified as decoys, while 18 out of 19 structures are predicted as natives with the structure (4UBP), predicted as decoy. Fig. 5(a) plots the network parameters (SLClu, ComSk2 and CCoe) for native structures predicted as natives (blue), decoy structures predicted as decoys (green) and native predicted as decoy (red). If we look closely, 4UBP (red line) fails to show a significant transition for SLClu and also for other parameters at higher I min . Also the values of ComSk2 and CCoe for 4UBP reduce to zero after the transition phase. Thus, the classifier is able to best capture the transition profile of network parameters as a function of I min and successfully classify the structures.
 ###### 
From cou.: 52 During the process of model building the model is trained and tested using the respective datasets. Apart from these validations, some more rigorous tests are performed for specific cases. For this, we selected three datasets: (1) Rosetta near native structures, (2) CASP9 quality assessed structures and (3) all native structures.
From Soup: 52 CASP9 predicted structures are tested for quality by independent assessors and the best quality structures are labelled as quality assessed (QA). A set of quality assessment techniques have been developed over the years for different methods of structure prediction such as template-based, 58 free modelling based predictions, 59 residue–residue contact prediction 60 and so on. Details about these methods and various statistics used to rank the predictions are provided in previous articles. 35,61
 ###### 
From cou.: 53 Rosetta near native structures are known to be closer to the native structures and therefore differentiating these from the native structures is a challenging task. A dataset of Rosetta native structures (19) and their corresponding near native structures (380) are concatenated to form the input file. This forms a negative control and we expect the classifier to predict all the near native structures as decoy/model structures while all the native proteins as native. A striking accuracy of 99.74% is obtained. All the 380 near-native structures are classified as decoys, while 18 out of 19 structures are predicted as natives with the structure (4UBP), predicted as decoy. Fig. 5(a) plots the network parameters (SLClu, ComSk2 and CCoe) for native structures predicted as natives (blue), decoy structures predicted as decoys (green) and native predicted as decoy (red). If we look closely, 4UBP (red line) fails to show a significant transition for SLClu and also for other parameters at higher Imin. Also the values of ComSk2 and CCoe for 4UBP reduce to zero after the transition phase. Thus, the classifier is able to best capture the transition profile of network parameters as a function of Imin and successfully classify the structures.
From Soup: 53 The SVM classifier built in this study is validated using the above-mentioned CASP9 quality assessed structures. In this case 54 native structures from CASP9 are concatenated with 1266 quality assessed CASP9 predictions and further classified using the classifier. Out of 54 native structures, 35 are predicted as native (true-positive) while 16 are predicted as non-native (false-negative) by the classifier. For the 1266 good-quality prediction models, 87 are predicted as native (false-positive) while the rest are predicted as decoy/model, giving an overall accuracy of 91.9%. To get a clear idea about this classification, we divided the structures into four classes: (1) native structures predicted as natives (nn), (2) native structures predicted as decoys/models (nd), (3) decoy/model structures predicted as decoys (dd) and finally (4) decoy/model structures predicted as natives (dn). The network profiles for each of the above classes are plotted as a function of I min and a significant interplay of the three characteristic features is observed. If we look at the SLClu profile in Fig. 5(b) , nn (blue) shows a higher value at lower I min regions along with a steeper transition profile around I min = 1%–4%. In fact, as expected the transition profile is steepest for nn as compared to dd (green). For the nd class (red), a lower value is observed at lower I min . For the dn class (black), a smooth transition from lower to higher I min is observed, however the values at lower I min are lower than the native. A definite distinction can also be seen by studying the profiles for ComSk2 and CCoe ( Fig. 5b ). The nn class shows higher values at lower I min and a steeper slope, while dd shows the reverse trend. It is interesting to note that the quality assessed structures being classified as dn class by SVM, have compensatory MHB values, as given in Table 4 . The table provides the percentage distribution of MHB for the four classes. It is shown in Fig. 3(a) that the MHB for native structures mostly lie in the region of 0.6–1.1 (60–110% of the protein size). As can be seen for the dn class, the majority of the structures have MHB values in the favourable region and quite interestingly, most of the members of the nd class have values in the unfavourable region, highlighting the existing synergy between the properties of the side chain and the backbone. It seems that this synergy has been well picked up by the SVM model as also highlighted by the feature selection results. Amongst the network parameters, ComSk2 and CCoe quite distinctively pick up the differences between the native proteins and the decoy/modelled structures.
 ###### 
From cou.: 54 CASP9 predicted structures are tested for quality by independent assessors and the best quality structures are labelled as quality assessed (QA). A set of quality assessment techniques have been developed over the years for different methods of structure prediction such as template-based,58 free modelling based predictions,59 residue–residue contact prediction60 and so on. Details about these methods and various statistics used to rank the predictions are provided in previous articles.35,61
From Soup: 54 The third and final validation dataset is the 5422 native structures already downloaded from RCSB that served as a positive control. An overall accuracy of 95.11% was obtained with 5157 predicted as native and the rest (265) as decoy/model structures. Again the network properties of the true positives (nn) and false negatives (nd) are plotted as a function of I min as shown in Fig. 5c . The difference in the network profiles for the two classes is clearly visible from the graph, with nn (blue) showing higher values at lower I min and a steeper transition profile as compared to the other class nd (red). This behaviour is consistent in all the network properties, indicating the usefulness of this method.
 ###### 
From cou.: 55 The SVM classifier built in this study is validated using the above-mentioned CASP9 quality assessed structures. In this case 54 native structures from CASP9 are concatenated with 1266 quality assessed CASP9 predictions and further classified using the classifier. Out of 54 native structures, 35 are predicted as native (true-positive) while 16 are predicted as non-native (false-negative) by the classifier. For the 1266 good-quality prediction models, 87 are predicted as native (false-positive) while the rest are predicted as decoy/model, giving an overall accuracy of 91.9%. To get a clear idea about this classification, we divided the structures into four classes: (1) native structures predicted as natives (nn), (2) native structures predicted as decoys/models (nd), (3) decoy/model structures predicted as decoys (dd) and finally (4) decoy/model structures predicted as natives (dn). The network profiles for each of the above classes are plotted as a function of Imin and a significant interplay of the three characteristic features is observed. If we look at the SLClu profile in Fig. 5(b), nn (blue) shows a higher value at lower Imin regions along with a steeper transition profile around Imin = 1%–4%. In fact, as expected the transition profile is steepest for nn as compared to dd (green). For the nd class (red), a lower value is observed at lower Imin. For the dn class (black), a smooth transition from lower to higher Imin is observed, however the values at lower Imin are lower than the native. A definite distinction can also be seen by studying the profiles for ComSk2 and CCoe (Fig. 5b). The nn class shows higher values at lower Imin and a steeper slope, while dd shows the reverse trend. It is interesting to note that the quality assessed structures being classified as dn class by SVM, have compensatory MHB values, as given in Table 4. The table provides the percentage distribution of MHB for the four classes. It is shown in Fig. 3(a) that the MHB for native structures mostly lie in the region of 0.6–1.1 (60–110% of the protein size). As can be seen for the dn class, the majority of the structures have MHB values in the favourable region and quite interestingly, most of the members of the nd class have values in the unfavourable region, highlighting the existing synergy between the properties of the side chain and the backbone. It seems that this synergy has been well picked up by the SVM model as also highlighted by the feature selection results. Amongst the network parameters, ComSk2 and CCoe quite distinctively pick up the differences between the native proteins and the decoy/modelled structures.
From Soup: 55 The SVM classifier is also used to assess the quality of the recently concluded CASP10 competition. A total of 31 869 predicted models corresponding to 83 targets are used [exclusive of NMR and cancelled targets], on which an RMSD filter of ≤5 Å is applied. This resulted in 8503 models of which 899 models are predicted as good quality from SVM. The CASP-10 site has provided GDT-TS 61,62 scores for all the models against which the results obtained by the SVM classifier are compared. The targets are divided as Template based modelling (TBM) or Free-modelling ( FM ) as given in CASP10. Out of the 83 targets, 54 are identified as TBM [8263 models] while 23 [240 models] are labelled as FM . In this study, statistics of CASP10 assessed scores and SVM selected models are shown for both TBM as well as FM targets; however, detailed analysis has been carried out only for TBM targets. Apart from the GDT-TS score, CASP10 predicted models are also assigned a side-chain specific, GDC score. 62 The SVM classifier is also compared against this score since both methods are based on side-chain atoms. A detailed discussion on the comparisons performed is provided below.
 ###### 
From cou.: 56 The third and final validation dataset is the 5422 native structures already downloaded from RCSB that served as a positive control. An overall accuracy of 95.11% was obtained with 5157 predicted as native and the rest (265) as decoy/model structures. Again the network properties of the true positives (nn) and false negatives (nd) are plotted as a function of Imin as shown in Fig. 5c. The difference in the network profiles for the two classes is clearly visible from the graph, with nn (blue) showing higher values at lower Imin and a steeper transition profile as compared to the other class nd (red). This behaviour is consistent in all the network properties, indicating the usefulness of this method.
From Soup: 56 A broad distribution of the number of models that are selected by the different methods is presented in Fig. 6(a and b) . For the TBM targets, out of the 8263 models, 5414 models have GDT-TS scores ≥70, while 884 are selected by SVM. Considering a GDC score ≥30 (this value is selected based on the distribution of GDC scores; data not shown) 4020 qualified as good models. Interestingly, 632 models are selected by all the methods [Venn diagram ( Fig. 6a )], indicating that most of the models (71.4%) selected by SVM could be easily validated by the GDT-TS scores. In the case of FM , a GDT-TS score ≥30 is considered for selection and again about 86.6% (13 out of 15 selected by SVM) have a high GDT-TS score. Further, the network properties of the models predicted by each method and their combinations are examined. Fig. 6(c–f) plot the different network parameters as a function of I min for models qualified by (a) all the methods [blue], (b) only SVM [green], (c) only GDC [cyan], (d) only GDT-TS [red] and (e) models not selected by any of the scoring scheme [black]. As expected, the characteristic patterns of native structures, described before in this work, are distinctively shown by the blue line [selected by all methods] while the black line [selected by none] deviates significantly from this pattern. Furthermore, models selected by any one of the methods display transition patterns with minor variations. Amongst these, models picked up by SVM show a pattern closer to the characteristic profile described earlier, followed by models selected by GDC and GDT-TS. The transition profile of GDC and SVM selected models seem to satisfy all the criteria of a characteristic profile, perhaps emphasising the importance of including side chain atoms for quality assessment.
 ###### 
From cou.: 57 The SVM classifier is also used to assess the quality of the recently concluded CASP10 competition. A total of 31869 predicted models corresponding to 83 targets are used [exclusive of NMR and cancelled targets], on which an RMSD filter of ≤5 Å is applied. This resulted in 8503 models of which 899 models are predicted as good quality from SVM. The CASP-10 site has provided GDT-TS61,62 scores for all the models against which the results obtained by the SVM classifier are compared. The targets are divided as Template based modelling (TBM) or Free-modelling (FM) as given in CASP10. Out of the 83 targets, 54 are identified as TBM [8263 models] while 23 [240 models] are labelled as FM. In this study, statistics of CASP10 assessed scores and SVM selected models are shown for both TBM as well as FM targets; however, detailed analysis has been carried out only for TBM targets. Apart from the GDT-TS score, CASP10 predicted models are also assigned a side-chain specific, GDC score.62 The SVM classifier is also compared against this score since both methods are based on side-chain atoms. A detailed discussion on the comparisons performed is provided below.
From Soup: 57 To evaluate the quality of models picked by different methods, RMSD distribution of the models at the C α and all-atom level is examined. Fig. 7(a and b) show the RMSD distribution of models selected by (left) only GDT-TS, (middle) both GDT-TS and SVM and (right) only SVM as a pie chart. At the C α level ( Fig. 7a ), models selected by GDT-TS have RMSD mainly in the range of 1.5 Å–3.0 Å [only GDT-TS (55%) and common panel (69%)]. Models with RMSD >3 Å and ≤5 Å have also been significantly selected by the GDT-TS scoring scheme. For the SVM selected models, the majority fall in the range of 1.5 Å–3 Å (69% + 18%) and 3 Å–5 Å (28% + 65%), a considerable percentage (17%) of models also have a RMSD value below 1.5 Å. This indicates that, while the network based method selects only a small number of models as compared to GDT-TS, it judiciously selects models with high accuracy as seen by the high percentage of models selected in the low RMSD range. Similar behaviour is seen for RMSD-all atom distribution. Strikingly, in this case models selected only by SVM show the lowest RMSD value (15%), while GDT-TS fails to pick any model with such a low value of RMSD. However, SVM also picks up a significant number of models as compared to GDT-TS with RMSD-all > 5 Å. This is a consequence of the fact that the network based method performs independent of the native structure and is solely based on the interaction strength of the side chain atoms, proper packing and orientation. The large RMSD in SVM selected models may arise due to different side-chain conformations and/or loop orientations (also may be due to false positives), indicating the possibility of structures with alternate conformations. An example of this is shown in Fig. 7c , where the model (T0667TS028_2) selected only by SVM for target Id T0667 has C α -RMSD value = 2.64 Å and GDT-TS score = 54.82. Although side chain interaction is the major feature that drives the present network method, backbone packing is also checked on the basis of main-chain hydrogen bonds. Fig. 3(b–d) represent the percentage distribution of MHB for models selected by GDT-TS, both methods and SVM. Most of the models selected by these methods have MHB in the preferred region, however in the case of GDT-TS; a significant proportion of these models also lie in the low probability zone. Another example presented is the target Id T0708 (4H17), for which all the models selected by SVM are also selected by the GDT-TS score. In this case, the selected models align extremely well with the native structure (Fig. S3, ESI † ) and exhibit low RMSD values, in the range of 1.66 Å to 1.98 Å.
 ###### 
From cou.: 58 A broad distribution of the number of models that are selected by the different methods is presented in Fig. 6(a and b). For the TBM targets, out of the 8263 models, 5414 models have GDT-TS scores ≥70, while 884 are selected by SVM. Considering a GDC score ≥30 (this value is selected based on the distribution of GDC scores; data not shown) 4020 qualified as good models. Interestingly, 632 models are selected by all the methods [Venn diagram (Fig. 6a)], indicating that most of the models (71.4%) selected by SVM could be easily validated by the GDT-TS scores. In the case of FM, a GDT-TS score ≥30 is considered for selection and again about 86.6% (13 out of 15 selected by SVM) have a high GDT-TS score. Further, the network properties of the models predicted by each method and their combinations are examined. Fig. 6(c–f) plot the different network parameters as a function of Imin for models qualified by (a) all the methods [blue], (b) only SVM [green], (c) only GDC [cyan], (d) only GDT-TS [red] and (e) models not selected by any of the scoring scheme [black]. As expected, the characteristic patterns of native structures, described before in this work, are distinctively shown by the blue line [selected by all methods] while the black line [selected by none] deviates significantly from this pattern. Furthermore, models selected by any one of the methods display transition patterns with minor variations. Amongst these, models picked up by SVM show a pattern closer to the characteristic profile described earlier, followed by models selected by GDC and GDT-TS. The transition profile of GDC and SVM selected models seem to satisfy all the criteria of a characteristic profile, perhaps emphasising the importance of including side chain atoms for quality assessment.
From Soup: 58 An interesting example is the target Id T0661 (4FCZ), in which the native structure is incomplete with N and C terminal residues missing. However several modellers have provided good models of the complete structure. Strikingly, SVM has selected these models [with RMSD range from 2.10 Å to 2.55 Å], whereas, GDT-TS has failed to identify these models. The native (blue) and the models (red) are superposed in Fig. 8 . An impressive observation is that the models have an intact core very much similar to that of the native. The N and C terminal helices are on the periphery of this core in the models. It should be emphasised that the network method is able to pick up these models without a reference structure. This is mainly due to the conceptual framework that the native structures have a well-packed core and characteristic network profiles.
 ###### 
From cou.: 59 To evaluate the quality of models picked by different methods, RMSD distribution of the models at the Cα and all-atom level is examined. Fig. 7(a and b) show the RMSD distribution of models selected by (left) only GDT-TS, (middle) both GDT-TS and SVM and (right) only SVM as a pie chart. At the Cα level (Fig. 7a), models selected by GDT-TS have RMSD mainly in the range of 1.5 Å–3.0 Å [only GDT-TS (55%) and common panel (69%)]. Models with RMSD >3 Å and ≤5 Å have also been significantly selected by the GDT-TS scoring scheme. For the SVM selected models, the majority fall in the range of 1.5 Å–3 Å (69% + 18%) and 3 Å–5 Å (28% + 65%), a considerable percentage (17%) of models also have a RMSD value below 1.5 Å. This indicates that, while the network based method selects only a small number of models as compared to GDT-TS, it judiciously selects models with high accuracy as seen by the high percentage of models selected in the low RMSD range. Similar behaviour is seen for RMSD-all atom distribution. Strikingly, in this case models selected only by SVM show the lowest RMSD value (15%), while GDT-TS fails to pick any model with such a low value of RMSD. However, SVM also picks up a significant number of models as compared to GDT-TS with RMSD-all > 5 Å. This is a consequence of the fact that the network based method performs independent of the native structure and is solely based on the interaction strength of the side chain atoms, proper packing and orientation. The large RMSD in SVM selected models may arise due to different side-chain conformations and/or loop orientations (also may be due to false positives), indicating the possibility of structures with alternate conformations. An example of this is shown in Fig. 7c, where the model (T0667TS028_2) selected only by SVM for target Id T0667 has Cα-RMSD value = 2.64 Å and GDT-TS score = 54.82. Although side chain interaction is the major feature that drives the present network method, backbone packing is also checked on the basis of main-chain hydrogen bonds. Fig. 3(b–d) represent the percentage distribution of MHB for models selected by GDT-TS, both methods and SVM. Most of the models selected by these methods have MHB in the preferred region, however in the case of GDT-TS; a significant proportion of these models also lie in the low probability zone. Another example presented is the target Id T0708 (4H17), for which all the models selected by SVM are also selected by the GDT-TS score. In this case, the selected models align extremely well with the native structure (Fig. S3, ESI†) and exhibit low RMSD values, in the range of 1.66 Å to 1.98 Å.
From Soup: 59 The methodology described in this study has been implemented as an independent module (PSN_QA) of our PSN analysis web-server, GraProStr and is freely available at http://vishgraph.mbu.iisc.ernet.in/GraProStr/PSN-QA.html. A screenshot of the home page of the tool is shown in Fig. 9a . It takes a structure in pdb format as input and calculates the network properties at different I min s. The parameters are then passed on to the SVM classifier for a quality check. The tool outputs the quality of the input structure as native or non-native like ( Fig. 9b ). For structures with multiple chains, a quality check is provided for each subunit. Moreover, files containing the network parameter values at different I min s and plots showing the transition profile are also available for download.
 ###### 
From cou.: 60 An interesting example is the target Id T0661 (4FCZ), in which the native structure is incomplete with N and C terminal residues missing. However several modellers have provided good models of the complete structure. Strikingly, SVM has selected these models [with RMSD range from 2.10 Å to 2.55 Å], whereas, GDT-TS has failed to identify these models. The native (blue) and the models (red) are superposed in Fig. 8. An impressive observation is that the models have an intact core very much similar to that of the native. The N and C terminal helices are on the periphery of this core in the models. It should be emphasised that the network method is able to pick up these models without a reference structure. This is mainly due to the conceptual framework that the native structures have a well-packed core and characteristic network profiles.
From Soup: 60 Proteins acquire their unique structures through a subtle balance of various energetic and topological factors. The secondary structures such as helices and sheets, stabilized by the backbone hydrogen bonds, ensure the packing of the polypeptide chain. In addition, the unique structure is stabilized by non-covalent interactions of the amino acid side-chains, which should be optimal at the global level. PSN, being an all atom representation, captures not only the global topological features but also the chemistry of the side-chains. Our earlier study of PSN at low interaction strength 36 was the first work in this area, which indicated the potential of the network method to distinguish decoy structures from the native ones. Also, by extensive data analysis, we had demonstrated that the transition profiles of the network parameters, such as the largest cluster or community, as a function of interaction strength ( I min = 0% to 7%), are the hallmarks of the native protein structures. 14 Investigation of protein structures incorporating this feature would discriminate the natives from decoys in a more accurate manner. Furthermore, it also lends support to the observation that the native structures indeed possess general topological features while they are absent in random models. And the present study has shown that the network profiles of decoys/models in which the side-chain packing is not optimal, deviate from those exhibited by native structures.
 ###### 
From cou.: 61 The methodology described in this study has been implemented as an independent module (PSN_QA) of our PSN analysis web-server, GraProStr and is freely available at http://vishgraph.mbu.iisc.ernet.in/GraProStr/PSN-QA.html. A screenshot of the home page of the tool is shown in Fig. 9a. It takes a structure in pdb format as input and calculates the network properties at different Imins. The parameters are then passed on to the SVM classifier for a quality check. The tool outputs the quality of the input structure as native or non-native like (Fig. 9b). For structures with multiple chains, a quality check is provided for each subunit. Moreover, files containing the network parameter values at different Imins and plots showing the transition profile are also available for download.
From Soup: 61 Specifically, in the present study, we have explored the side-chain network profiles for a large number of native proteins and decoy/modelled structures, and the backbone packing is considered through main-chain hydrogen bonds. The size of the largest community as a function of I min represented in Fig. 10 for the native and the decoys/models clearly indicates the importance of the slope at the transition region and the relative values above and below the transition. The size of the community is larger at lower I min [0–2%] and smaller at the higher I min [>4%] in the native protein , when compared with the decoys, leading to a steeper slope for the native. The consistent display of the steep transition profile by the native structures is discussed in the results section by comparing with random models [Fig. S2, ESI † ] which drastically deviate from the native, while the decoys/models exhibit behaviour close to the native structures. An interesting observation from the structural cartoons in Fig. 10 is that a greater number of residues involved in forming k-2 community lie on the secondary structures of the native protein , while they are largely positioned on the loops for decoy structures. The optimal balance between the side-chain interaction and the backbone packing is crucial for the uniqueness of the protein structure. These topological properties have been integrated in SVM to build a classifier and the importance of the contributing features is mathematically evaluated. The model has effectively captured the above mentioned features [accuracy = 94.22%, AUC = 0.9853], specifically identifying the transition related parameters as top contributors. The classifier has successfully identified the good-quality models from the pool of CASP10 predicted structures. This is the first rigorous attempt to make use of the global topological features unique to the native structures, to assess modelled structures. In addition, the present study reinforces the fact that the native proteins acquire their unique structures by side-chains interactions, leading to optimal global connections at different interaction strengths. This is captured by the evaluation of network parameters on the PSNs of a large number of native proteins and the decoys, by and large fail to exhibit the subtleties in the network profiles. The trained SVM model has successfully captured the subtle features.
 ###### 
From cou.: 62 Proteins acquire their unique structures through a subtle balance of various energetic and topological factors. The secondary structures such as helices and sheets, stabilized by the backbone hydrogen bonds, ensure the packing of the polypeptide chain. In addition, the unique structure is stabilized by non-covalent interactions of the amino acid side-chains, which should be optimal at the global level. PSN, being an all atom representation, captures not only the global topological features but also the chemistry of the side-chains. Our earlier study of PSN at low interaction strength36 was the first work in this area, which indicated the potential of the network method to distinguish decoy structures from the native ones. Also, by extensive data analysis, we had demonstrated that the transition profiles of the network parameters, such as the largest cluster or community, as a function of interaction strength (Imin = 0% to 7%), are the hallmarks of the native protein structures.14 Investigation of protein structures incorporating this feature would discriminate the natives from decoys in a more accurate manner. Furthermore, it also lends support to the observation that the native structures indeed possess general topological features while they are absent in random models. And the present study has shown that the network profiles of decoys/models in which the side-chain packing is not optimal, deviate from those exhibited by native structures.
From Soup: 62 It is to be pointed out that the methodology developed here can be easily used for quality assessment of the models predicted in the absence of native structures. The present work has shown that the native structures exhibit characteristic topological features [measured from several network parameters] such as (a) higher value at lower I min , (b) lower value at higher I min and most importantly (c) steep transition from I min = 0% to I min = 4%. Here we have performed assessment of the CASP10 predicted models for 83 targets and the detailed comparison of SVM selected models with others are shown in Fig. 6–8 and Fig. S3 (ESI † ). Overall, the classifier is able to pick up structures that show native-like patterns such as good packing, well-formed secondary structures and proper orientation of the secondary structures, reiterating the fact that analysing parameters at the backbone level, may not be sufficient and a rigorous analysis at the backbone as well as the sidechain level may be required to distinctively differentiate a native structure from the decoy/model structure. SVM is also able to pick up a larger percentage of models that are closer to the native and we believe that integration of the network model with the existing pipeline procedures will enhance the predictability of the already available tools.
 ###### 
From cou.: 63 Specifically, in the present study, we have explored the side-chain network profiles for a large number of native proteins and decoy/modelled structures, and the backbone packing is considered through main-chain hydrogen bonds. The size of the largest community as a function of Imin represented in Fig. 10 for the native and the decoys/models clearly indicates the importance of the slope at the transition region and the relative values above and below the transition. The size of the community is larger at lower Imin [0–2%] and smaller at the higher Imin [>4%] in the native protein, when compared with the decoys, leading to a steeper slope for the native. The consistent display of the steep transition profile by the native structures is discussed in the results section by comparing with random models [Fig. S2, ESI†] which drastically deviate from the native, while the decoys/models exhibit behaviour close to the native structures. An interesting observation from the structural cartoons in Fig. 10 is that a greater number of residues involved in forming k-2 community lie on the secondary structures of the native protein, while they are largely positioned on the loops for decoy structures. The optimal balance between the side-chain interaction and the backbone packing is crucial for the uniqueness of the protein structure. These topological properties have been integrated in SVM to build a classifier and the importance of the contributing features is mathematically evaluated. The model has effectively captured the above mentioned features [accuracy = 94.22%, AUC = 0.9853], specifically identifying the transition related parameters as top contributors. The classifier has successfully identified the good-quality models from the pool of CASP10 predicted structures. This is the first rigorous attempt to make use of the global topological features unique to the native structures, to assess modelled structures. In addition, the present study reinforces the fact that the native proteins acquire their unique structures by side-chains interactions, leading to optimal global connections at different interaction strengths. This is captured by the evaluation of network parameters on the PSNs of a large number of native proteins and the decoys, by and large fail to exhibit the subtleties in the network profiles. The trained SVM model has successfully captured the subtle features.
From Soup: 63 To summarise, a large number of native structures and decoy/model structures were used to generate protein structure networks at different interaction strengths [ I min ]. The profiles of the network properties as a function of I min for native proteins and the decoy/model structures are compared to identify patterns specific to native structures. The network parameters generated at different I min and main-chain hydrogen bonds are integrated into Support Vector Machine to develop a classifier. An accuracy of 94.11% is obtained. The feature selection methods have captured the main chain hydrogen bonds and transition profiles of network parameters as the top contributors to the native structures. Classifier is further validated with specific test cases and later used for predicting the quality of CASP10 predicted models. Thus, a robust and general classifier to distinguish native protein from decoy/model is built using PSNs at different interaction strengths, which can be used for building good models and also to assess predicted models. A web-server (http://vishgraph.mbu.iisc.ernet.in/GraProStr/PSN-QA.html) has been made available for this purpose. The study also reinforces the observation that the native structures have unique global-topological transition profiles.
 ###### 
From cou.: 64 It is to be pointed out that the methodology developed here can be easily used for quality assessment of the models predicted in the absence of native structures. The present work has shown that the native structures exhibit characteristic topological features [measured from several network parameters] such as (a) higher value at lower Imin, (b) lower value at higher Imin and most importantly (c) steep transition from Imin = 0% to Imin = 4%. Here we have performed assessment of the CASP10 predicted models for 83 targets and the detailed comparison of SVM selected models with others are shown in Fig. 6–8 and Fig. S3 (ESI†). Overall, the classifier is able to pick up structures that show native-like patterns such as good packing, well-formed secondary structures and proper orientation of the secondary structures, reiterating the fact that analysing parameters at the backbone level, may not be sufficient and a rigorous analysis at the backbone as well as the sidechain level may be required to distinctively differentiate a native structure from the decoy/model structure. SVM is also able to pick up a larger percentage of models that are closer to the native and we believe that integration of the network model with the existing pipeline procedures will enhance the predictability of the already available tools.
From Soup: 64 We acknowledge the computational resources provided the Department of Biotechnology (DBT), through the Centre of Excellence (COE). SC thanks Department of Science and Technology (Mathematical Biology Grant, IMI) for fellowship. SV acknowledges the Council of Scientific and Industrial Research (CSIR, India) for Emeritus Scientist, CSIR.
 ###### 
From cou.: 65 To summarise, a large number of native structures and decoy/model structures were used to generate protein structure networks at different interaction strengths [Imin]. The profiles of the network properties as a function of Imin for native proteins and the decoy/model structures are compared to identify patterns specific to native structures. The network parameters generated at different Imin and main-chain hydrogen bonds are integrated into Support Vector Machine to develop a classifier. An accuracy of 94.11% is obtained. The feature selection methods have captured the main chain hydrogen bonds and transition profiles of network parameters as the top contributors to the native structures. Classifier is further validated with specific test cases and later used for predicting the quality of CASP10 predicted models. Thus, a robust and general classifier to distinguish native protein from decoy/model is built using PSNs at different interaction strengths, which can be used for building good models and also to assess predicted models. A web-server (http://vishgraph.mbu.iisc.ernet.in/GraProStr/PSN-QA.html) has been made available for this purpose. The study also reinforces the observation that the native structures have unique global-topological transition profiles.
From Soup: 65 C. B. Anfinsen, Science , 1973, 181 , 223–230 CAS .
 ###### 
From cou.: 66 We acknowledge the computational resources provided the Department of Biotechnology (DBT), through the Centre of Excellence (COE). SC thanks Department of Science and Technology (Mathematical Biology Grant, IMI) for fellowship. SV acknowledges the Council of Scientific and Industrial Research (CSIR, India) for Emeritus Scientist, CSIR.
From Soup: 66 P. Lewis, D. Kotelchuck and H. Scheraga, Proc. Natl. Acad. Sci. U. S. A. , 1970, 65 , 810–815 CrossRef CAS .
 ###### 
From cou.: 67 C. B. Anfinsen, Science, 1973, 181, 223–230 CAS .
From Soup: 67 S. Burley and G. Petsko, Science , 1985, 229 , 23 CAS .
 ###### 
From cou.: 68 P. Lewis, D. Kotelchuck and H. Scheraga, Proc. Natl. Acad. Sci. U. S. A., 1970, 65, 810–815 CrossRef CAS .
From Soup: 68 H. J. Dyson, P. E. Wright and H. A. Scheraga, Proc. Natl. Acad. Sci. U. S. A. , 2006, 103 , 13057–13061 CrossRef CAS .
 ###### 
From cou.: 69 S. Burley and G. Petsko, Science, 1985, 229, 23 CAS .
From Soup: 69 J. T. Kellis and D. Kerstin Nyberg, Nature , 1988, 333 , 784–786 CrossRef CAS .
 ###### 
From cou.: 70 H. J. Dyson, P. E. Wright and H. A. Scheraga, Proc. Natl. Acad. Sci. U. S. A., 2006, 103, 13057–13061 CrossRef CAS .
From Soup: 70 C. N. Pace, H. Fu, K. L. Fryar, J. Landua, S. R. Trevino, B. A. Shirley, M. M. N. Hendricks, S. Iimura, K. Gajiwala and J. M. Scholtz, J. Mol. Biol. , 2011, 408 , 514–528 CrossRef CAS .
 ###### 
From cou.: 71 J. T. Kellis and D. Kerstin Nyberg, Nature, 1988, 333, 784–786 CrossRef CAS .
From Soup: 71 L. Serrano, M. Bycroft and A. R. Fersht, J. Mol. Biol. , 1991, 218 , 465–475 CrossRef CAS .
 ###### 
From cou.: 72 C. N. Pace, H. Fu, K. L. Fryar, J. Landua, S. R. Trevino, B. A. Shirley, M. M. N. Hendricks, S. Iimura, K. Gajiwala and J. M. Scholtz, J. Mol. Biol., 2011, 408, 514–528 CrossRef CAS .
From Soup: 72 G. D. Rose, P. J. Fleming, J. R. Banavar and A. Maritan, Proc. Natl. Acad. Sci. U. S. A. , 2006, 103 , 16623–16633 CrossRef CAS .
 ###### 
From cou.: 73 L. Serrano, M. Bycroft and A. R. Fersht, J. Mol. Biol., 1991, 218, 465–475 CrossRef CAS .
From Soup: 73 K. A. Dill, S. B. Ozkan, M. S. Shell and T. R. Weikl, Annu. Rev. Biophys. , 2008, 37 , 289 CrossRef CAS .
 ###### 
From cou.: 74 G. D. Rose, P. J. Fleming, J. R. Banavar and A. Maritan, Proc. Natl. Acad. Sci. U. S. A., 2006, 103, 16623–16633 CrossRef CAS .
From Soup: 74 C. M. Dobson, Nature , 2003, 426 , 884–890 CrossRef CAS .
 ###### 
From cou.: 75 K. A. Dill, S. B. Ozkan, M. S. Shell and T. R. Weikl, Annu. Rev. Biophys., 2008, 37, 289 CrossRef CAS .
From Soup: 75 A. R. Fersht, Nat. Rev. Mol. Cell Biol. , 2008, 9 , 650–654 CrossRef CAS .
 ###### 
From cou.: 76 C. M. Dobson, Nature, 2003, 426, 884–890 CrossRef CAS .
From Soup: 76 M. Karplus, Nat. Chem. Biol. , 2011, 7 , 401–404 CrossRef CAS .
 ###### 
From cou.: 77 A. R. Fersht, Nat. Rev. Mol. Cell Biol., 2008, 9, 650–654 CrossRef CAS .
From Soup: 77 C. Atilgan, O. B. Okan and A. R. Atilgan, Proteins: Struct., Funct., Bioinf. , 2010, 78 , 3363–3375 CrossRef CAS .
 ###### 
From cou.: 78 M. Karplus, Nat. Chem. Biol., 2011, 7, 401–404 CrossRef CAS .
From Soup: 78 G. Bagler and S. Sinha, Phys. A , 2005, 346 , 27–33 CrossRef CAS .
 ###### 
From cou.: 79 C. Atilgan, O. B. Okan and A. R. Atilgan, Proteins: Struct., Funct., Bioinf., 2010, 78, 3363–3375 CrossRef CAS .
From Soup: 79 A. R. Atilgan, P. Akan and C. Baysal, Biophys. J. , 2004, 86 , 85 CrossRef CAS .
 ###### 
From cou.: 80 G. Bagler and S. Sinha, Phys. A, 2005, 346, 27–33 CrossRef CAS .
From Soup: 80 L. H. Greene and V. A. Higman, J. Mol. Biol. , 2003, 334 , 781–791 CrossRef CAS .
 ###### 
From cou.: 81 A. R. Atilgan, P. Akan and C. Baysal, Biophys. J., 2004, 86, 85 CrossRef CAS .
From Soup: 81 M. Vassura, L. Margara, P. Fariselli and R. Casadio, Artif. Intell. Med. , 2009, 45 , 229–237 CrossRef .
 ###### 
From cou.: 82 L. H. Greene and V. A. Higman, J. Mol. Biol., 2003, 334, 781–791 CrossRef CAS .
From Soup: 82 T. J. Taylor and I. I. Vaisman, Phys. Rev. E , 2006, 73 , 041925 CrossRef .
 ###### 
From cou.: 83 M. Vassura, L. Margara, P. Fariselli and R. Casadio, Artif. Intell. Med., 2009, 45, 229–237 CrossRef .
From Soup: 83 A. Küçükural, U. Sezerman and A. Erçil, Adv. Bioinf. Comput. Biol. , 2008, 6 , 59–67 Search PubMed .
 ###### 
From cou.: 84 T. J. Taylor and I. I. Vaisman, Phys. Rev. E, 2006, 73, 041925 CrossRef .
From Soup: 84 N. Kannan and S. Vishveshwara, J. Mol. Biol. , 1999, 292 , 441–464 CrossRef CAS .
 ###### 
From cou.: 85 A. Küçükural, U. Sezerman and A. Erçil, Adv. Bioinf. Comput. Biol., 2008, 6, 59–67 Search PubMed .
From Soup: 85 A. Sukhwal, M. Bhattacharyya and S. Vishveshwara, Acta Crystallogr., Sect. D: Biol. Crystallogr. , 2011, 67 , 429–439 CAS .
 ###### 
From cou.: 86 N. Kannan and S. Vishveshwara, J. Mol. Biol., 1999, 292, 441–464 CrossRef CAS .
From Soup: 86 R. Sathyapriya, M. Vijayabaskar and S. Vishveshwara, PLoS Comput. Biol. , 2008, 4 , e1000170 CAS .
 ###### 
From cou.: 87 A. Sukhwal, M. Bhattacharyya and S. Vishveshwara, Acta Crystallogr., Sect. D: Biol. Crystallogr., 2011, 67, 429–439 CAS .
From Soup: 87 D. Deb and S. Vishveshwara, Biophys. J. , 2009, 97 , 1787–1794 CrossRef CAS .
 ###### 
From cou.: 88 R. Sathyapriya, M. Vijayabaskar and S. Vishveshwara, PLoS Comput. Biol., 2008, 4, e1000170 CAS .
From Soup: 88 S. Wu, J. Skolnick and Y. Zhang, BMC Biol. , 2007, 5 , 17 CrossRef .
 ###### 
From cou.: 89 D. Deb and S. Vishveshwara, Biophys. J., 2009, 97, 1787–1794 CrossRef CAS .
From Soup: 89 J. Zhang and Y. Zhang, PLoS One , 2010, 5 , e15386 Search PubMed .
 ###### 
From cou.: 90 S. Wu, J. Skolnick and Y. Zhang, BMC Biol., 2007, 5, 17 CrossRef .
From Soup: 90 E. S. Huang, S. Subbiah, J. Tsai and M. Levitt, J. Mol. Biol. , 1996, 257 , 716–725 CrossRef CAS .
 ###### 
From cou.: 91 J. Zhang and Y. Zhang, PLoS One, 2010, 5, e15386 Search PubMed .
From Soup: 91 B. Park and M. Levitt, J. Mol. Biol. , 1996, 258 , 367–392 CrossRef CAS .
 ###### 
From cou.: 92 E. S. Huang, S. Subbiah, J. Tsai and M. Levitt, J. Mol. Biol., 1996, 257, 716–725 CrossRef CAS .
From Soup: 92 B. Ranjit and C. Pinak, BMC Struct. Biol. , 2009, 9 , 76–84 CrossRef .
 ###### 
From cou.: 93 B. Park and M. Levitt, J. Mol. Biol., 1996, 258, 367–392 CrossRef CAS .
From Soup: 93 D. Gilis, J. Biomol. Struct. Dyn. , 2004, 21 , 725–735 CAS .
 ###### 
From cou.: 94 B. Ranjit and C. Pinak, BMC Struct. Biol., 2009, 9, 76–84 CrossRef .
From Soup: 94 R. Zhou, B. D. Silverman, A. K. Royyuru and P. Athma, Proteins , 2003, 52 , 561–572 CrossRef CAS .
 ###### 
From cou.: 95 D. Gilis, J. Biomol. Struct. Dyn., 2004, 21, 725–735 CAS .
From Soup: 95 M. Kalman and N. Ben-Tal, Bioinformatics , 2010, 26 , 1299–1307 CrossRef CAS .
 ###### 
From cou.: 96 R. Zhou, B. D. Silverman, A. K. Royyuru and P. Athma, Proteins, 2003, 52, 561–572 CrossRef CAS .
From Soup: 96 R. P. Bahadur and P. Chakrabarti, BMC Struct. Biol. , 2009, 9 , 76 CrossRef .
 ###### 
From cou.: 97 M. Kalman and N. Ben-Tal, Bioinformatics, 2010, 26, 1299–1307 CrossRef CAS .
From Soup: 97 A. Nath Jha, S. Vishveshwara and J. R. Banavar, Protein Sci. , 2010, 19 , 603–616 CrossRef .
 ###### 
From cou.: 98 R. P. Bahadur and P. Chakrabarti, BMC Struct. Biol., 2009, 9, 76 CrossRef .
From Soup: 98 S. Miyazawa and R. L. Jernigan, Proteins: Struct., Funct., Bioinf. , 1999, 34 , 49–68 CrossRef CAS .
 ###### 
From cou.: 99 A. Nath Jha, S. Vishveshwara and J. R. Banavar, Protein Sci., 2010, 19, 603–616 CrossRef .
From Soup: 99 J. Moult, K. Fidelis, A. Kryshtafovych and A. Tramontano, Proteins: Struct., Funct., Bioinf. , 2011, 79 (Suppl 10), 1–5 CrossRef CAS .
 ###### 
From cou.: 100 S. Miyazawa and R. L. Jernigan, Proteins: Struct., Funct., Bioinf., 1999, 34, 49–68 CrossRef CAS .
From Soup: 100 S. Chatterjee, M. Bhattacharyya and S. Vishveshwara, J. Biomol. Struct. Dyn. , 2012, 29 , 1110–1126 CAS .
 ###### 
From cou.: 101 J. Moult, K. Fidelis, A. Kryshtafovych and A. Tramontano, Proteins: Struct., Funct., Bioinf., 2011, 79(Suppl 10), 1–5 CrossRef CAS .
From Soup: 101 K. Brinda and S. Vishveshwara, Biophys. J. , 2005, 89 , 4159 CrossRef CAS .
 ###### 
From cou.: 102 S. Chatterjee, M. Bhattacharyya and S. Vishveshwara, J. Biomol. Struct. Dyn., 2012, 29, 1110–1126 CAS .
From Soup: 102 W. S. Noble, Nat. Biotechnol. , 2006, 24 , 1565–1567 CrossRef CAS .
 ###### 
From cou.: 103 K. Brinda and S. Vishveshwara, Biophys. J., 2005, 89, 4159 CrossRef CAS .
From Soup: 103 J. Guo, H. Chen, Z. Sun and Y. Lin, Proteins: Struct., Funct., Bioinf. , 2004, 54 , 738–743 CrossRef CAS .
 ###### 
From cou.: 104 W. S. Noble, Nat. Biotechnol., 2006, 24, 1565–1567 CrossRef CAS .
From Soup: 104 S. Hua and Z. Sun, J. Mol. Biol. , 2001, 308 , 397–408 CrossRef CAS .
 ###### 
From cou.: 105 J. Guo, H. Chen, Z. Sun and Y. Lin, Proteins: Struct., Funct., Bioinf., 2004, 54, 738–743 CrossRef CAS .
From Soup: 105 J. Ward, L. J. McGuffin, B. F. Buxton and D. T. Jones, Bioinformatics , 2003, 19 , 1650–1655 CrossRef CAS .
 ###### 
From cou.: 106 S. Hua and Z. Sun, J. Mol. Biol., 2001, 308, 397–408 CrossRef CAS .
From Soup: 106 H. Kim and H. Park, Protein Eng. , 2003, 16 , 553–560 CrossRef CAS .
 ###### 
From cou.: 107 J. Ward, L. J. McGuffin, B. F. Buxton and D. T. Jones, Bioinformatics, 2003, 19, 1650–1655 CrossRef CAS .
From Soup: 107 Y. D. Cai, X. J. Liu, X. Xu and K. C. Chou, Comput. Chem. , 2002, 26 , 293–296 CrossRef CAS .
 ###### 
From cou.: 108 H. Kim and H. Park, Protein Eng., 2003, 16, 553–560 CrossRef CAS .
From Soup: 108 J. R. Bradford and D. R. Westhead, Bioinformatics , 2005, 21 , 1487–1494 CrossRef CAS .
 ###### 
From cou.: 109 Y. D. Cai, X. J. Liu, X. Xu and K. C. Chou, Comput. Chem., 2002, 26, 293–296 CrossRef CAS .
From Soup: 109 Q. Dong, Y. Chen and S. Zhou, International Journal of General Systems , 2011, 40 , 417–425 CrossRef .
 ###### 
From cou.: 110 J. R. Bradford and D. R. Westhead, Bioinformatics, 2005, 21, 1487–1494 CrossRef CAS .
From Soup: 110 P. Mereghetti, M. L. Ganadu, E. Papaleo, P. Fantucci and L. De Gioia, BMC Bioinf. , 2008, 9 , 66 CrossRef .
 ###### 
From cou.: 111 Q. Dong, Y. Chen and S. Zhou, International Journal of General Systems, 2011, 40, 417–425 CrossRef .
From Soup: 111 R. Bonneau, J. Tsai, I. Ruczinski, D. Chivian, C. Rohl, C. E. M. Strauss and D. Baker, Proteins: Struct., Funct., Bioinf. , 2002, 45 , 119–126 CrossRef .
 ###### 
From cou.: 112 P. Mereghetti, M. L. Ganadu, E. Papaleo, P. Fantucci and L. De Gioia, BMC Bioinf., 2008, 9, 66 CrossRef .
From Soup: 112 J. Tsai, R. Bonneau, A. V. Morozov, B. Kuhlman, C. A. Rohl and D. Baker, Proteins: Struct., Funct., Bioinf. , 2003, 53 , 76–87 CrossRef CAS .
 ###### 
From cou.: 113 R. Bonneau, J. Tsai, I. Ruczinski, D. Chivian, C. Rohl, C. E. M. Strauss and D. Baker, Proteins: Struct., Funct., Bioinf., 2002, 45, 119–126 CrossRef .
From Soup: 113 R. Samudrala and M. Levitt, Protein Sci. , 2008, 9 , 1399–1401 CrossRef .
 ###### 
From cou.: 114 J. Tsai, R. Bonneau, A. V. Morozov, B. Kuhlman, C. A. Rohl and D. Baker, Proteins: Struct., Funct., Bioinf., 2003, 53, 76–87 CrossRef CAS .
From Soup: 114 H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. Bhat, H. Weissig, I. N. Shindyalov and P. E. Bourne, Nucleic Acids Res. , 2000, 28 , 235–242 CrossRef CAS .
 ###### 
From cou.: 115 R. Samudrala and M. Levitt, Protein Sci., 2008, 9, 1399–1401 CrossRef .
From Soup: 115 T. H. Cormen, C. E. Leiserson, R. L. Rivest and C. Stein, Introduction to algorithms , MIT press, 2001 Search PubMed .
 ###### 
From cou.: 116 H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. Bhat, H. Weissig, I. N. Shindyalov and P. E. Bourne, Nucleic Acids Res., 2000, 28, 235–242 CrossRef CAS .
From Soup: 116 M. Bhattacharyya, A. Ghosh, P. Hansia and S. Vishveshwara, Proteins: Struct., Funct., Bioinf. , 2010, 78 , 506–517 CrossRef CAS .
 ###### 
From cou.: 117 T. H. Cormen, C. E. Leiserson, R. L. Rivest and C. Stein, Introduction to algorithms, MIT press, 2001 Search PubMed .
From Soup: 117 G. Palla, I. Derényi, I. Farkas and T. Vicsek, Nature , 2005, 435 , 814–818 CrossRef CAS .
 ###### 
From cou.: 118 M. Bhattacharyya, A. Ghosh, P. Hansia and S. Vishveshwara, Proteins: Struct., Funct., Bioinf., 2010, 78, 506–517 CrossRef CAS .
From Soup: 118 C. C. Chang and C. J. Lin, ACM Transactions on Intelligent Systems and Technology (TIST) , 2011, 2 , 27 Search PubMed .
 ###### 
From cou.: 119 G. Palla, I. Derényi, I. Farkas and T. Vicsek, Nature, 2005, 435, 814–818 CrossRef CAS .
From Soup: 119 I. K. McDonald and J. M. Thornton, J. Mol. Biol. , 1994, 238 , 777–793 CrossRef CAS .
 ###### 
From cou.: 120 C. C. Chang and C. J. Lin, ACM Transactions on Intelligent Systems and Technology (TIST), 2011, 2, 27 Search PubMed .
From Soup: 120 Y. W. Chen and C. J. Lin, Feature Extraction , 2006, 315–324 CrossRef .
 ###### 
From cou.: 121 I. K. McDonald and J. M. Thornton, J. Mol. Biol., 1994, 238, 777–793 CrossRef CAS .
From Soup: 121 M. Robnik-Šikonja and I. Kononenko, Mach. Learn. , 2003, 53 , 23–69 CrossRef .
 ###### 
From cou.: 122 Y. W. Chen and C. J. Lin, Feature Extraction, 2006, 315–324 CrossRef .
From Soup: 122 V. Mariani, F. Kiefer, T. Schmidt, J. Haas and T. Schwede, Proteins: Struct., Funct., Bioinf. , 2011, 79 (Suppl 10), 37–58 CrossRef CAS .
 ###### 
From cou.: 123 M. Robnik-Šikonja and I. Kononenko, Mach. Learn., 2003, 53, 23–69 CrossRef .
From Soup: 123 L. Kinch, S. Yong Shi, Q. Cong, H. Cheng, Y. Liao and N. V. Grishin, Proteins: Struct., Funct., Bioinf. , 2011, 79 (Suppl 10), 59–73 CrossRef CAS .
 ###### 
From cou.: 124 V. Mariani, F. Kiefer, T. Schmidt, J. Haas and T. Schwede, Proteins: Struct., Funct., Bioinf., 2011, 79(Suppl 10), 37–58 CrossRef CAS .
From Soup: 124 B. Monastyrskyy, K. Fidelis, A. Tramontano and A. Kryshtafovych, Proteins: Struct., Funct., Bioinf. , 2011, 79 , 119–125 CrossRef CAS .
 ###### 
From cou.: 125 L. Kinch, S. Yong Shi, Q. Cong, H. Cheng, Y. Liao and N. V. Grishin, Proteins: Struct., Funct., Bioinf., 2011, 79(Suppl 10), 59–73 CrossRef CAS .
From Soup: 125 A. Kryshtafovych, K. Fidelis and A. Tramontano, Proteins: Struct., Funct., Bioinf. , 2011, 79 (Suppl 10), 91–106 CrossRef CAS .
 ###### 
From cou.: 126 B. Monastyrskyy, K. Fidelis, A. Tramontano and A. Kryshtafovych, Proteins: Struct., Funct., Bioinf., 2011, 79, 119–125 CrossRef CAS .
From Soup: 126 V. Mariani, F. Kiefer, T. Schmidt, J. Haas and T. Schwede, Proteins , 2011, 79 (Suppl 10), 37–58 CrossRef CAS .
 ###### 
From cou.: 127 A. Kryshtafovych, K. Fidelis and A. Tramontano, Proteins: Struct., Funct., Bioinf., 2011, 79(Suppl 10), 91–106 CrossRef CAS .
From Soup: 127 S. N. Soffer and A. Vázquez, Phys. Rev. E , 2005, 71 , 057101 CrossRef .
 ###### 
From cou.: 128 V. Mariani, F. Kiefer, T. Schmidt, J. Haas and T. Schwede, Proteins, 2011, 79(Suppl 10), 37–58 CrossRef CAS .
From cou.: 129 S. N. Soffer and A. Vázquez, Phys. Rev. E, 2005, 71, 057101 CrossRef .
